{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 13 â€” Recurrent Neural Networks\n",
    "\n",
    "> **ê°•ì˜ ì‹œê°„:** ì•½ 2ì‹œê°„  \n",
    "> **ì£¼ì œ:** ìˆœí™˜ ì‹ ê²½ë§ â€” Vanilla RNN, LSTM, GRU, ì‹œê³„ì—´ ì˜ˆì¸¡\n",
    "\n",
    "---\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "| # | ëª©í‘œ | ì˜ˆìƒ ì‹œê°„ |\n",
    "|---|---|---|\n",
    "| 1 | Vanilla RNN: ìˆ˜ë™ êµ¬í˜„, í•™ìŠµ, ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ | 30ë¶„ |\n",
    "| 2 | LSTM: ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜, í•™ìŠµ, ë¯¸ë˜ ì˜ˆì¸¡ | 35ë¶„ |\n",
    "| 3 | GRU: ë‹¨ìˆœí™”ëœ ê²Œì´íŒ…, 3ëª¨ë¸ ë¹„êµ | 25ë¶„ |\n",
    "| 4 | Exercise | 25ë¶„ |\n",
    "\n",
    "---\n",
    "\n",
    "**ë°ì´í„°ì…‹:** numpy í•©ì„± ì‹œê³„ì—´ (ì¸í„°ë„· ë¶ˆí•„ìš”)  \n",
    "- ì‚¬ì¸íŒŒ: `y = sin(t) + 0.1Ã—noise`, 1200 íƒ€ì„ìŠ¤í…  \n",
    "- ë³µì¡ ì‹ í˜¸: `sin(t) + 0.5Â·sin(3t) + 0.3Â·sin(7t) + noise`, 1600 íƒ€ì„ìŠ¤í…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Using device   : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë°ì´í„° ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "\n",
    "# ì‚¬ì¸íŒŒ (Part 1â€“2)\n",
    "T_sine = 1200\n",
    "t_sine = np.linspace(0, 12 * np.pi, T_sine)\n",
    "sine_signal = (np.sin(t_sine) + 0.1 * np.random.randn(T_sine)).astype(np.float32)\n",
    "\n",
    "# ë³µì¡ ì‹ í˜¸ (Part 3)\n",
    "T_complex = 1600\n",
    "t_complex = np.linspace(0, 16 * np.pi, T_complex)\n",
    "complex_signal = (np.sin(t_complex)\n",
    "                  + 0.5 * np.sin(3 * t_complex)\n",
    "                  + 0.3 * np.sin(7 * t_complex)\n",
    "                  + 0.1 * np.random.randn(T_complex)).astype(np.float32)\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "SEQ_LEN = 30\n",
    "\n",
    "def make_sequences(series, seq_len=SEQ_LEN):\n",
    "    \"\"\"\n",
    "    ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ (ì…ë ¥, íƒ€ê¹ƒ) ìŒ ìƒì„±.\n",
    "    ì…ë ¥: series[i : i+seq_len]\n",
    "    íƒ€ê¹ƒ: series[i+1 : i+seq_len+1]  (í•œ ìŠ¤í… ì• ì˜ˆì¸¡)\n",
    "    Returns: X (N, seq_len, 1), y (N, seq_len, 1)\n",
    "    \"\"\"\n",
    "    X = np.array([series[i:i+seq_len]       for i in range(len(series)-seq_len)])\n",
    "    y = np.array([series[i+1:i+seq_len+1]   for i in range(len(series)-seq_len)])\n",
    "    return (torch.FloatTensor(X).unsqueeze(-1),\n",
    "            torch.FloatTensor(y).unsqueeze(-1))\n",
    "\n",
    "def split_data(X, y, train_ratio=0.8):\n",
    "    n = int(len(X) * train_ratio)\n",
    "    return X[:n], y[:n], X[n:], y[n:]\n",
    "\n",
    "# ì‚¬ì¸íŒŒ ë°ì´í„° ì¤€ë¹„\n",
    "X_sine, y_sine = make_sequences(sine_signal)\n",
    "X_tr, y_tr, X_te, y_te = split_data(X_sine, y_sine)\n",
    "\n",
    "train_ds  = TensorDataset(X_tr, y_tr)\n",
    "train_ldr = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "print('=== ì‚¬ì¸íŒŒ ë°ì´í„°ì…‹ ===')\n",
    "print(f'  ì´ íƒ€ì„ìŠ¤í… : {T_sine}')\n",
    "print(f'  seq_len     : {SEQ_LEN}')\n",
    "print(f'  ì´ ì‹œí€€ìŠ¤   : {len(X_sine)}  (X shape: {tuple(X_sine.shape)})')\n",
    "print(f'  Train       : {len(X_tr)}  / Test: {len(X_te)}')\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 3.5))\n",
    "axes[0].plot(t_sine[:200], sine_signal[:200], color='steelblue', lw=1.5)\n",
    "axes[0].set_title('ì‚¬ì¸íŒŒ ì‹ í˜¸ (ì²˜ìŒ 200 íƒ€ì„ìŠ¤í…)'); axes[0].set_xlabel('t'); axes[0].set_ylabel('y')\n",
    "\n",
    "axes[1].plot(t_complex[:400], complex_signal[:400], color='tomato', lw=1.5)\n",
    "axes[1].set_title('ë³µì¡ ì‹ í˜¸ (ì²˜ìŒ 400 íƒ€ì„ìŠ¤í…)'); axes[1].set_xlabel('t'); axes[1].set_ylabel('y')\n",
    "\n",
    "plt.suptitle('í•™ìŠµ ë°ì´í„° ì‹œê°í™”', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. Vanilla RNN\n",
    "\n",
    "### 1-1. RNNì˜ í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "RNNì€ ì´ì „ íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœë¥¼ í˜„ì¬ ì…ë ¥ê³¼ ê²°í•©í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$h_t = \\tanh(W_{ih} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b)$$\n",
    "$$\\hat{y}_t = W_{ho} \\cdot h_t + b_o$$\n",
    "\n",
    "| ê¸°í˜¸ | í¬ê¸° | ì„¤ëª… |\n",
    "|---|---|---|\n",
    "| $x_t$ | (input_size,) | íƒ€ì„ìŠ¤í… $t$ì˜ ì…ë ¥ |\n",
    "| $h_t$ | (hidden_size,) | íƒ€ì„ìŠ¤í… $t$ì˜ ì€ë‹‰ ìƒíƒœ |\n",
    "| $W_{ih}$ | (hidden, input) | ì…ë ¥â†’ì€ë‹‰ ê°€ì¤‘ì¹˜ |\n",
    "| $W_{hh}$ | (hidden, hidden) | ì€ë‹‰â†’ì€ë‹‰ ìˆœí™˜ ê°€ì¤‘ì¹˜ (í•µì‹¬!) |\n",
    "\n",
    "### 1-2. í•µì‹¬ íŠ¹ì§•\n",
    "- **ê°€ì¤‘ì¹˜ ê³µìœ **: ëª¨ë“  íƒ€ì„ìŠ¤í…ì—ì„œ ë™ì¼í•œ $W_{ih}, W_{hh}, b$ ì‚¬ìš©\n",
    "- **ì‹œê°„ ì „ê°œ(Unrolling)**: ì‹œí€€ìŠ¤ë¥¼ ë”°ë¼ í¼ì¹˜ë©´ ë§¤ìš° ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì™€ ë™ì¼\n",
    "- **BPTT**: ì‹œê°„ ì—­ì „íŒŒ(Backpropagation Through Time)ë¡œ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN ìˆ˜ë™ êµ¬í˜„ (numpy) â€” nn.RNNê³¼ ë™ì¼í•œ ì—°ì‚°\n",
    "\n",
    "def rnn_step_manual(x_t, h_prev, W_ih, W_hh, b):\n",
    "    \"\"\"ë‹¨ì¼ íƒ€ì„ìŠ¤í… RNN ì—…ë°ì´íŠ¸\n",
    "    h_t = tanh(W_ih @ x_t + W_hh @ h_prev + b)\n",
    "    \"\"\"\n",
    "    return np.tanh(W_ih @ x_t + W_hh @ h_prev + b)\n",
    "\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ì„¤ì • (input=1, hidden=4)\n",
    "np.random.seed(0)\n",
    "input_size, hidden_size = 1, 4\n",
    "W_ih = np.random.randn(hidden_size, input_size)  * 0.3\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.3\n",
    "b    = np.zeros(hidden_size)\n",
    "\n",
    "# ì‚¬ì¸íŒŒ 20 íƒ€ì„ìŠ¤í…ìœ¼ë¡œ ìˆœì „íŒŒ\n",
    "t_demo    = np.linspace(0, 2 * np.pi, 20)\n",
    "x_demo    = np.sin(t_demo)\n",
    "h         = np.zeros(hidden_size)\n",
    "hiddens   = []\n",
    "\n",
    "for val in x_demo:\n",
    "    h = rnn_step_manual(np.array([val]), h, W_ih, W_hh, b)\n",
    "    hiddens.append(h.copy())\n",
    "\n",
    "hiddens = np.array(hiddens)   # (20, 4)\n",
    "\n",
    "# PyTorch nn.RNNìœ¼ë¡œ ë™ì¼ íŒŒë¼ë¯¸í„° ê²€ì¦\n",
    "rnn_check = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "with torch.no_grad():\n",
    "    rnn_check.weight_ih_l0.copy_(torch.FloatTensor(W_ih))\n",
    "    rnn_check.weight_hh_l0.copy_(torch.FloatTensor(W_hh))\n",
    "    rnn_check.bias_ih_l0.zero_()\n",
    "    rnn_check.bias_hh_l0.zero_()\n",
    "    x_pt  = torch.FloatTensor(x_demo).view(1, -1, 1)\n",
    "    out_pt, _ = rnn_check(x_pt)\n",
    "\n",
    "max_diff = np.max(np.abs(hiddens - out_pt[0].numpy()))\n",
    "print(f'ìˆ˜ë™ RNN vs nn.RNN ìµœëŒ€ ì˜¤ì°¨: {max_diff:.2e}  (â‰ˆ0 ì´ë©´ ë™ì¼)')\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "axes[0].plot(t_demo, x_demo, 'b-o', ms=4, lw=2, label='ì…ë ¥ x_t')\n",
    "axes[0].set_title('ì…ë ¥ ì‹œí€€ìŠ¤ (ì‚¬ì¸íŒŒ, 20 íƒ€ì„ìŠ¤í…)')\n",
    "axes[0].set_xlabel('t'); axes[0].set_ylabel('ê°’'); axes[0].legend()\n",
    "\n",
    "colors_h = ['steelblue', 'tomato', 'seagreen', 'darkorange']\n",
    "for i in range(hidden_size):\n",
    "    axes[1].plot(hiddens[:, i], color=colors_h[i], lw=2, label=f'h[{i}]')\n",
    "axes[1].set_title('ì€ë‹‰ ìƒíƒœ h_t (hidden_size=4)')\n",
    "axes[1].set_xlabel('íƒ€ì„ìŠ¤í…'); axes[1].set_ylabel('í™œì„±í™”ê°’'); axes[1].legend()\n",
    "\n",
    "plt.suptitle('Vanilla RNN ìˆ˜ë™ ìˆœì „íŒŒ: h_t = tanh(W_ihÂ·x_t + W_hhÂ·h_{t-1} + b)', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\níŒŒë¼ë¯¸í„° êµ¬ì¡°:')\n",
    "print(f'  W_ih : {W_ih.shape}  (hidden_size Ã— input_size)')\n",
    "print(f'  W_hh : {W_hh.shape}  (hidden_size Ã— hidden_size) â† ìˆœí™˜ ì—°ê²°')\n",
    "print(f'  b    : {b.shape}')\n",
    "print(f'  ì´ íŒŒë¼ë¯¸í„°: {W_ih.size + W_hh.size + b.size}ê°œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VanillaRNN í´ë˜ìŠ¤ ë° í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"Many-to-Many Vanilla RNN: ê° íƒ€ì„ìŠ¤í…ì˜ ë‹¤ìŒ ê°’ ì˜ˆì¸¡\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, n_layers,\n",
    "                          batch_first=True, nonlinearity='tanh')\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)   # out: (B, T, hidden)\n",
    "        return self.fc(out)    # (B, T, 1)\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \"\"\"Many-to-Many LSTM\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1, n_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0.0)\n",
    "        self.fc   = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # out: (B, T, hidden)\n",
    "        return self.fc(out)    # (B, T, 1)\n",
    "\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "    \"\"\"Many-to-Many GRU\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1, n_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout if n_layers > 1 else 0.0)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)   # out: (B, T, hidden)\n",
    "        return self.fc(out)    # (B, T, 1)\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def train_rnn(model, train_loader, X_val, y_val,\n",
    "              n_epochs=50, lr=0.001, verbose=True):\n",
    "    \"\"\"ë²”ìš© RNN í•™ìŠµ í•¨ìˆ˜ (MSE ì†ì‹¤, Gradient Clipping)\"\"\"\n",
    "    model     = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    X_v = X_val.to(device)\n",
    "    y_v = y_val.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        ep_loss = 0.\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(Xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "            optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "\n",
    "        train_losses.append(ep_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_v)\n",
    "            val_losses.append(criterion(val_pred, y_v).item())\n",
    "\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} | '\n",
    "                  f'Train MSE: {train_losses[-1]:.6f} | '\n",
    "                  f'Val MSE: {val_losses[-1]:.6f}')\n",
    "\n",
    "    model.cpu()\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def predict_future(model, seed_seq, n_steps=100):\n",
    "    \"\"\"ìê¸°íšŒê·€ ë¯¸ë˜ ì˜ˆì¸¡: seed_seq (seq_len, 1) í…ì„œë¡œ n_steps ì˜ˆì¸¡\"\"\"\n",
    "    model.eval()\n",
    "    preds   = []\n",
    "    current = seed_seq.unsqueeze(0).to(device)  # (1, seq_len, 1)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_steps):\n",
    "            out  = model(current)          # (1, seq_len, 1)\n",
    "            next_val = out[:, -1:, :]      # (1, 1, 1) â€” ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ë§Œ\n",
    "            preds.append(next_val.squeeze().item())\n",
    "            current = torch.cat([current[:, 1:, :], next_val], dim=1)  # ì‹œí€€ìŠ¤ í•œ ì¹¸ ì´ë™\n",
    "    model.cpu()\n",
    "    return preds\n",
    "\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
    "for cls, name in [(VanillaRNN, 'VanillaRNN'), (LSTMNet, 'LSTMNet'), (GRUNet, 'GRUNet')]:\n",
    "    m = cls(hidden_size=32)\n",
    "    print(f'{name:<12}: {count_params(m):,} íŒŒë¼ë¯¸í„°')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla RNN í•™ìŠµ â€” ì‚¬ì¸íŒŒ ë‹¤ìŒ ìŠ¤í… ì˜ˆì¸¡\n",
    "print('=== Vanilla RNN í•™ìŠµ (50 ì—í¬í¬) ===')\n",
    "torch.manual_seed(42)\n",
    "model_rnn = VanillaRNN(hidden_size=32)\n",
    "rnn_tr_loss, rnn_val_loss = train_rnn(\n",
    "    model_rnn, train_ldr, X_te, y_te, n_epochs=50\n",
    ")\n",
    "\n",
    "# ì˜ˆì¸¡ ì‹œê°í™”\n",
    "model_rnn.eval()\n",
    "with torch.no_grad():\n",
    "    pred_rnn = model_rnn(X_te).squeeze(-1).numpy()  # (N, seq_len)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ ì²˜ìŒ 200 íƒ€ì„ìŠ¤í… ì‹œê°í™”\n",
    "true_vals = y_te[:, :, 0].numpy().flatten()[:200]\n",
    "pred_vals = pred_rnn.flatten()[:200]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(range(1, 51), rnn_tr_loss,  'b-', lw=2, label='Train MSE')\n",
    "axes[0].plot(range(1, 51), rnn_val_loss, 'r-', lw=2, label='Val MSE')\n",
    "axes[0].set_title('í•™ìŠµ ê³¡ì„ '); axes[0].set_xlabel('ì—í¬í¬')\n",
    "axes[0].set_ylabel('MSE'); axes[0].legend()\n",
    "\n",
    "axes[1].plot(true_vals, 'b-', lw=1.5, alpha=0.8, label='ì‹¤ì œ')\n",
    "axes[1].plot(pred_vals, 'r--', lw=1.5, alpha=0.8, label='ì˜ˆì¸¡ (RNN)')\n",
    "axes[1].set_title('í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ (ì²˜ìŒ 200 íƒ€ì„ìŠ¤í…)')\n",
    "axes[1].set_xlabel('íƒ€ì„ìŠ¤í…'); axes[1].set_ylabel('ê°’'); axes[1].legend()\n",
    "\n",
    "plt.suptitle(f'Vanilla RNN ì‚¬ì¸íŒŒ ì˜ˆì¸¡ (Val MSE: {rnn_val_loss[-1]:.6f})', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ (Vanishing Gradient) ì‹œê°í™”\n",
    "# ëœë¤ ì´ˆê¸°í™”ëœ RNN/LSTMì—ì„œ ë§ˆì§€ë§‰ ì¶œë ¥ì˜ ê° ì…ë ¥ íƒ€ì„ìŠ¤í…ë³„ ê·¸ë˜ë””ì–¸íŠ¸ ì¸¡ì •\n",
    "\n",
    "SEQ_GRAD = 60  # ê¸´ ì‹œí€€ìŠ¤\n",
    "torch.manual_seed(0)\n",
    "\n",
    "rnn_vg  = nn.RNN(1, 32, batch_first=True)\n",
    "lstm_vg = nn.LSTM(1, 32, batch_first=True)\n",
    "gru_vg  = nn.GRU(1, 32, batch_first=True)\n",
    "\n",
    "results_vg = {}\n",
    "\n",
    "for model_layer, name in [(rnn_vg, 'Vanilla RNN'), (lstm_vg, 'LSTM'), (gru_vg, 'GRU')]:\n",
    "    x = torch.randn(1, SEQ_GRAD, 1, requires_grad=True)\n",
    "    out, _ = model_layer(x)\n",
    "    # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì¶œë ¥ì˜ í•©ì„ ìŠ¤ì¹¼ë¼ ì†ì‹¤ë¡œ ì‚¬ìš©\n",
    "    out[:, -1, :].sum().backward()\n",
    "    # ê° ì…ë ¥ íƒ€ì„ìŠ¤í…ì—ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°\n",
    "    grad_norms = x.grad[0, :, 0].abs().detach().numpy()\n",
    "    results_vg[name] = grad_norms\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "colors_vg = {'Vanilla RNN': 'tomato', 'LSTM': 'seagreen', 'GRU': 'steelblue'}\n",
    "\n",
    "# ì„ í˜• ìŠ¤ì¼€ì¼\n",
    "for name, grads in results_vg.items():\n",
    "    axes[0].plot(grads, color=colors_vg[name], lw=2, label=name)\n",
    "axes[0].set_title('ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° (ì„ í˜• ìŠ¤ì¼€ì¼)')\n",
    "axes[0].set_xlabel('ì…ë ¥ íƒ€ì„ìŠ¤í… t'); axes[0].set_ylabel('|âˆ‚L/âˆ‚x_t|')\n",
    "axes[0].legend()\n",
    "\n",
    "# ë¡œê·¸ ìŠ¤ì¼€ì¼ (ì†Œì‹¤ íš¨ê³¼ ë” ì˜ ë³´ì„)\n",
    "for name, grads in results_vg.items():\n",
    "    axes[1].semilogy(grads + 1e-20, color=colors_vg[name], lw=2, label=name)\n",
    "axes[1].set_title('ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° (ë¡œê·¸ ìŠ¤ì¼€ì¼)')\n",
    "axes[1].set_xlabel('ì…ë ¥ íƒ€ì„ìŠ¤í… t'); axes[1].set_ylabel('|âˆ‚L/âˆ‚x_t| (log)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(f'ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¹„êµ (seq_len={SEQ_GRAD})\\n'\n",
    "             'ì´ˆê¸° íƒ€ì„ìŠ¤í…(ì™¼ìª½)ì¼ìˆ˜ë¡ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì‘ë‹¤ = í•™ìŠµì´ ì–´ë µë‹¤', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('=== ì´ˆê¸° 10 íƒ€ì„ìŠ¤í… í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ===')\n",
    "for name, grads in results_vg.items():\n",
    "    print(f'  {name:<12}: {grads[:10].mean():.2e}')\n",
    "\n",
    "print()\n",
    "print('ğŸ“Œ í•µì‹¬:')\n",
    "print('  - Vanilla RNN: ì´ˆê¸° íƒ€ì„ìŠ¤í… ê·¸ë˜ë””ì–¸íŠ¸ â‰ˆ 0 â†’ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ë¶ˆê°€')\n",
    "print('  - LSTM/GRU   : ì…€ ìƒíƒœ(ë˜ëŠ” ê²Œì´íŠ¸)ê°€ ê·¸ë˜ë””ì–¸íŠ¸ \"í†µë¡œ\" ì—­í•  â†’ ì¥ê¸° ì˜ì¡´ì„± ìœ ì§€')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. LSTM (Long Short-Term Memory)\n",
    "\n",
    "### 2-1. LSTM ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜\n",
    "\n",
    "LSTMì€ **ì…€ ìƒíƒœ(Cell State) $C_t$** ë¼ëŠ” ë³„ë„ì˜ ì •ë³´ íë¦„ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "3ê°€ì§€ ê²Œì´íŠ¸ê°€ ì •ë³´ì˜ íë¦„ì„ ì œì–´í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad \\text{(ë§ê° ê²Œì´íŠ¸: ë¬´ì—‡ì„ ìŠì„ì§€)}$$\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad \\text{(ì…ë ¥ ê²Œì´íŠ¸: ë¬´ì—‡ì„ ì €ì¥í• ì§€)}$$\n",
    "$$g_t = \\tanh(W_g \\cdot [h_{t-1}, x_t] + b_g) \\quad \\text{(í›„ë³´ ê°’)}$$\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad \\text{(ì¶œë ¥ ê²Œì´íŠ¸: ë¬´ì—‡ì„ ì¶œë ¥í• ì§€)}$$\n",
    "\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot g_t \\quad \\text{(ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸)}$$\n",
    "$$h_t = o_t \\odot \\tanh(C_t) \\quad \\text{(ì€ë‹‰ ìƒíƒœ)}$$\n",
    "\n",
    "**í•µì‹¬**: $C_t$ëŠ” **ë§ì…ˆ ì—°ì‚°**ìœ¼ë¡œ ì—…ë°ì´íŠ¸ â†’ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì‚¬ë¼ì§€ì§€ ì•Šê³  íë¥¼ ìˆ˜ ìˆìŒ!\n",
    "\n",
    "| í•­ëª© | Vanilla RNN | LSTM |\n",
    "|---|---|---|\n",
    "| ìƒíƒœ | $h_t$ 1ê°œ | $h_t$ + $C_t$ 2ê°œ |\n",
    "| ê²Œì´íŠ¸ | ì—†ìŒ | 3ê°œ (forget, input, output) |\n",
    "| íŒŒë¼ë¯¸í„° | $4 \\times \\text{hidden}^2$ | $16 \\times \\text{hidden}^2$ (4ë°°) |\n",
    "| ì¥ê¸° ì˜ì¡´ì„± | ì·¨ì•½ | ê°•í•¨ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM í•™ìŠµ â€” ì‚¬ì¸íŒŒ ì˜ˆì¸¡\n",
    "print('=== LSTM í•™ìŠµ (50 ì—í¬í¬) ===')\n",
    "torch.manual_seed(42)\n",
    "model_lstm = LSTMNet(hidden_size=32)\n",
    "lstm_tr_loss, lstm_val_loss = train_rnn(\n",
    "    model_lstm, train_ldr, X_te, y_te, n_epochs=50\n",
    ")\n",
    "\n",
    "# ë¹„êµ ì‹œê°í™”\n",
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    pred_lstm = model_lstm(X_te).squeeze(-1).numpy()\n",
    "\n",
    "ep = range(1, 51)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„ \n",
    "axes[0].plot(ep, rnn_tr_loss,   'tomato',   lw=2, ls='--', label='RNN Train')\n",
    "axes[0].plot(ep, rnn_val_loss,  'tomato',   lw=2, label='RNN Val')\n",
    "axes[0].plot(ep, lstm_tr_loss,  'seagreen', lw=2, ls='--', label='LSTM Train')\n",
    "axes[0].plot(ep, lstm_val_loss, 'seagreen', lw=2, label='LSTM Val')\n",
    "axes[0].set_title('í•™ìŠµ ê³¡ì„  (MSE)'); axes[0].set_xlabel('ì—í¬í¬')\n",
    "axes[0].set_ylabel('MSE'); axes[0].legend(fontsize=8)\n",
    "\n",
    "# RNN ì˜ˆì¸¡\n",
    "n_show = 150\n",
    "true_flat = y_te[:, :, 0].numpy().flatten()[:n_show]\n",
    "axes[1].plot(true_flat,                   'b-',  lw=1.5, label='ì‹¤ì œ')\n",
    "axes[1].plot(pred_rnn.flatten()[:n_show], 'r--', lw=1.5, label=f'RNN ({rnn_val_loss[-1]:.5f})')\n",
    "axes[1].set_title('RNN ì˜ˆì¸¡'); axes[1].set_xlabel('íƒ€ì„ìŠ¤í…'); axes[1].legend(fontsize=9)\n",
    "\n",
    "# LSTM ì˜ˆì¸¡\n",
    "axes[2].plot(true_flat,                    'b-',  lw=1.5, label='ì‹¤ì œ')\n",
    "axes[2].plot(pred_lstm.flatten()[:n_show], 'g--', lw=1.5, label=f'LSTM ({lstm_val_loss[-1]:.5f})')\n",
    "axes[2].set_title('LSTM ì˜ˆì¸¡'); axes[2].set_xlabel('íƒ€ì„ìŠ¤í…'); axes[2].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Vanilla RNN vs LSTM â€” ì‚¬ì¸íŒŒ ì˜ˆì¸¡ ë¹„êµ', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nìµœì¢… Val MSE:')\n",
    "print(f'  RNN  : {rnn_val_loss[-1]:.6f}')\n",
    "print(f'  LSTM : {lstm_val_loss[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìê¸°íšŒê·€ ë¯¸ë˜ ì˜ˆì¸¡ (Autoregressive Multi-step Prediction)\n",
    "# ë§ˆì§€ë§‰ í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì‹œë“œë¡œ 100 ìŠ¤í… ì•ì„ ì˜ˆì¸¡\n",
    "\n",
    "seed_seq   = X_te[-1]               # (seq_len, 1) â€” ë§ˆì§€ë§‰ í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤\n",
    "n_future   = 100\n",
    "\n",
    "future_rnn  = predict_future(model_rnn,  seed_seq, n_steps=n_future)\n",
    "future_lstm = predict_future(model_lstm, seed_seq, n_steps=n_future)\n",
    "\n",
    "# ë¹„êµ ì‹œê°í™”\n",
    "# ì‹œë“œ ë’¤ì˜ ì‹¤ì œ ì‚¬ì¸íŒŒê°’\n",
    "seed_start_idx = len(sine_signal) - len(X_te) + (len(X_te) - 1) + SEQ_LEN\n",
    "true_future    = sine_signal[seed_start_idx:seed_start_idx + n_future] \\\n",
    "                 if seed_start_idx + n_future <= len(sine_signal) \\\n",
    "                 else np.full(n_future, np.nan)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "seed_vals = seed_seq[:, 0].numpy()\n",
    "\n",
    "for ax, (name, future), color in [\n",
    "    (axes[0], ('RNN',  future_rnn),  'tomato'),\n",
    "    (axes[1], ('LSTM', future_lstm), 'seagreen'),\n",
    "]:\n",
    "    # ì‹œë“œ ì‹œí€€ìŠ¤\n",
    "    ax.plot(range(SEQ_LEN), seed_vals, 'b-', lw=2, label='ì‹œë“œ (ì‹¤ì œ)')\n",
    "    # ë¯¸ë˜ ì˜ˆì¸¡\n",
    "    ax.plot(range(SEQ_LEN, SEQ_LEN + n_future), future, color=color, lw=2,\n",
    "            ls='--', label=f'{name} ì˜ˆì¸¡')\n",
    "    # ì‹¤ì œ ë¯¸ë˜ê°’ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "    if not np.isnan(true_future).all():\n",
    "        ax.plot(range(SEQ_LEN, SEQ_LEN + n_future), true_future,\n",
    "                'gray', lw=1.5, alpha=0.7, label='ì‹¤ì œ ë¯¸ë˜')\n",
    "    ax.axvline(x=SEQ_LEN, color='black', ls=':', lw=1.5, label='ì˜ˆì¸¡ ì‹œì‘')\n",
    "    ax.set_title(f'{name}: {n_future}ìŠ¤í… ë¯¸ë˜ ì˜ˆì¸¡')\n",
    "    ax.set_xlabel('íƒ€ì„ìŠ¤í…'); ax.set_ylabel('ê°’'); ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('ìê¸°íšŒê·€ ë¯¸ë˜ ì˜ˆì¸¡: RNN vs LSTM', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('ğŸ“Œ LSTMì€ ì¥ê¸° íŒ¨í„´(ì‚¬ì¸íŒŒ ì£¼ê¸°)ì„ ë” ì˜ ìœ ì§€í•©ë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. GRU (Gated Recurrent Unit)\n",
    "\n",
    "### 3-1. GRU: LSTMì˜ ê²½ëŸ‰í™” ë²„ì „\n",
    "\n",
    "2014ë…„ Cho et al.ì´ ì œì•ˆí•œ GRUëŠ” LSTMì„ **ë‹¨ìˆœí™”**í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¤„ì…ë‹ˆë‹¤.\n",
    "\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t]) \\quad \\text{(ë¦¬ì…‹ ê²Œì´íŠ¸: ê³¼ê±°ë¥¼ ì–¼ë§ˆë‚˜ ë¦¬ì…‹)}$$\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t]) \\quad \\text{(ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸: ì–¼ë§ˆë‚˜ ì—…ë°ì´íŠ¸)}$$\n",
    "$$\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1},\\; x_t]) \\quad \\text{(í›„ë³´ ì€ë‹‰ ìƒíƒœ)}$$\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "| ë¹„êµ | LSTM | GRU |\n",
    "|---|---|---|\n",
    "| ìƒíƒœ | $h_t$ + $C_t$ | $h_t$ ë§Œ |\n",
    "| ê²Œì´íŠ¸ ìˆ˜ | 3ê°œ (forget, input, output) | 2ê°œ (reset, update) |\n",
    "| íŒŒë¼ë¯¸í„° | $4 \\times 4 \\times H^2$ | $3 \\times 4 \\times H^2$ (25% ì ˆê°) |\n",
    "| ì„±ëŠ¥ | ë³µì¡í•œ íŒ¨í„´ì— ìœ ë¦¬ | ì‘ì€ ë°ì´í„°, ë¹ ë¥¸ í•™ìŠµì— ìœ ë¦¬ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU í•™ìŠµ ë° 3ëª¨ë¸ ë¹„êµ (ì‚¬ì¸íŒŒ)\n",
    "print('=== GRU í•™ìŠµ (50 ì—í¬í¬) ===')\n",
    "torch.manual_seed(42)\n",
    "model_gru = GRUNet(hidden_size=32)\n",
    "gru_tr_loss, gru_val_loss = train_rnn(\n",
    "    model_gru, train_ldr, X_te, y_te, n_epochs=50\n",
    ")\n",
    "\n",
    "# 3ëª¨ë¸ ë¹„êµ ì‹œê°í™”\n",
    "model_gru.eval()\n",
    "with torch.no_grad():\n",
    "    pred_gru = model_gru(X_te).squeeze(-1).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ep = range(1, 51)\n",
    "\n",
    "for (name, tl, vl, color) in [\n",
    "    ('RNN',  rnn_tr_loss,  rnn_val_loss,  'tomato'),\n",
    "    ('LSTM', lstm_tr_loss, lstm_val_loss, 'seagreen'),\n",
    "    ('GRU',  gru_tr_loss,  gru_val_loss,  'steelblue'),\n",
    "]:\n",
    "    axes[0].plot(ep, tl, color=color, lw=1.5, ls='--', alpha=0.6)\n",
    "    axes[0].plot(ep, vl, color=color, lw=2, label=f'{name} Val ({vl[-1]:.5f})')\n",
    "\n",
    "axes[0].set_title('3ëª¨ë¸ í•™ìŠµ ê³¡ì„  (Val MSE, â€”)')\n",
    "axes[0].set_xlabel('ì—í¬í¬'); axes[0].set_ylabel('MSE')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "n_show = 150\n",
    "true_flat = y_te[:, :, 0].numpy().flatten()[:n_show]\n",
    "axes[1].plot(true_flat,                    'b-',  lw=2,   label='ì‹¤ì œ')\n",
    "axes[1].plot(pred_rnn.flatten()[:n_show],  'r--', lw=1.5, label='RNN')\n",
    "axes[1].plot(pred_lstm.flatten()[:n_show], 'g:',  lw=2,   label='LSTM')\n",
    "axes[1].plot(pred_gru.flatten()[:n_show],  color='steelblue', ls='-.', lw=1.5, label='GRU')\n",
    "axes[1].set_title('í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ ë¹„êµ'); axes[1].set_xlabel('íƒ€ì„ìŠ¤í…')\n",
    "axes[1].set_ylabel('ê°’'); axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Vanilla RNN vs LSTM vs GRU â€” ì‚¬ì¸íŒŒ ì˜ˆì¸¡', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì„±ëŠ¥ ìš”ì•½ í‘œ\n",
    "print(f'\\n{\"ëª¨ë¸\":<10} {\"Val MSE\":>12} {\"íŒŒë¼ë¯¸í„°\":>10}')\n",
    "print('-' * 36)\n",
    "for name, vl, m in [\n",
    "    ('RNN',  rnn_val_loss,  model_rnn),\n",
    "    ('LSTM', lstm_val_loss, model_lstm),\n",
    "    ('GRU',  gru_val_loss,  model_gru),\n",
    "]:\n",
    "    mark = ' â˜…' if vl[-1] == min(rnn_val_loss[-1], lstm_val_loss[-1], gru_val_loss[-1]) else ''\n",
    "    print(f'{name:<10} {vl[-1]:>12.6f} {count_params(m):>10,}{mark}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³µì¡ ì‹ í˜¸(ë‹¤ì¤‘ ì£¼íŒŒìˆ˜)ì—ì„œ 3ëª¨ë¸ ë¹„êµ\n",
    "# sin(t) + 0.5Â·sin(3t) + 0.3Â·sin(7t) + noise\n",
    "\n",
    "X_cx, y_cx = make_sequences(complex_signal)\n",
    "X_cx_tr, y_cx_tr, X_cx_te, y_cx_te = split_data(X_cx, y_cx)\n",
    "\n",
    "cx_ds  = TensorDataset(X_cx_tr, y_cx_tr)\n",
    "cx_ldr = DataLoader(cx_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "cx_results = {}\n",
    "print('=== ë³µì¡ ì‹ í˜¸ í•™ìŠµ (50 ì—í¬í¬) ===\\n')\n",
    "for cls, name in [(VanillaRNN, 'RNN'), (LSTMNet, 'LSTM'), (GRUNet, 'GRU')]:\n",
    "    torch.manual_seed(42)\n",
    "    m = cls(hidden_size=64)\n",
    "    print(f'--- {name} ---')\n",
    "    tl, vl = train_rnn(m, cx_ldr, X_cx_te, y_cx_te, n_epochs=50)\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = m(X_cx_te).squeeze(-1).numpy()\n",
    "    cx_results[name] = (tl, vl, pred, m)\n",
    "    print()\n",
    "\n",
    "# ì‹œê°í™”\n",
    "n_show = 200\n",
    "true_cx = y_cx_te[:, :, 0].numpy().flatten()[:n_show]\n",
    "ep = range(1, 51)\n",
    "colors_cx = {'RNN': 'tomato', 'LSTM': 'seagreen', 'GRU': 'steelblue'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, (tl, vl, pred, m) in cx_results.items():\n",
    "    axes[0].plot(ep, vl, color=colors_cx[name], lw=2, label=f'{name} ({vl[-1]:.5f})')\n",
    "\n",
    "axes[0].set_title('ë³µì¡ ì‹ í˜¸ Val MSE')\n",
    "axes[0].set_xlabel('ì—í¬í¬'); axes[0].set_ylabel('MSE')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "axes[1].plot(true_cx, 'b-', lw=2, alpha=0.9, label='ì‹¤ì œ')\n",
    "for name, (_, vl, pred, _) in cx_results.items():\n",
    "    axes[1].plot(pred.flatten()[:n_show], color=colors_cx[name], lw=1.5,\n",
    "                 ls='--', label=f'{name} ({vl[-1]:.5f})')\n",
    "axes[1].set_title('ë³µì¡ ì‹ í˜¸ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡')\n",
    "axes[1].set_xlabel('íƒ€ì„ìŠ¤í…'); axes[1].set_ylabel('ê°’')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('ë³µì¡ ì‹ í˜¸: sin(t)+0.5Â·sin(3t)+0.3Â·sin(7t)+noise', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n{\"ëª¨ë¸\":<6} {\"Val MSE (ë³µì¡ ì‹ í˜¸)\":>22} {\"íŒŒë¼ë¯¸í„°\":>10}')\n",
    "print('-' * 44)\n",
    "for name, (_, vl, _, m) in cx_results.items():\n",
    "    print(f'{name:<6} {vl[-1]:>22.6f} {count_params(m):>10,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise\n",
    "\n",
    "### Exercise 1. LSTM í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "\n",
    "`hidden_size`ì™€ `n_layers`ë¥¼ ì¡°í•©í•˜ì—¬ ìµœì  LSTM ì„¤ì •ì„ ì°¾ìœ¼ì„¸ìš”.\n",
    "\n",
    "| hidden_size | n_layers | ë¹„ê³  |\n",
    "|---|---|---|\n",
    "| 16 | 1 | |\n",
    "| 32 | 1 | (Part 2ì™€ ë™ì¼) |\n",
    "| 64 | 1 | |\n",
    "| 16 | 2 | |\n",
    "| 32 | 2 | |\n",
    "| 64 | 2 | |\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "- ë°ì´í„°: ì‚¬ì¸íŒŒ (`train_ldr`, `X_te`, `y_te`)\n",
    "- ë™ì¼ ì¡°ê±´: n_epochs=50, Adam, lr=0.001\n",
    "- ìµœì¢… Val MSE + íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í‘œë¡œ ì¶œë ¥\n",
    "- ê²°ê³¼ë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ ë˜ëŠ” íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: LSTM í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "\n",
    "hp_configs = [\n",
    "    {'hidden_size': 16, 'n_layers': 1},\n",
    "    {'hidden_size': 32, 'n_layers': 1},\n",
    "    {'hidden_size': 64, 'n_layers': 1},\n",
    "    {'hidden_size': 16, 'n_layers': 2},\n",
    "    {'hidden_size': 32, 'n_layers': 2},\n",
    "    {'hidden_size': 64, 'n_layers': 2},\n",
    "]\n",
    "\n",
    "hp_results = {}\n",
    "\n",
    "for cfg in hp_configs:\n",
    "    key = f'h={cfg[\"hidden_size\"]}, L={cfg[\"n_layers\"]}'\n",
    "    # Your code here: LSTMNet ìƒì„± ë° í•™ìŠµ\n",
    "    # torch.manual_seed(42)\n",
    "    # m = LSTMNet(**cfg)\n",
    "    # tl, vl = train_rnn(m, train_ldr, X_te, y_te, n_epochs=50, verbose=False)\n",
    "    # hp_results[key] = (vl[-1], count_params(m))\n",
    "    pass\n",
    "\n",
    "# Your code here: ê²°ê³¼ í‘œ ì¶œë ¥\n",
    "# print(f'{\"ì„¤ì •\":<20} {\"Val MSE\":>12} {\"íŒŒë¼ë¯¸í„°\":>10}')\n",
    "# ...\n",
    "\n",
    "# Your code here: ë§‰ëŒ€ê·¸ë˜í”„ ì‹œê°í™” (x=ì„¤ì •, y=Val MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 2. ë³µì¡ ì‹ í˜¸ ìê¸°íšŒê·€ ì˜ˆì¸¡\n",
    "\n",
    "ë³µì¡ ì‹ í˜¸(`complex_signal`)ì—ì„œ ìµœì  LSTMì„ í•™ìŠµí•˜ê³  **100ìŠ¤í… ë¯¸ë˜ë¥¼ ìê¸°íšŒê·€ ë°©ì‹**ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì„¸ìš”.\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "1. `cx_ldr`, `X_cx_te`, `y_cx_te`ë¡œ LSTM í•™ìŠµ (n_epochs=50, hidden_size=64)\n",
    "2. `predict_future()` í•¨ìˆ˜ë¡œ 100ìŠ¤í… ë¯¸ë˜ ì˜ˆì¸¡\n",
    "3. ì‹œê°í™”: Train/Val Loss ê³¡ì„  + ì‹œë“œ ì‹œí€€ìŠ¤ + ì˜ˆì¸¡ ë¯¸ë˜ê°’\n",
    "4. ì‚¬ì¸íŒŒ ì˜ˆì¸¡ ë•Œì™€ ë¹„êµ: ë” ì–´ë µìŠµë‹ˆê¹Œ, ì‰½ìŠµë‹ˆê¹Œ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: ë³µì¡ ì‹ í˜¸ ìê¸°íšŒê·€ ì˜ˆì¸¡\n",
    "\n",
    "# Your code here: LSTM í•™ìŠµ (cx_ldr, X_cx_te, y_cx_te, hidden_size=64)\n",
    "torch.manual_seed(42)\n",
    "model_cx_lstm = LSTMNet(hidden_size=64)\n",
    "\n",
    "# cx_tl, cx_vl = train_rnn(model_cx_lstm, cx_ldr, X_cx_te, y_cx_te, n_epochs=50)\n",
    "\n",
    "# Your code here: ë§ˆì§€ë§‰ í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ 100ìŠ¤í… ë¯¸ë˜ ì˜ˆì¸¡\n",
    "# seed = X_cx_te[-1]  # (seq_len, 1)\n",
    "# future_cx = predict_future(model_cx_lstm, seed, n_steps=100)\n",
    "\n",
    "# Your code here: ì‹œê°í™”\n",
    "# - ì™¼ìª½: Train/Val Loss ê³¡ì„ \n",
    "# - ì˜¤ë¥¸ìª½: ì‹œë“œ(íŒŒë€ìƒ‰) + ì˜ˆì¸¡ ë¯¸ë˜(ë¹¨ê°„ ì ì„ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 3. (ë„ì „) ì‹œí€€ìŠ¤ ë¶„ë¥˜ â€” LSTM Classifier\n",
    "\n",
    "**ì¶”ì„¸ ìƒìŠ¹(â†‘) vs ì¶”ì„¸ í•˜ê°•(â†“)** ì´ì§„ ë¶„ë¥˜ ê³¼ì œë¥¼ LSTMìœ¼ë¡œ í’€ì–´ë³´ì„¸ìš”.\n",
    "\n",
    "**ë°ì´í„°**: í•©ì„± ì‹œí€€ìŠ¤ (seq_len=30, noise í¬í•¨)\n",
    "```\n",
    "í´ë˜ìŠ¤ 1 (ìƒìŠ¹): ëˆ„ì í•© + noise â†’ ì „ì²´ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” íŒ¨í„´\n",
    "í´ë˜ìŠ¤ 0 (í•˜ê°•): -(ëˆ„ì í•©) + noise â†’ ì „ì²´ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” íŒ¨í„´\n",
    "```\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "1. `generate_trend_sequences()` í•¨ìˆ˜ êµ¬í˜„ (1000 ìƒ˜í”Œ ìƒì„±)\n",
    "2. `LSTMClassifier` í´ë˜ìŠ¤ êµ¬í˜„: ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœë¡œ ë¶„ë¥˜ (many-to-one)\n",
    "3. `CrossEntropyLoss`ë¡œ í•™ìŠµ (n_epochs=30)\n",
    "4. í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì¶œë ¥ ë° í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
    "\n",
    "**íŒíŠ¸:**\n",
    "```python\n",
    "# many-to-one: ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©\n",
    "out, (h, c) = self.lstm(x)\n",
    "return self.fc(out[:, -1, :])   # (B, n_classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: ì‹œí€€ìŠ¤ ë¶„ë¥˜\n",
    "\n",
    "def generate_trend_sequences(n_samples=1000, seq_len=30, noise=0.3):\n",
    "    \"\"\"\n",
    "    ì¶”ì„¸ ìƒìŠ¹(label=1) / ì¶”ì„¸ í•˜ê°•(label=0) ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    Returns: X (N, seq_len, 1), y (N,) LongTensor\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # - trend = +1 ë˜ëŠ” -1 (0.5 í™•ë¥ )\n",
    "    # - signal = trend * cumsum(abs(randn(seq_len))) / seq_len + noise * randn(seq_len)\n",
    "    # - ê° ì‹œí€€ìŠ¤ë¥¼ í‘œì¤€í™” (mean=0, std=1)\n",
    "    # - label: trend > 0 ì´ë©´ 1, ì•„ë‹ˆë©´ 0\n",
    "    pass\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Many-to-One LSTM ë¶„ë¥˜ê¸°: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¡œ í´ë˜ìŠ¤ ì˜ˆì¸¡\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=32, n_layers=1, n_classes=2):\n",
    "        super().__init__()\n",
    "        # Your code here: nn.LSTM + nn.Linear\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here: LSTM â†’ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì¶œë ¥ â†’ Linear\n",
    "        pass\n",
    "\n",
    "\n",
    "# Your code here:\n",
    "# 1. generate_trend_sequences()ë¡œ ë°ì´í„° ìƒì„± (train 800, test 200)\n",
    "# 2. LSTMClassifier ìƒì„± ë° CrossEntropyLossë¡œ í•™ìŠµ (n_epochs=30)\n",
    "# 3. í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì¶œë ¥\n",
    "# 4. í˜¼ë™ í–‰ë ¬(confusion matrix) ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| ê°œë… | í•µì‹¬ ë‚´ìš© |\n",
    "|---|---|\n",
    "| **Vanilla RNN** | $h_t = \\tanh(W_{ih}x_t + W_{hh}h_{t-1} + b)$; ê°€ì¤‘ì¹˜ ê³µìœ , BPTT |\n",
    "| **ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤** | $W_{hh}$ë¥¼ ë°˜ë³µ ê³± â†’ ì´ˆê¸° íƒ€ì„ìŠ¤í… ê·¸ë˜ë””ì–¸íŠ¸ â‰ˆ 0; ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ë¶ˆê°€ |\n",
    "| **LSTM** | ì…€ ìƒíƒœ $C_t$ + 3ê²Œì´íŠ¸(forget/input/output); ë§ì…ˆ ì—…ë°ì´íŠ¸ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ë³´ì¡´ |\n",
    "| **ë§ê° ê²Œì´íŠ¸** | $f_t = \\sigma(\\cdot)$; ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ |\n",
    "| **GRU** | LSTM ë‹¨ìˆœí™”; 2ê²Œì´íŠ¸(reset/update), ì…€ ìƒíƒœ ì—†ìŒ, íŒŒë¼ë¯¸í„° 25% ì ˆê° |\n",
    "| **Many-to-Many** | ê° íƒ€ì„ìŠ¤í… ì¶œë ¥ ì‚¬ìš© â†’ ì‹œí€€ìŠ¤ ì˜ˆì¸¡ (ì‹œê³„ì—´, ë²ˆì—­) |\n",
    "| **Many-to-One** | ë§ˆì§€ë§‰ ì¶œë ¥ë§Œ ì‚¬ìš© â†’ ì‹œí€€ìŠ¤ ë¶„ë¥˜ (ê°ì„± ë¶„ì„, ì¶”ì„¸ ë¶„ë¥˜) |\n",
    "| **ìê¸°íšŒê·€ ì˜ˆì¸¡** | ì˜ˆì¸¡ê°’ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì¬ì‚¬ìš© â†’ ì¥ê¸° ë¯¸ë˜ ì˜ˆì¸¡ (ì˜¤ì°¨ ëˆ„ì ) |\n",
    "| **Gradient Clipping** | `clip_grad_norm_(..., 1.0)`; RNN ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€ |\n",
    "| **Seq2Seq** | Encoder(RNN) â†’ context vector â†’ Decoder(RNN); ë²ˆì—­/ìš”ì•½ ê¸°ë°˜ ì•„ì´ë””ì–´ |\n",
    "\n",
    "---\n",
    "\n",
    "**ë‹¤ìŒ ê°•ì˜ (Week 14):** Transformer â€” Attention ë©”ì»¤ë‹ˆì¦˜, Self-Attention, BERT/GPT ê°œìš”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
