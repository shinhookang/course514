{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 15 â€” Transfer Learning & Modern AI\n",
    "\n",
    "> **ê°•ì˜ ì‹œê°„:** ì•½ 2ì‹œê°„  \n",
    "> **ì£¼ì œ:** ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì¬ì‚¬ìš© â€” Feature Extraction, Fine-tuning, ë°ì´í„° íš¨ìœ¨ì„±\n",
    "\n",
    "---\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "| # | ëª©í‘œ | ì˜ˆìƒ ì‹œê°„ |\n",
    "|---|---|---|\n",
    "| 1 | ì‚¬ì „í•™ìŠµ ResNet18 êµ¬ì¡° íƒìƒ‰ + Feature Extraction | 30ë¶„ |\n",
    "| 2 | Fine-tuning ì „ëµ â€” ì°¨ë“± LR, Unfreeze ë¹„êµ | 30ë¶„ |\n",
    "| 3 | ë°ì´í„° íš¨ìœ¨ì„± ë¶„ì„ + ì˜¤ë¥˜ ë¶„ì„ | 30ë¶„ |\n",
    "| 4 | Exercise | 30ë¶„ |\n",
    "\n",
    "---\n",
    "\n",
    "**ë°ì´í„°ì…‹:** PIL í•©ì„± ë„í˜• ì´ë¯¸ì§€ (ì¸í„°ë„· ë¶ˆí•„ìš”, ì½”ë“œ ë‚´ ìë™ ìƒì„±)  \n",
    "- 3 í´ë˜ìŠ¤: `circle`, `square`, `triangle`  \n",
    "- Train 300/class (900ê°œ) Â· Test 100/class (300ê°œ), 96Ã—96 PIL â†’ Resize(224)  \n",
    "\n",
    "> âš ï¸ **ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ:** ìµœì´ˆ ì‹¤í–‰ ì‹œ ResNet18 ê°€ì¤‘ì¹˜(~45MB)ê°€ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.  \n",
    "> GPU/MPS ì‚¬ìš© ê¶Œì¥ â€” CPUì—ì„œ fine-tuningì€ 10~15ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'PyTorch     : {torch.__version__}')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# ImageNet ì •ê·œí™” ìƒìˆ˜ (pretrained ResNet í•„ìˆ˜)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ í•©ì„± ë„í˜• ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (PIL ìƒì„±, ì¸í„°ë„· ë¶ˆí•„ìš”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CLASS_NAMES  = ['circle', 'square', 'triangle']\n",
    "N_TRAIN_PER  = 300   # í´ë˜ìŠ¤ë‹¹ í•™ìŠµ ìƒ˜í”Œ ìˆ˜\n",
    "N_TEST_PER   = 100   # í´ë˜ìŠ¤ë‹¹ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "def make_shape_image(label, size=96, seed=None):\n",
    "    \"\"\"PIL ë„í˜• ì´ë¯¸ì§€ ìƒì„± (label: 0=circle, 1=square, 2=triangle)\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    bg  = tuple(int(v) for v in rng.randint(190, 241, 3))\n",
    "    img = Image.new('RGB', (size, size), bg)\n",
    "    drw = ImageDraw.Draw(img)\n",
    "    col = tuple(int(v) for v in rng.randint(30, 151, 3))\n",
    "    lo  = size // 4 + size // 5\n",
    "    hi  = size - lo\n",
    "    cx  = int(rng.randint(lo, max(lo + 1, hi)))\n",
    "    cy  = int(rng.randint(lo, max(lo + 1, hi)))\n",
    "    r   = int(rng.randint(size // 5, size // 3 + 1))\n",
    "    if label == 0:     # Circle\n",
    "        drw.ellipse([cx-r, cy-r, cx+r, cy+r], fill=col)\n",
    "    elif label == 1:   # Square\n",
    "        drw.rectangle([cx-r, cy-r, cx+r, cy+r], fill=col)\n",
    "    else:              # Triangle\n",
    "        drw.polygon([(cx, cy-r), (cx+r, cy+r), (cx-r, cy+r)], fill=col)\n",
    "    return img\n",
    "\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "    \"\"\"PIL ë„í˜• ì´ë¯¸ì§€ Dataset â€” ë©”ëª¨ë¦¬ì— PIL ì´ë¯¸ì§€ ì €ì¥ í›„ transform ì ìš©\"\"\"\n",
    "    def __init__(self, n_per_class, split='train', transform=None, seed=42):\n",
    "        self.transform = transform if transform is not None else base_transform\n",
    "        self.imgs, self.labels = [], []\n",
    "        offset = 0 if split == 'train' else 10000  # train/test seed ë¶„ë¦¬\n",
    "        for cls in range(len(CLASS_NAMES)):\n",
    "            for i in range(n_per_class):\n",
    "                self.imgs.append(make_shape_image(cls, size=96, seed=seed + cls * 1000 + offset + i))\n",
    "                self.labels.append(cls)\n",
    "\n",
    "    def __len__(self):  return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]\n",
    "\n",
    "\n",
    "print('ë°ì´í„°ì…‹ ìƒì„± ì¤‘...')\n",
    "t0 = time.time()\n",
    "train_ds  = ShapeDataset(N_TRAIN_PER, split='train', transform=base_transform)\n",
    "test_ds   = ShapeDataset(N_TEST_PER,  split='test',  transform=base_transform)\n",
    "print(f'ì™„ë£Œ: {time.time()-t0:.1f}s  |  Train: {len(train_ds)}  Test: {len(test_ds)}')\n",
    "\n",
    "train_ldr = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "test_ldr  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# â”€â”€ ìƒ˜í”Œ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 7))\n",
    "for cls in range(3):\n",
    "    for i in range(5):\n",
    "        img = make_shape_image(cls, size=96, seed=42 + cls * 100 + i)\n",
    "        axes[cls, i].imshow(img)\n",
    "        axes[cls, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[cls, i].set_ylabel(CLASS_NAMES[cls], rotation=0, ha='right',\n",
    "                                    labelpad=50, fontsize=11)\n",
    "plt.suptitle('í•©ì„± ë„í˜• ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ìƒ˜í”Œ (3 í´ë˜ìŠ¤ Ã— 5ê°œ)', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ì •ê·œí™” í›„ í…ì„œ í™•ì¸\n",
    "t, l = train_ds[0]\n",
    "print(f'\\ní…ì„œ shape: {t.shape}  range: [{t.min():.2f}, {t.max():.2f}]')\n",
    "print(f'í´ë˜ìŠ¤: {CLASS_NAMES[l]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. ì‚¬ì „í•™ìŠµ ëª¨ë¸ & Feature Extraction\n",
    "\n",
    "### 1-1. ì™œ Transfer Learningì¸ê°€?\n",
    "\n",
    "| ìƒí™© | ì¶”ì²œ ì „ëµ |\n",
    "|---|---|\n",
    "| ë°ì´í„° ì ìŒ + ìœ ì‚¬ ë„ë©”ì¸ | Feature Extraction (backbone ê³ ì •) |\n",
    "| ë°ì´í„° ë³´í†µ + ìœ ì‚¬ ë„ë©”ì¸ | Fine-tuning (ì „ì²´ ë˜ëŠ” ì¼ë¶€ unfreeze) |\n",
    "| ë°ì´í„° ë§ìŒ + ë‹¤ë¥¸ ë„ë©”ì¸ | Fine-tuning (ì „ì²´) ë˜ëŠ” scratch |\n",
    "| ë°ì´í„° ì¶©ë¶„ + ë™ì¼ ë„ë©”ì¸ | ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì§ì ‘ ì‚¬ìš© |\n",
    "\n",
    "### 1-2. ResNet18 êµ¬ì¡°\n",
    "\n",
    "```\n",
    "ì…ë ¥ (3, 224, 224)\n",
    "  â”‚\n",
    "  conv1 (64, 7Ã—7, stride=2) â†’ BN â†’ ReLU â†’ MaxPool  : (64, 56, 56)\n",
    "  â”‚\n",
    "  layer1 : BasicBlock Ã— 2  (64 â†’ 64)                : (64, 56, 56)\n",
    "  layer2 : BasicBlock Ã— 2  (64 â†’ 128, stride=2)     : (128, 28, 28)\n",
    "  layer3 : BasicBlock Ã— 2  (128 â†’ 256, stride=2)    : (256, 14, 14)\n",
    "  layer4 : BasicBlock Ã— 2  (256 â†’ 512, stride=2)    : (512, 7, 7)\n",
    "  â”‚\n",
    "  AdaptiveAvgPool â†’ (512, 1, 1) â†’ Flatten â†’ fc(512â†’1000)\n",
    "```\n",
    "\n",
    "**Transfer Learningì—ì„œ:** fc(512â†’1000) â†’ fc(512â†’n_classes) êµì²´\n",
    "\n",
    "### 1-3. Feature Extraction ì‘ë™ ì›ë¦¬\n",
    "\n",
    "```python\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' not in name:\n",
    "        param.requires_grad = False   # backbone ë™ê²°\n",
    "# â†’ optimizerê°€ fc íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸\n",
    "```\n",
    "\n",
    "**ì¥ì :** í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° 11.7M â†’ **1,539ê°œ** (99.99% ê°ì†Œ!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ResNet18 êµ¬ì¡° íƒìƒ‰ + Conv1 í•„í„° ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ~45MB ë‹¤ìš´ë¡œë“œ)\n",
    "try:\n",
    "    _model_inspect = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "except AttributeError:  # torchvision < 0.13\n",
    "    _model_inspect = models.resnet18(pretrained=True)\n",
    "\n",
    "print('=== ResNet18 ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ===')\n",
    "total_params = 0\n",
    "layer_info = [\n",
    "    ('conv1 + bn1',  ['conv1', 'bn1']),\n",
    "    ('layer1',       ['layer1']),\n",
    "    ('layer2',       ['layer2']),\n",
    "    ('layer3',       ['layer3']),\n",
    "    ('layer4',       ['layer4']),\n",
    "    ('fc (head)',    ['fc']),\n",
    "]\n",
    "print(f'{\"ê·¸ë£¹\":<18} {\"íŒŒë¼ë¯¸í„° ìˆ˜\":>12} {\"ëˆ„ì \":>12}')\n",
    "print('-' * 46)\n",
    "for group_name, prefixes in layer_info:\n",
    "    n = sum(p.numel() for name, p in _model_inspect.named_parameters()\n",
    "            if any(name.startswith(pf) for pf in prefixes))\n",
    "    total_params += n\n",
    "    print(f'{group_name:<18} {n:>12,} {total_params:>12,}')\n",
    "print(f'{\"ì´í•©\":<18} {sum(p.numel() for p in _model_inspect.parameters()):>12,}')\n",
    "\n",
    "# â”€â”€ Conv1 í•„í„° ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "conv1_w = _model_inspect.conv1.weight.data.clone()   # (64, 3, 7, 7)\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < 32:\n",
    "        filt = conv1_w[i].permute(1, 2, 0).numpy()   # (7, 7, 3)\n",
    "        filt = (filt - filt.min()) / (filt.max() - filt.min() + 1e-8)\n",
    "        ax.imshow(filt)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('ResNet18 Conv1 ì‚¬ì „í•™ìŠµ í•„í„° (64ê°œ ì¤‘ 32ê°œ í‘œì‹œ, 7Ã—7Ã—3)\\n'\n",
    "             'ImageNetìœ¼ë¡œ í•™ìŠµëœ ì—ì§€Â·ë°©í–¥ì„±Â·ìƒ‰ìƒ íŠ¹ì§• ê°ì§€ê¸°', fontsize=11)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "del _model_inspect   # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "print('\\nğŸ’¡ ì´ í•„í„°ë“¤ì´ ë„í˜• ì´ë¯¸ì§€ì˜ ì—ì§€ì™€ ê²½ê³„ë¥¼ ê°ì§€í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ í—¬í¼ í•¨ìˆ˜ ë° Feature Extraction ì‹¤í—˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def make_pretrained_resnet18(n_classes=3):\n",
    "    \"\"\"ì‚¬ì „í•™ìŠµ ResNet18: fcë§Œ n_classesë¡œ êµì²´\"\"\"\n",
    "    try:\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    except AttributeError:\n",
    "        m = models.resnet18(pretrained=True)\n",
    "    m.fc = nn.Linear(m.fc.in_features, n_classes)\n",
    "    return m\n",
    "\n",
    "\n",
    "def make_random_resnet18(n_classes=3):\n",
    "    \"\"\"ëœë¤ ì´ˆê¸°í™” ResNet18: fcë§Œ n_classesë¡œ êµì²´\"\"\"\n",
    "    m = models.resnet18(weights=None)\n",
    "    m.fc = nn.Linear(m.fc.in_features, n_classes)\n",
    "    return m\n",
    "\n",
    "\n",
    "def set_trainable(model, layer_names=None):\n",
    "    \"\"\"\n",
    "    layer_names=None  â†’ ì „ì²´ unfreeze (ëª¨ë‘ í•™ìŠµ)\n",
    "    layer_names=[]    â†’ ì „ì²´ freeze\n",
    "    layer_names=['fc'] â†’ 'fc'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    if layer_names is None:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "        return\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = any(name.startswith(ln) for ln in layer_names)\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def count_trainable(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader,\n",
    "                n_epochs=20, lr=1e-3, optimizer_fn=None, verbose=True):\n",
    "    \"\"\"ê³µìš© í•™ìŠµ í•¨ìˆ˜: CrossEntropyLoss + Adam, requires_grad íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸\"\"\"\n",
    "    model = model.to(device)\n",
    "    if optimizer_fn is None:\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = optim.Adam(params, lr=lr, weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = optimizer_fn(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, val_accs = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        ep_loss = 0.\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(imgs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "        train_losses.append(ep_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n",
    "                correct += (model(imgs).argmax(1) == labels).sum().item()\n",
    "                total   += len(labels)\n",
    "        val_accs.append(correct / total)\n",
    "\n",
    "        if verbose and (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} | Loss: {train_losses[-1]:.4f} | '\n",
    "                  f'Val Acc: {val_accs[-1]:.4f}')\n",
    "    model.cpu()\n",
    "    return train_losses, val_accs\n",
    "\n",
    "\n",
    "def extract_backbone_features(model, loader):\n",
    "    \"\"\"fc ì´ì „ backboneìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ â†’ (N, 512) tensor\"\"\"\n",
    "    backbone = nn.Sequential(*list(model.children())[:-1])  # fc ì œê±°\n",
    "    backbone.eval().to(device)\n",
    "    feats, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            out = backbone(imgs.to(device)).squeeze(-1).squeeze(-1)  # (B, 512)\n",
    "            feats.append(out.cpu())\n",
    "            labs.append(torch.tensor(labels))\n",
    "    backbone.cpu()\n",
    "    return torch.cat(feats), torch.cat(labs)\n",
    "\n",
    "\n",
    "# â”€â”€ Feature Extraction ì‹¤í—˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('=== ì‹¤í—˜ A: Random Init (ì „ì²´ í•™ìŠµ) ===')\n",
    "torch.manual_seed(42)\n",
    "model_rand = make_random_resnet18()\n",
    "set_trainable(model_rand, None)   # ì „ì²´ í•™ìŠµ\n",
    "print(f'í•™ìŠµ íŒŒë¼ë¯¸í„°: {count_trainable(model_rand):,} / ì „ì²´: {count_params(model_rand):,}')\n",
    "rand_losses, rand_accs = train_model(model_rand, train_ldr, test_ldr, n_epochs=20)\n",
    "\n",
    "print('\\n=== ì‹¤í—˜ B: Feature Extraction (backbone ê³ ì •, headë§Œ í•™ìŠµ) ===')\n",
    "torch.manual_seed(42)\n",
    "model_fe = make_pretrained_resnet18()\n",
    "set_trainable(model_fe, ['fc'])   # fcë§Œ í•™ìŠµ\n",
    "print(f'í•™ìŠµ íŒŒë¼ë¯¸í„°: {count_trainable(model_fe):,} / ì „ì²´: {count_params(model_fe):,}')\n",
    "fe_losses, fe_accs = train_model(model_fe, train_ldr, test_ldr, n_epochs=20)\n",
    "\n",
    "# â”€â”€ ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ep = range(1, 21)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(ep, rand_losses, 'tomato',   lw=2, label='Random Init')\n",
    "axes[0].plot(ep, fe_losses,   'seagreen', lw=2, label='Feature Extraction')\n",
    "axes[0].set_title('Train Loss'); axes[0].set_xlabel('ì—í¬í¬')\n",
    "axes[0].set_ylabel('CE Loss'); axes[0].legend()\n",
    "\n",
    "axes[1].plot(ep, rand_accs, 'tomato',   lw=2, label=f'Random Init ({rand_accs[-1]:.4f})')\n",
    "axes[1].plot(ep, fe_accs,   'seagreen', lw=2, label=f'Feature Extraction ({fe_accs[-1]:.4f})')\n",
    "axes[1].axhline(y=1/3, color='gray', ls='--', lw=1, label='ë¬´ì‘ìœ„ ê¸°ì¤€ì„  (0.333)')\n",
    "axes[1].set_title('Val Accuracy'); axes[1].set_xlabel('ì—í¬í¬')\n",
    "axes[1].set_ylabel('Accuracy'); axes[1].set_ylim(0, 1.05); axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Random Init vs Feature Extraction â€” í•©ì„± ë„í˜• ë¶„ë¥˜ (20 ì—í¬í¬)', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f'\\nìµœì¢… Val Accuracy:')\n",
    "print(f'  Random Init       : {rand_accs[-1]:.4f}  ({count_trainable(model_rand):,} params í•™ìŠµ)')\n",
    "print(f'  Feature Extraction: {fe_accs[-1]:.4f}  ({count_trainable(model_fe):,} params í•™ìŠµ)')\n",
    "print(f'\\nğŸ’¡ Feature Extractionì´ íŒŒë¼ë¯¸í„°ë¥¼ {count_trainable(model_rand)//count_trainable(model_fe)}ë°° ì ê²Œ í•™ìŠµí•˜ë©´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. Fine-tuning ì „ëµ\n",
    "\n",
    "### 2-1. Fine-tuningì´ë€?\n",
    "\n",
    "Feature Extractionì€ backboneì„ ì™„ì „íˆ ê³ ì •í•©ë‹ˆë‹¤.  \n",
    "**Fine-tuning**ì€ backboneê¹Œì§€ ì—…ë°ì´íŠ¸í•˜ì—¬ ìƒˆ ë„ë©”ì¸ì— ë” ì˜ ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "| ì „ëµ | í•™ìŠµ ë ˆì´ì–´ | ì í•©í•œ ìƒí™© |\n",
    "|---|---|---|\n",
    "| Feature Extraction | fcë§Œ | ë°ì´í„° ì ìŒ, ë„ë©”ì¸ ìœ ì‚¬ |\n",
    "| Partial Fine-tuning | layer4 + fc | ë°ì´í„° ë³´í†µ, ì¼ë¶€ ì ì‘ í•„ìš” |\n",
    "| Full Fine-tuning | ì „ì²´ | ë°ì´í„° ì¶©ë¶„, ë„ë©”ì¸ ë‹¤ë¦„ |\n",
    "\n",
    "### 2-2. Differential Learning Rate (ì°¨ë“± í•™ìŠµë¥ )\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam([\n",
    "    {'params': backbone_params, 'lr': 1e-4},   # backbone: ì†Œí­ ì—…ë°ì´íŠ¸\n",
    "    {'params': head_params,     'lr': 1e-3},   # head: ë¹ ë¥¸ í•™ìŠµ\n",
    "])\n",
    "```\n",
    "\n",
    "**ì´ìœ :** backboneì€ ImageNetì—ì„œ ì´ë¯¸ ì¢‹ì€ íŠ¹ì§• í•™ìŠµ ì™„ë£Œ  \n",
    "â†’ í° LRë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ **Catastrophic Forgetting** (ê¸°ì¡´ íŠ¹ì§• ì†ìƒ) ìœ„í—˜\n",
    "\n",
    "### 2-3. Catastrophic Forgetting\n",
    "\n",
    "- ë„ˆë¬´ í° LRë¡œ fine-tuning ì‹œ â†’ ImageNet íŠ¹ì§•ì„ ìŠê³  ì˜¤íˆë ¤ ì„±ëŠ¥ í•˜ë½\n",
    "- í•´ê²°: ì°¨ë“± LR, gradual unfreezing, ì‘ì€ LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Fine-tuning (ì°¨ë“± LR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('=== ì‹¤í—˜ C: Full Fine-tuning (ì°¨ë“± LR) ===')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_ft = make_pretrained_resnet18()\n",
    "set_trainable(model_ft, None)   # ì „ì²´ unfreeze\n",
    "print(f'í•™ìŠµ íŒŒë¼ë¯¸í„°: {count_trainable(model_ft):,}')\n",
    "\n",
    "def diff_lr_optimizer(model, backbone_lr=1e-4, head_lr=1e-3):\n",
    "    \"\"\"backboneê³¼ headì— ì°¨ë“± í•™ìŠµë¥  ì ìš©\"\"\"\n",
    "    backbone_params = [p for n, p in model.named_parameters() if 'fc' not in n]\n",
    "    head_params     = list(model.fc.parameters())\n",
    "    return optim.Adam([\n",
    "        {'params': backbone_params, 'lr': backbone_lr},\n",
    "        {'params': head_params,     'lr': head_lr},\n",
    "    ], weight_decay=1e-4)\n",
    "\n",
    "ft_losses, ft_accs = train_model(\n",
    "    model_ft, train_ldr, test_ldr, n_epochs=20,\n",
    "    optimizer_fn=diff_lr_optimizer\n",
    ")\n",
    "\n",
    "print(f'\\nìµœì¢… Val Accuracy (Fine-tuning): {ft_accs[-1]:.4f}')\n",
    "print('â†’ backboneê¹Œì§€ ìƒˆ ë„ë©”ì¸ì— ì ì‘í•˜ë¯€ë¡œ Feature Extractionë³´ë‹¤ ë†’ê±°ë‚˜ ë¹„ìŠ·í•©ë‹ˆë‹¤.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì„¸ ê°€ì§€ ì „ëµ ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ep = range(1, 21)\n",
    "strategies = [\n",
    "    ('Random Init',          rand_losses, rand_accs, 'tomato',    count_trainable(model_rand)),\n",
    "    ('Feature Extraction',   fe_losses,   fe_accs,   'steelblue', count_trainable(model_fe)),\n",
    "    ('Fine-tuning (ì°¨ë“± LR)', ft_losses,   ft_accs,   'seagreen',  count_trainable(model_ft)),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, losses, accs, color, _ in strategies:\n",
    "    axes[0].plot(ep, losses, color=color, lw=2, label=name)\n",
    "    axes[1].plot(ep, accs,   color=color, lw=2, label=f'{name} ({accs[-1]:.4f})')\n",
    "\n",
    "axes[0].set_title('Train Loss ë¹„êµ'); axes[0].set_xlabel('ì—í¬í¬')\n",
    "axes[0].set_ylabel('CE Loss'); axes[0].legend(fontsize=9)\n",
    "\n",
    "axes[1].axhline(y=1/3, color='gray', ls='--', lw=1, label='ë¬´ì‘ìœ„ ê¸°ì¤€ì„ ')\n",
    "axes[1].set_title('Val Accuracy ë¹„êµ'); axes[1].set_xlabel('ì—í¬í¬')\n",
    "axes[1].set_ylabel('Accuracy'); axes[1].set_ylim(0, 1.05); axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Transfer Learning ì „ëµ ë¹„êµ â€” í•©ì„± ë„í˜• ë¶„ë¥˜ (20 ì—í¬í¬)', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ìš”ì•½ í‘œ\n",
    "print(f'\\n{\"ì „ëµ\":<25} {\"Val Acc\":>10} {\"í•™ìŠµ íŒŒë¼ë¯¸í„°\":>15}')\n",
    "print('-' * 54)\n",
    "for name, _, accs, _, n_p in strategies:\n",
    "    mark = ' â˜…' if accs[-1] == max(s[2][-1] for s in strategies) else ''\n",
    "    print(f'{name:<25} {accs[-1]:>10.4f} {n_p:>15,}{mark}')\n",
    "\n",
    "print('\\nğŸ“Œ í•µì‹¬ ê´€ì°°:')\n",
    "print('  - Feature Extraction: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†€ë¼ìš´ ì„±ëŠ¥ â†’ ì†ŒëŸ‰ ë°ì´í„°ì— ìµœì ')\n",
    "print('  - Fine-tuning: backbone ì ì‘ â†’ ë” ë†’ì€ ì ì¬ ì„±ëŠ¥, ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”')\n",
    "print('  - Random Init: ì‚¬ì „ì§€ì‹ ì—†ì´ ì‹œì‘ â†’ ê°™ì€ ë°ì´í„°ë¡œ ìƒëŒ€ì  ì—´ìœ„')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. ë°ì´í„° íš¨ìœ¨ì„± + ì˜¤ë¥˜ ë¶„ì„\n",
    "\n",
    "### 3-1. í•µì‹¬ ì§ˆë¬¸\n",
    "\n",
    "> **\"ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ìˆì–´ì•¼ Transfer Learningì´ ì˜ë¯¸ ìˆì„ê¹Œ?\"**\n",
    "\n",
    "- ì¼ë°˜ì ìœ¼ë¡œ **ë°ì´í„°ê°€ ì ì„ìˆ˜ë¡** Transfer Learningì˜ ì´ì ì´ í¬ë‹¤\n",
    "- ê·¹ë‹¨ì  ê²½ìš°: í´ë˜ìŠ¤ë‹¹ 10ê°œ ì´ë¯¸ì§€ë§Œ ìˆì–´ë„ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì´ ìœ ë¦¬\n",
    "\n",
    "### 3-2. Pre-computation ìµœì í™”\n",
    "\n",
    "```python\n",
    "# Backboneì„ 1íšŒë§Œ í†µê³¼ â†’ 512ì°¨ì› íŠ¹ì§• ë²¡í„° ì €ì¥\n",
    "feat_pre, y = extract_backbone_features(pretrained_model, train_ldr)\n",
    "# â†’ ì´í›„ Linear Headë¥¼ íŠ¹ì§• ë²¡í„°ë¡œë§Œ í•™ìŠµ (ë§¤ìš° ë¹ ë¦„)\n",
    "```\n",
    "\n",
    "ì´ ë°©ì‹ì€ backbone ì‹¤í–‰ì„ 1íšŒë¡œ ì¤„ì—¬ ë°ì´í„° í¬ê¸° ì‹¤í—˜ì„ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë°ì´í„° íš¨ìœ¨ì„± ë¶„ì„ (Pre-computed Features í™œìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print('Backbone íŠ¹ì§• ì¶”ì¶œ ì¤‘...')\n",
    "t0 = time.time()\n",
    "\n",
    "# ì‚¬ì „í•™ìŠµ backbone íŠ¹ì§• ì¶”ì¶œ (1íšŒ)\n",
    "_pretrained_for_feat = make_pretrained_resnet18()\n",
    "feat_pre_tr, y_tr_feat = extract_backbone_features(_pretrained_for_feat, train_ldr)\n",
    "feat_pre_te, y_te_feat = extract_backbone_features(_pretrained_for_feat, test_ldr)\n",
    "\n",
    "# ëœë¤ ì´ˆê¸°í™” backbone íŠ¹ì§• ì¶”ì¶œ (1íšŒ)\n",
    "_random_for_feat = make_random_resnet18()\n",
    "feat_rand_tr, _ = extract_backbone_features(_random_for_feat, train_ldr)\n",
    "feat_rand_te, _ = extract_backbone_features(_random_for_feat, test_ldr)\n",
    "\n",
    "print(f'íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {time.time()-t0:.1f}s')\n",
    "print(f'  Train features: {feat_pre_tr.shape}  Test features: {feat_pre_te.shape}')\n",
    "del _pretrained_for_feat, _random_for_feat\n",
    "\n",
    "\n",
    "def train_linear_head(feat_tr, y_tr, feat_te, y_te, n_epochs=50, lr=0.01):\n",
    "    \"\"\"Pre-computed featuresë¡œ ì„ í˜• ë¶„ë¥˜ê¸° í•™ìŠµ â†’ Val Accuracy ë°˜í™˜\"\"\"\n",
    "    model = nn.Linear(feat_tr.shape[1], len(CLASS_NAMES)).to(device)\n",
    "    opt   = optim.Adam(model.parameters(), lr=lr)\n",
    "    crit  = nn.CrossEntropyLoss()\n",
    "\n",
    "    ds  = TensorDataset(feat_tr, y_tr)\n",
    "    ldr = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for Xb, yb in ldr:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            crit(model(Xb), yb).backward()\n",
    "            opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc = (model(feat_te.to(device)).argmax(1) == y_te.to(device)).float().mean().item()\n",
    "    model.cpu()\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ë°ì´í„° í¬ê¸°ë³„ ë¹„êµ (í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜)\n",
    "n_sizes    = [30, 60, 100, 150, 200, 300]\n",
    "accs_pre   = []\n",
    "accs_rand  = []\n",
    "\n",
    "print('\\në°ì´í„° íš¨ìœ¨ì„± ì‹¤í—˜ ì¤‘...')\n",
    "print(f'{\"n/class\":>8} {\"Pretrained\":>12} {\"Random Init\":>12}')\n",
    "print('-' * 36)\n",
    "\n",
    "for n in n_sizes:\n",
    "    n_total = n * len(CLASS_NAMES)\n",
    "    # nê°œ/í´ë˜ìŠ¤ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (ê° í´ë˜ìŠ¤ 300ê°œê°€ ìˆœì„œëŒ€ë¡œ ìˆìŒ)\n",
    "    idx = []\n",
    "    for cls in range(len(CLASS_NAMES)):\n",
    "        idx.extend(range(cls * N_TRAIN_PER, cls * N_TRAIN_PER + n))\n",
    "    idx = torch.tensor(idx)\n",
    "\n",
    "    acc_p = train_linear_head(feat_pre_tr[idx],  y_tr_feat[idx], feat_pre_te,  y_te_feat)\n",
    "    acc_r = train_linear_head(feat_rand_tr[idx], y_tr_feat[idx], feat_rand_te, y_te_feat)\n",
    "    accs_pre.append(acc_p)\n",
    "    accs_rand.append(acc_r)\n",
    "    print(f'{n:>8} {acc_p:>12.4f} {acc_r:>12.4f}')\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "n_total_list = [n * len(CLASS_NAMES) for n in n_sizes]\n",
    "ax.plot(n_total_list, accs_pre,  'seagreen', lw=2.5, marker='o', ms=8,\n",
    "        label='Pretrained backbone (ImageNet)')\n",
    "ax.plot(n_total_list, accs_rand, 'tomato',   lw=2.5, marker='s', ms=8,\n",
    "        label='Random Init backbone')\n",
    "ax.axhline(y=1/3, color='gray', ls='--', lw=1, label='ë¬´ì‘ìœ„ ê¸°ì¤€ì„  (1/3)')\n",
    "ax.fill_between(n_total_list,\n",
    "                [max(0, p-r) for p,r in zip(accs_pre, accs_rand)],\n",
    "                [0]*len(n_sizes),\n",
    "                alpha=0.15, color='seagreen', label='Transfer Learning ì´ë“')\n",
    "ax.set_title('ë°ì´í„° íš¨ìœ¨ì„±: Transfer Learning vs ëœë¤ ì´ˆê¸°í™”\\n'\n",
    "             '(Pre-computed backbone features â†’ Linear Classifier)', fontsize=11)\n",
    "ax.set_xlabel('í•™ìŠµ ìƒ˜í”Œ ìˆ˜ (ì „ì²´)'); ax.set_ylabel('Test Accuracy')\n",
    "ax.set_ylim(0, 1.05); ax.legend(fontsize=10)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\nğŸ“Œ í•µì‹¬:')\n",
    "print('  - ì†ŒëŸ‰ ë°ì´í„°(n=30/class, 90ê°œ)ì—ì„œë„ Pretrained >> Random')\n",
    "print('  - ë°ì´í„°ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ë‘˜ì˜ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ” ê²½í–¥')\n",
    "print('  - ì‹¤ì œ ì‚°ì—… í˜„ì¥: ë¼ë²¨ë§ ë¹„ìš© ë•Œë¬¸ì— ì†ŒëŸ‰ ë°ì´í„°ê°€ ì¼ë°˜ì  â†’ TL í•„ìˆ˜')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì˜¤ë¥˜ ë¶„ì„: í˜¼ë™ í–‰ë ¬ + ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Fine-tuning ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ì…‹ ì „ì²´ ì˜ˆì¸¡\n",
    "model_ft.eval()\n",
    "all_preds, all_labels, all_imgs_raw = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_ldr:\n",
    "        preds = model_ft(imgs).argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(torch.tensor(labels).numpy())\n",
    "\n",
    "all_preds  = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "final_acc  = (all_preds == all_labels).mean()\n",
    "\n",
    "# í˜¼ë™ í–‰ë ¬\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, annot_kws={'size': 14})\n",
    "axes[0].set_title(f'í˜¼ë™ í–‰ë ¬ (Fine-tuning, Test Acc={final_acc:.4f})', fontsize=10)\n",
    "axes[0].set_xlabel('ì˜ˆì¸¡ (Predicted)'); axes[0].set_ylabel('ì‹¤ì œ (True)')\n",
    "\n",
    "# ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œ ì‹œê°í™”\n",
    "wrong_idx = np.where(all_preds != all_labels)[0]\n",
    "n_show    = min(6, len(wrong_idx))\n",
    "\n",
    "if n_show > 0:\n",
    "    selected = wrong_idx[:n_show]\n",
    "    for i, idx in enumerate(selected):\n",
    "        ax = axes[1] if n_show == 1 else plt.subplot(2, 6, i+7)\n",
    "        pass\n",
    "\n",
    "# ë³„ë„ figureë¡œ ì˜¤ë¥˜ ìƒ˜í”Œ ì‹œê°í™”\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œ (ì•„ë˜ ë³„ë„ ê·¸ë˜í”„ ì°¸ì¡°)', fontsize=10)\n",
    "\n",
    "plt.suptitle(f'Fine-tuning ëª¨ë¸ ì˜¤ë¥˜ ë¶„ì„ (Test Accuracy: {final_acc:.4f})', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ì˜¤ë¥˜ ìƒ˜í”Œ ì‹œê°í™” (ì›ë³¸ PIL ì´ë¯¸ì§€)\n",
    "if len(wrong_idx) > 0:\n",
    "    n_show = min(8, len(wrong_idx))\n",
    "    fig, axes = plt.subplots(1, n_show, figsize=(3 * n_show, 3.5))\n",
    "    if n_show == 1: axes = [axes]\n",
    "\n",
    "    for i, idx in enumerate(wrong_idx[:n_show]):\n",
    "        orig_img = test_ds.imgs[idx]   # ì›ë³¸ PIL ì´ë¯¸ì§€\n",
    "        axes[i].imshow(orig_img)\n",
    "        axes[i].set_title(\n",
    "            f'ì‹¤ì œ: {CLASS_NAMES[all_labels[idx]]}\\nì˜ˆì¸¡: {CLASS_NAMES[all_preds[idx]]}',\n",
    "            fontsize=9, color='red'\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(f'ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œ ({len(wrong_idx)}ê°œ ì¤‘ {n_show}ê°œ í‘œì‹œ)', fontsize=11)\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print('ëª¨ë“  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì •í™• ë¶„ë¥˜!')\n",
    "\n",
    "print(f'\\nì˜¤ë¥˜ ìƒ˜í”Œ ìˆ˜: {len(wrong_idx)} / {len(all_labels)} ({len(wrong_idx)/len(all_labels)*100:.1f}%)')\n",
    "print('\\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:')\n",
    "for cls in range(len(CLASS_NAMES)):\n",
    "    mask = all_labels == cls\n",
    "    acc_cls = (all_preds[mask] == all_labels[mask]).mean()\n",
    "    print(f'  {CLASS_NAMES[cls]:<12}: {acc_cls:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise\n",
    "\n",
    "### Exercise 1. Fine-tuning Depth íƒìƒ‰\n",
    "\n",
    "**ë ˆì´ì–´ ì–¸í”„ë¦¬ì¦ˆ ê¹Šì´**ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.\n",
    "\n",
    "| ì„¤ì • | í•™ìŠµ ë ˆì´ì–´ | ì˜ˆìƒ í•™ìŠµ íŒŒë¼ë¯¸í„° |\n",
    "|---|---|---|\n",
    "| `head only` | fc | ~1,539 |\n",
    "| `layer4 + head` | layer4, fc | ~2.6M |\n",
    "| `layer3+4 + head` | layer3, layer4, fc | ~7.1M |\n",
    "| `all` | ì „ì²´ | 11.7M |\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "- ë™ì¼ ì¡°ê±´: `n_epochs=15, lr=3e-4, weight_decay=1e-4`\n",
    "- `set_trainable()` í•¨ìˆ˜ í™œìš©\n",
    "- ê° ì„¤ì •ì˜ í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜ + ìµœì¢… Val Accuracy í‘œ ì¶œë ¥\n",
    "- ë§‰ëŒ€ê·¸ë˜í”„ ì‹œê°í™” (x=ì„¤ì •, y=Val Acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Fine-tuning Depth íƒìƒ‰\n",
    "\n",
    "configs = {\n",
    "    'head only':        ['fc'],\n",
    "    'layer4 + head':    ['layer4', 'fc'],\n",
    "    'layer3+4 + head':  ['layer3', 'layer4', 'fc'],\n",
    "    'all layers':       None,   # None = ì „ì²´ unfreeze\n",
    "}\n",
    "ex1_results = {}   # {config_name: (val_acc, n_trainable)}\n",
    "\n",
    "for name, layers in configs.items():\n",
    "    print(f'--- {name} í•™ìŠµ ì¤‘ ---')\n",
    "    torch.manual_seed(42)\n",
    "    # Your code here:\n",
    "    # 1) make_pretrained_resnet18() ìƒì„±\n",
    "    # 2) set_trainable(m, layers) ì ìš©\n",
    "    # 3) print í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "    # 4) train_model() í˜¸ì¶œ (n_epochs=15, lr=3e-4)\n",
    "    # 5) ex1_results[name] = (accs[-1], count_trainable(m))\n",
    "    pass\n",
    "\n",
    "# Your code here: ê²°ê³¼ í‘œ ì¶œë ¥\n",
    "# print(f'{\"ì„¤ì •\":<20} {\"Val Acc\":>10} {\"í•™ìŠµ íŒŒë¼ë¯¸í„°\":>15}')\n",
    "# ...\n",
    "\n",
    "# Your code here: ë§‰ëŒ€ê·¸ë˜í”„ ì‹œê°í™” (x=ì„¤ì •ëª…, y=Val Acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 2. Data Augmentation íš¨ê³¼\n",
    "\n",
    "Augmentation ìœ ë¬´ì— ë”°ë¥¸ Fine-tuning ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.\n",
    "\n",
    "**`aug_transform`** (ì´ë¯¸ setupì—ì„œ ì •ì˜ë¨):\n",
    "```python\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "```\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "1. `ShapeDataset(N_TRAIN_PER, split='train', transform=aug_transform)` ìƒì„±\n",
    "2. No Aug vs Aug: Full Fine-tuning (n_epochs=20) Val Accuracy í•™ìŠµ ê³¡ì„  ë¹„êµ\n",
    "3. ê´€ì°°: Augmentationì´ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Data Augmentation íš¨ê³¼\n",
    "\n",
    "# Your code here: aug_transform ì ìš© DataLoader ìƒì„±\n",
    "# train_ds_aug = ShapeDataset(N_TRAIN_PER, split='train', transform=aug_transform)\n",
    "# train_ldr_aug = DataLoader(train_ds_aug, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Your code here: No Aug Fine-tuning (ft_losses, ft_accsëŠ” ì´ë¯¸ Part 2ì—ì„œ í•™ìŠµë¨ â†’ ì¬ì‚¬ìš© ê°€ëŠ¥)\n",
    "# torch.manual_seed(42)\n",
    "# model_aug = make_pretrained_resnet18()\n",
    "# set_trainable(model_aug, None)\n",
    "# _, accs_aug = train_model(model_aug, train_ldr_aug, test_ldr, n_epochs=20,\n",
    "#                           optimizer_fn=diff_lr_optimizer)\n",
    "\n",
    "# Your code here: í•™ìŠµ ê³¡ì„  ë¹„êµ (No Aug vs Aug)\n",
    "# íŒíŠ¸: ft_accs (No Aug, Part2ì—ì„œ í•™ìŠµ) vs accs_aug (Aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 3. (ë„ì „) t-SNE Feature Space ì‹œê°í™”\n",
    "\n",
    "Random backboneê³¼ Pretrained backboneì˜ íŠ¹ì§• ê³µê°„ì„ **t-SNE**ë¡œ 2D ì‹œê°í™”í•˜ì—¬\n",
    "í´ë˜ìŠ¤ ë¶„ë¦¬ë„ë¥¼ ë¹„êµí•˜ì„¸ìš”.\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "1. `feat_pre_te` (ì‚¬ì „í•™ìŠµ íŠ¹ì§•)ì™€ `feat_rand_te` (ëœë¤ íŠ¹ì§•) ì‚¬ìš© (Part3ì—ì„œ ì¶”ì¶œë¨)\n",
    "2. `TSNE(n_components=2, random_state=42, perplexity=30)` ì ìš©\n",
    "3. 2ì—´ subplot: Random backbone t-SNE | Pretrained backbone t-SNE\n",
    "4. ê° í´ë˜ìŠ¤ë¥¼ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ scatter plot\n",
    "5. ê´€ì°°: ì–´ëŠ backboneì´ í´ë˜ìŠ¤ë¥¼ ë” ì˜ ë¶„ë¦¬í•˜ëŠ”ê°€?\n",
    "\n",
    "**íŒíŠ¸:**\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "emb  = tsne.fit_transform(feat.numpy())   # (N, 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 (ë„ì „): t-SNE Feature Space ì‹œê°í™”\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# feat_pre_te, feat_rand_te, y_te_feat ëŠ” Part 3ì—ì„œ ì¶”ì¶œë¨\n",
    "print(f'í…ŒìŠ¤íŠ¸ íŠ¹ì§• shape: {feat_pre_te.shape}  (N=300, dim=512)')\n",
    "\n",
    "# Your code here:\n",
    "# 1) TSNEë¡œ feat_rand_te, feat_pre_te ê°ê° 2D ì„ë² ë”©\n",
    "# 2) 2ì—´ subplot: ì™¼ìª½=Random, ì˜¤ë¥¸ìª½=Pretrained\n",
    "# 3) scatter plot (í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ: CLASS_NAMES 3ê°€ì§€)\n",
    "# 4) ë²”ë¡€ ì¶”ê°€\n",
    "# 5) ê´€ì°° ê²°ê³¼ ì£¼ì„ìœ¼ë¡œ ì‘ì„±\n",
    "\n",
    "# íŒíŠ¸:\n",
    "# colors = ['tomato', 'steelblue', 'seagreen']\n",
    "# for cls in range(3):\n",
    "#     mask = y_te_feat.numpy() == cls\n",
    "#     ax.scatter(emb[mask, 0], emb[mask, 1], c=colors[cls], label=CLASS_NAMES[cls], alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| ê°œë… | í•µì‹¬ ë‚´ìš© |\n",
    "|---|---|\n",
    "| **Transfer Learning** | ImageNet ì‚¬ì „í•™ìŠµ íŠ¹ì§• ì¬ì‚¬ìš© â†’ ì†ŒëŸ‰ ë°ì´í„°ì—ì„œ ë†’ì€ ì„±ëŠ¥ |\n",
    "| **Feature Extraction** | backbone ê³ ì •, headë§Œ êµì²´Â·í•™ìŠµ â†’ ë¹ ë¥´ê³  overfitting ì ìŒ |\n",
    "| **Fine-tuning** | backboneê¹Œì§€ ìƒˆ ë„ë©”ì¸ì— ë§ê²Œ ì—…ë°ì´íŠ¸ â†’ ë†’ì€ ì„±ëŠ¥, ë” ë§ì€ ë°ì´í„° í•„ìš” |\n",
    "| **Differential LR** | backbone 0.1Ã—LR, head 1Ã—LR â†’ ê¸°ì¡´ íŠ¹ì§• ë³´ì¡´ + head ë¹ ë¥¸ í•™ìŠµ |\n",
    "| **Catastrophic Forgetting** | í° LR fine-tuning â†’ ImageNet íŠ¹ì§• ì†ìƒ ìœ„í—˜; ì°¨ë“± LRë¡œ ë°©ì§€ |\n",
    "| **Pre-computation** | backbone íŠ¹ì§•ì„ 1íšŒë§Œ ì¶”ì¶œ â†’ linear head í•™ìŠµì´ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë¹ ë¦„ |\n",
    "| **ResNet18 êµ¬ì¡°** | conv1â†’layer1~4â†’avgpoolâ†’fc; BasicBlock (2 conv + skip connection) |\n",
    "| **ImageNet ì •ê·œí™”** | mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]; ì‚¬ì „í•™ìŠµ ì…ë ¥ ë§ì¶¤ í•„ìˆ˜ |\n",
    "| **Frozen Layers** | `param.requires_grad=False`; Adamì´ í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ |\n",
    "| **ë°ì´í„° íš¨ìœ¨ì„±** | í´ë˜ìŠ¤ë‹¹ 30ê°œë§Œìœ¼ë¡œë„ Pretrained >> Random; ì‹¤ì œ ì‚°ì—… í˜„ì¥ì˜ í•µì‹¬ ì´ìœ  |\n",
    "\n",
    "---\n",
    "\n",
    "**ë‹¤ìŒ ê°•ì˜:** BERT, GPT â€” Transformer ê¸°ë°˜ ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ + íŒŒì¸íŠœë‹ íŒ¨ëŸ¬ë‹¤ì„\n"
   ]
  }
 ]
}