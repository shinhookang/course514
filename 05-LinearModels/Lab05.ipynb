{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 05 — Linear Models\n",
    "\n",
    "> **주제:** 선형 회귀 이론부터 PyTorch 구현까지\n",
    "\n",
    "---\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| # | 목표 |  \n",
    "|---|---| \n",
    "| 1 | 선형 회귀 수식과 Closed-form Solution 이해  \n",
    "| 2 | MSE / MAE 손실 함수의 특성 비교  \n",
    "| 3 | Batch / SGD / Mini-batch 경사 하강법 구현 및 비교  \n",
    "| 4 | PyTorch `autograd`, `nn.Linear`로 집값 예측 모델 구현  \n",
    "| 5 | 다항 특성, Ridge / Lasso 정규화 개념  \n",
    "\n",
    "---\n",
    "\n",
    "**데이터셋:** California Housing (sklearn) — 캘리포니아 지역별 주택 가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 한글 폰트\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "print('PyTorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. 선형 회귀 이론\n",
    "\n",
    "### 1-1. 문제 정의\n",
    "\n",
    "**회귀(Regression)** 란 연속적인 수치 출력을 예측하는 지도 학습 문제입니다.\n",
    "\n",
    "선형 회귀는 입력 특성 $\\mathbf{x}$와 출력 $y$ 사이를 **선형 함수**로 모델링합니다.\n",
    "\n",
    "$$\\hat{y} = w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d + b = \\mathbf{w}^\\top \\mathbf{x} + b$$\n",
    "\n",
    "- $\\mathbf{w}$ : 가중치(weight) — 각 특성이 예측에 미치는 영향\n",
    "- $b$ : 편향(bias) — 절편\n",
    "\n",
    "전체 데이터셋 행렬 표기:\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = X\\mathbf{w} + b, \\quad X \\in \\mathbb{R}^{n \\times d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D 예제: 방 크기 → 집값\n",
    "# 실제 관계: price = 3.0 * size + 5.0 + noise\n",
    "n = 80\n",
    "size  = rng.uniform(20, 100, n)\n",
    "price = 3.0 * size + 5.0 + rng.normal(0, 8, n)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(size, price, alpha=0.7, edgecolors='k', s=50)\n",
    "plt.xlabel('방 크기 (m²)')\n",
    "plt.ylabel('집값 (만 달러)')\n",
    "plt.title('1D 선형 회귀 예제 — 방 크기 vs 집값')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'샘플 수: {n}')\n",
    "print('실제 관계: price = 3.0 * size + 5.0 + noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md2",
   "metadata": {},
   "source": [
    "### 1-2. Closed-form Solution (정규 방정식)\n",
    "\n",
    "MSE 손실을 최소화하는 $\\mathbf{w}$를 **해석적으로** 구할 수 있습니다.\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{n} \\|X\\mathbf{w} - \\mathbf{y}\\|^2$$\n",
    "\n",
    "$\\nabla_\\mathbf{w} \\mathcal{L} = 0$ 으로 놓으면:\n",
    "\n",
    "$$\\boxed{\\mathbf{w}^* = (X^\\top X)^{-1} X^\\top \\mathbf{y}}$$\n",
    "\n",
    "> **언제 쓸 수 있나?** 특성 수 $d$가 작을 때 ($d < 10{,}000$).  \n",
    "> 역행렬 계산이 $\\mathcal{O}(d^3)$이므로 고차원에서는 비실용적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 방정식 직접 구현\n",
    "# 편향 항을 위해 X에 1열 추가: X_b = [1 | x]\n",
    "X_b = np.column_stack([np.ones(n), size])   # (80, 2)\n",
    "y   = price\n",
    "\n",
    "# w* = (X^T X)^{-1} X^T y\n",
    "w_star = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "b_hat, w_hat = w_star[0], w_star[1]\n",
    "\n",
    "print(f'추정 기울기 w : {w_hat:.4f}  (실제: 3.0)')\n",
    "print(f'추정 절편   b : {b_hat:.4f}  (실제: 5.0)')\n",
    "\n",
    "# 예측 및 시각화\n",
    "x_line = np.linspace(20, 100, 200)\n",
    "y_line = w_hat * x_line + b_hat\n",
    "y_pred_ne = w_hat * size + b_hat\n",
    "mse_ne    = np.mean((y_pred_ne - price)**2)\n",
    "r2_ne     = 1 - np.sum((price - y_pred_ne)**2) / np.sum((price - price.mean())**2)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(size, price, alpha=0.6, edgecolors='k', s=40, label='데이터')\n",
    "plt.plot(x_line, y_line, color='tomato', lw=2.5,\n",
    "         label=f'정규 방정식: y = {w_hat:.2f}x + {b_hat:.2f}')\n",
    "for xi, yi, ypi in zip(size, price, y_pred_ne):\n",
    "    plt.plot([xi, xi], [yi, ypi], color='gray', alpha=0.3, lw=0.8)\n",
    "plt.xlabel('방 크기 (m²)')\n",
    "plt.ylabel('집값 (만 달러)')\n",
    "plt.title(f'Closed-form Solution  MSE={mse_ne:.2f}  R²={r2_ne:.4f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md3",
   "metadata": {},
   "source": [
    "### 1-3. 다중 선형 회귀 (Multiple Linear Regression)\n",
    "\n",
    "실제 집값에는 여러 특성이 영향을 미칩니다.  \n",
    "California Housing 데이터셋을 불러와 다중 회귀를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame.copy()\n",
    "\n",
    "print('특성 수  :', len(housing.feature_names))\n",
    "print('샘플 수  :', len(df))\n",
    "print('\\n특성 설명:')\n",
    "descriptions = {\n",
    "    'MedInc'    : '중위 소득 (만 달러)',\n",
    "    'HouseAge'  : '주택 연령 (년)',\n",
    "    'AveRooms'  : '평균 방 수',\n",
    "    'AveBedrms' : '평균 침실 수',\n",
    "    'Population': '인구 수',\n",
    "    'AveOccup'  : '평균 거주자 수',\n",
    "    'Latitude'  : '위도',\n",
    "    'Longitude' : '경도',\n",
    "}\n",
    "for feat, desc in descriptions.items():\n",
    "    print(f'  {feat:<12}: {desc}')\n",
    "print('\\n목표값: MedHouseVal — 중위 주택 가격 (×$100,000)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분포 및 상관관계\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].hist(df['MedHouseVal'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(df['MedHouseVal'].median(), color='tomato', lw=2,\n",
    "                label=f\"중앙값 {df['MedHouseVal'].median():.2f}\")\n",
    "axes[0].set_title('주택 가격 분포')\n",
    "axes[0].set_xlabel('중위 주택 가격 (×$100K)')\n",
    "axes[0].legend()\n",
    "\n",
    "corr = df.corr().round(2)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            vmin=-1, vmax=1, ax=axes[1], linewidths=0.3, annot_kws={'size': 7})\n",
    "axes[1].set_title('특성 간 상관관계')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 방정식으로 다중 선형 회귀\n",
    "X_multi = df[housing.feature_names].values\n",
    "y_multi = df['MedHouseVal'].values\n",
    "X_multi_b = np.column_stack([np.ones(len(X_multi)), X_multi])\n",
    "\n",
    "# lstsq: 수치적으로 더 안정적인 최소제곱법\n",
    "w_multi = np.linalg.lstsq(X_multi_b, y_multi, rcond=None)[0]\n",
    "\n",
    "print('다중 회귀 계수:')\n",
    "print(f'  편향(b)   : {w_multi[0]:.4f}')\n",
    "for feat, w in zip(housing.feature_names, w_multi[1:]):\n",
    "    print(f'  {feat:<12}: {w:+.4f}')\n",
    "\n",
    "y_pred_multi = X_multi_b @ w_multi\n",
    "print(f'\\nTrain MSE : {mean_squared_error(y_multi, y_pred_multi):.4f}')\n",
    "print(f'Train R²  : {r2_score(y_multi, y_pred_multi):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. 손실 함수 (Loss Functions)\n",
    "\n",
    "### 2-1. MSE vs MAE vs Huber\n",
    "\n",
    "| 손실 함수 | 수식 | 특징 |\n",
    "|---|---|---|\n",
    "| **MSE** | $\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$ | 이상치에 민감, 미분 가능 |\n",
    "| **MAE** | $\\frac{1}{n}\\sum|y_i - \\hat{y}_i|$ | 이상치에 강건 |\n",
    "| **Huber** | MSE + MAE 결합 | 두 장점 모두 |\n",
    "| **RMSE** | $\\sqrt{\\text{MSE}}$ | 단위가 $y$와 동일 |\n",
    "| **R²** | $1 - \\frac{\\text{SS}_{res}}{\\text{SS}_{tot}}$ | 1에 가까울수록 좋음 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 존재 시 MSE vs MAE 민감도 비교\n",
    "np.random.seed(0)\n",
    "x_toy = np.linspace(0, 10, 30)\n",
    "y_toy = 2*x_toy + 1 + np.random.randn(30)*2\n",
    "x_out = np.append(x_toy, [2, 4])\n",
    "y_out = np.append(y_toy, [30, -10])   # 극단 이상치\n",
    "\n",
    "def fit_line(x, y):\n",
    "    Xb = np.column_stack([np.ones_like(x), x])\n",
    "    return np.linalg.lstsq(Xb, y, rcond=None)[0]\n",
    "\n",
    "w_clean   = fit_line(x_toy, y_toy)\n",
    "x_line2   = np.linspace(0, 10, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "for ax, (x_d, y_d, title) in zip(axes, [\n",
    "    (x_toy, y_toy, '이상치 없음'),\n",
    "    (x_out, y_out, '이상치 포함'),\n",
    "]):\n",
    "    ax.scatter(x_d[:30], y_d[:30], color='steelblue', s=40, zorder=3)\n",
    "    if len(x_d) > 30:\n",
    "        ax.scatter(x_d[30:], y_d[30:], color='red', s=80, marker='*', zorder=4, label='이상치')\n",
    "    w = fit_line(x_d, y_d)\n",
    "    ax.plot(x_line2, w[1]*x_line2+w[0], 'tomato', lw=2, label=f'MSE 최적선 (w={w[1]:.2f})')\n",
    "    ax.plot(x_line2, w_clean[1]*x_line2+w_clean[0], 'green', lw=1.5, linestyle='--',\n",
    "            label=f'클린 선 (w={w_clean[1]:.2f})')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(fontsize=8)\n",
    "plt.suptitle('MSE는 이상치에 민감하다', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 시각화\n",
    "err   = np.linspace(-4, 4, 300)\n",
    "delta = 1.0\n",
    "mse_v   = err**2\n",
    "mae_v   = np.abs(err)\n",
    "huber_v = np.where(np.abs(err) <= delta, 0.5*err**2, delta*(np.abs(err)-0.5*delta))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(err, mse_v,   label='MSE',            color='steelblue', lw=2)\n",
    "axes[0].plot(err, mae_v,   label='MAE',            color='tomato',    lw=2)\n",
    "axes[0].plot(err, huber_v, label=f'Huber (d={delta})', color='seagreen', lw=2)\n",
    "axes[0].set_title('손실 함수 비교')\n",
    "axes[0].set_xlabel('오차 (y - ŷ)')\n",
    "axes[0].set_ylabel('손실')\n",
    "axes[0].set_ylim(-0.5, 8)\n",
    "axes[0].legend()\n",
    "\n",
    "grad_mse   = 2*err\n",
    "grad_mae   = np.sign(err)\n",
    "grad_huber = np.where(np.abs(err) <= delta, err, delta*np.sign(err))\n",
    "axes[1].plot(err, grad_mse,   label='dMSE/de',   color='steelblue', lw=2)\n",
    "axes[1].plot(err, grad_mae,   label='dMAE/de',   color='tomato',    lw=2)\n",
    "axes[1].plot(err, grad_huber, label='dHuber/de', color='seagreen',  lw=2)\n",
    "axes[1].set_title('그래디언트 크기 비교')\n",
    "axes[1].set_xlabel('오차 (y - ŷ)')\n",
    "axes[1].set_ylabel('그래디언트')\n",
    "axes[1].set_ylim(-3, 3)\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 손실 함수 사용\n",
    "y_true_t = torch.tensor([3.0, 5.0, 2.5, 7.0])\n",
    "y_pred_t = torch.tensor([2.8, 5.5, 2.0, 6.5])\n",
    "\n",
    "print(f'MSE   Loss : {nn.MSELoss()(y_pred_t, y_true_t).item():.4f}')\n",
    "print(f'MAE   Loss : {nn.L1Loss()(y_pred_t, y_true_t).item():.4f}')\n",
    "print(f'Huber Loss : {nn.HuberLoss(delta=1.0)(y_pred_t, y_true_t).item():.4f}')\n",
    "\n",
    "def regression_metrics(y_true, y_pred, label=''):\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f'{label}')\n",
    "    print(f'  MSE  : {mse:.4f}')\n",
    "    print(f'  RMSE : {rmse:.4f}')\n",
    "    print(f'  MAE  : {mae:.4f}')\n",
    "    print(f'  R²   : {r2:.4f}')\n",
    "    return dict(mse=mse, rmse=rmse, mae=mae, r2=r2)\n",
    "\n",
    "regression_metrics(y_multi, y_pred_multi, '다중 선형 회귀 (정규 방정식)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. 경사 하강법 (Gradient Descent)\n",
    "\n",
    "### 3-1. 핵심 아이디어\n",
    "\n",
    "정규 방정식은 고차원에서 역행렬 계산이 비실용적입니다.  \n",
    "**경사 하강법**은 손실을 반복적으로 줄이며 최적해를 찾습니다.\n",
    "\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\cdot \\nabla_\\mathbf{w} \\mathcal{L}$$\n",
    "\n",
    "MSE의 그래디언트:\n",
    "\n",
    "$$\\nabla_\\mathbf{w} \\mathcal{L}_{\\text{MSE}} = \\frac{2}{n} X^\\top (X\\mathbf{w} - \\mathbf{y})$$\n",
    "\n",
    "| 방법 | 배치 크기 | 장점 | 단점 |\n",
    "|---|---|---|---|\n",
    "| **Batch GD** | 전체 $n$ | 안정적 수렴 | 느림, 메모리 큼 |\n",
    "| **SGD** | 1개 | 빠름, 지역 최소 탈출 | 불안정, 진동 큼 |\n",
    "| **Mini-batch GD** | $k$개 | 두 장점 균형 | 배치 크기 튜닝 필요 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D 예제로 3가지 경사 하강법 구현\n",
    "X_gd    = size.reshape(-1, 1)\n",
    "y_gd    = price\n",
    "x_mean, x_std = X_gd.mean(), X_gd.std()\n",
    "y_mean, y_std = y_gd.mean(),  y_gd.std()\n",
    "X_norm  = (X_gd - x_mean) / x_std\n",
    "y_norm  = (y_gd - y_mean) / y_std\n",
    "n_gd    = len(y_norm)\n",
    "\n",
    "def gd_step(w, b, X, y, lr):\n",
    "    n_   = len(y)\n",
    "    err  = X.ravel()*w + b - y\n",
    "    dw   = (2/n_) * (err * X.ravel()).sum()\n",
    "    db   = (2/n_) * err.sum()\n",
    "    return w - lr*dw, b - lr*db\n",
    "\n",
    "def mse_val(w, b, X, y):\n",
    "    return np.mean((X.ravel()*w + b - y)**2)\n",
    "\n",
    "EPOCHS, LR, BATCH = 200, 0.05, 16\n",
    "\n",
    "# Batch GD\n",
    "w_b, b_b = 0.0, 0.0\n",
    "loss_batch = []\n",
    "for _ in range(EPOCHS):\n",
    "    w_b, b_b = gd_step(w_b, b_b, X_norm, y_norm, LR)\n",
    "    loss_batch.append(mse_val(w_b, b_b, X_norm, y_norm))\n",
    "\n",
    "# SGD\n",
    "w_s, b_s = 0.0, 0.0\n",
    "loss_sgd  = []\n",
    "for ep in range(EPOCHS):\n",
    "    idx = rng.permutation(n_gd)\n",
    "    for i in idx:\n",
    "        w_s, b_s = gd_step(w_s, b_s, X_norm[i:i+1], y_norm[i:i+1], LR)\n",
    "    loss_sgd.append(mse_val(w_s, b_s, X_norm, y_norm))\n",
    "\n",
    "# Mini-batch GD\n",
    "w_m, b_m = 0.0, 0.0\n",
    "loss_mini = []\n",
    "for ep in range(EPOCHS):\n",
    "    idx = rng.permutation(n_gd)\n",
    "    for start in range(0, n_gd, BATCH):\n",
    "        bi = idx[start:start+BATCH]\n",
    "        w_m, b_m = gd_step(w_m, b_m, X_norm[bi], y_norm[bi], LR)\n",
    "    loss_mini.append(mse_val(w_m, b_m, X_norm, y_norm))\n",
    "\n",
    "print(f'최종 손실 - Batch:{loss_batch[-1]:.5f}  SGD:{loss_sgd[-1]:.5f}  Mini:{loss_mini[-1]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(loss_batch, label='Batch GD',   color='steelblue', lw=2)\n",
    "axes[0].plot(loss_mini,  label='Mini-batch', color='seagreen',  lw=2)\n",
    "axes[0].plot(loss_sgd,   label='SGD',        color='tomato',    lw=1.5, alpha=0.8)\n",
    "axes[0].set_title('3가지 경사 하강법 손실 수렴 비교')\n",
    "axes[0].set_xlabel('에폭')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(loss_batch[:30], label='Batch GD',   color='steelblue', lw=2)\n",
    "axes[1].plot(loss_mini[:30],  label='Mini-batch', color='seagreen',  lw=2)\n",
    "axes[1].plot(loss_sgd[:30],   label='SGD',        color='tomato',    lw=1.5, alpha=0.8)\n",
    "axes[1].set_title('초기 수렴 속도 (첫 30 에폭)')\n",
    "axes[1].set_xlabel('에폭')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md2",
   "metadata": {},
   "source": [
    "### 3-2. 학습률(Learning Rate) 영향\n",
    "\n",
    "학습률 $\\eta$는 가장 중요한 하이퍼파라미터입니다.\n",
    "\n",
    "- $\\eta$ **너무 큼** → 발산(divergence)\n",
    "- $\\eta$ **너무 작음** → 느린 수렴\n",
    "- $\\eta$ **적절** → 안정적, 빠른 수렴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.05, 0.2, 0.8]\n",
    "colors_lr      = ['purple', 'steelblue', 'seagreen', 'tomato']\n",
    "EPOCHS_LR      = 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors_lr):\n",
    "    w, b = 0.0, 0.0\n",
    "    losses_lr = []\n",
    "    for _ in range(EPOCHS_LR):\n",
    "        w, b = gd_step(w, b, X_norm, y_norm, lr)\n",
    "        loss = mse_val(w, b, X_norm, y_norm)\n",
    "        if loss > 1e6 or np.isnan(loss):\n",
    "            losses_lr.extend([np.nan] * (EPOCHS_LR - len(losses_lr)))\n",
    "            break\n",
    "        losses_lr.append(loss)\n",
    "    axes[0].plot(losses_lr, label=f'lr={lr}', color=color, lw=2)\n",
    "\n",
    "axes[0].set_title('학습률에 따른 수렴 특성')\n",
    "axes[0].set_xlabel('에폭')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_ylim(-0.1, 3)\n",
    "axes[0].legend()\n",
    "\n",
    "# 손실 등고선 + 경사 하강 경로\n",
    "W_r = np.linspace(-2, 2, 80)\n",
    "B_r = np.linspace(-2, 2, 80)\n",
    "WW, BB = np.meshgrid(W_r, B_r)\n",
    "LL = np.array([[mse_val(wi, bi, X_norm, y_norm) for wi, bi in zip(Wr, Br)]\n",
    "               for Wr, Br in zip(WW, BB)])\n",
    "axes[1].contourf(WW, BB, LL, levels=25, cmap='RdYlGn_r', alpha=0.8)\n",
    "for lr, color in zip([0.05, 0.2, 0.8], ['steelblue', 'seagreen', 'tomato']):\n",
    "    w, b = -1.5, -1.5\n",
    "    wh, bh = [w], [b]\n",
    "    for _ in range(20):\n",
    "        w, b = gd_step(w, b, X_norm, y_norm, lr)\n",
    "        if abs(w) > 5 or abs(b) > 5: break\n",
    "        wh.append(w); bh.append(b)\n",
    "    axes[1].plot(wh, bh, 'o-', color=color, markersize=4, lw=1.5, label=f'lr={lr}')\n",
    "axes[1].set_title('손실 등고선 + 경사 하강 경로 (20스텝)')\n",
    "axes[1].set_xlabel('w'); axes[1].set_ylabel('b')\n",
    "axes[1].legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. PyTorch로 선형 회귀 구현\n",
    "\n",
    "### 4-1. PyTorch `autograd` 기초\n",
    "\n",
    "PyTorch는 **자동 미분(autograd)** 을 제공합니다.  \n",
    "`requires_grad=True`로 설정된 텐서는 연산 그래프를 기록하고,  \n",
    "`.backward()` 호출 시 연쇄 법칙으로 그래디언트를 자동 계산합니다.\n",
    "\n",
    "```\n",
    "순전파: x → y_pred → loss\n",
    "역전파: loss.backward() → w.grad, b.grad 자동 계산\n",
    "업데이트: w = w - lr * w.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autograd 동작 원리\n",
    "x_ag  = torch.tensor([2.0, 3.0, 4.0])\n",
    "w_ag  = torch.tensor(1.5, requires_grad=True)\n",
    "b_ag  = torch.tensor(0.0, requires_grad=True)\n",
    "y_tgt = torch.tensor([5.0, 7.0, 9.0])   # w=2, b=1이면 완벽\n",
    "\n",
    "# 순전파\n",
    "y_hat_ag = w_ag * x_ag + b_ag\n",
    "loss_ag  = ((y_hat_ag - y_tgt)**2).mean()\n",
    "print(f'예측: {y_hat_ag.tolist()}')\n",
    "print(f'손실: {loss_ag.item():.4f}')\n",
    "\n",
    "# 역전파\n",
    "loss_ag.backward()\n",
    "print(f'\\ndL/dw = {w_ag.grad.item():.4f}  (이 방향으로 w를 줄이면 손실 감소)')\n",
    "print(f'dL/db = {b_ag.grad.item():.4f}')\n",
    "\n",
    "# 수동 검증\n",
    "e = y_hat_ag.detach().numpy() - y_tgt.numpy()\n",
    "dw_manual = 2/3 * (e * x_ag.numpy()).sum()\n",
    "print(f'\\n수동 계산 dL/dw = {dw_manual:.4f}  (일치 확인)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md2",
   "metadata": {},
   "source": [
    "### 4-2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df[housing.feature_names].values.astype(np.float32)\n",
    "y_all = df['MedHouseVal'].values.astype(np.float32)\n",
    "\n",
    "X_tv, X_te, y_tv, y_te = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_tv,  y_tv,  test_size=0.25, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_tr_s = scaler_X.fit_transform(X_tr)\n",
    "X_va_s = scaler_X.transform(X_va)\n",
    "X_te_s = scaler_X.transform(X_te)\n",
    "y_tr_s = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()\n",
    "y_va_s = scaler_y.transform(y_va.reshape(-1,1)).ravel()\n",
    "y_te_s = scaler_y.transform(y_te.reshape(-1,1)).ravel()\n",
    "\n",
    "def make_loader(X, y, batch_size=256, shuffle=False):\n",
    "    ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = make_loader(X_tr_s, y_tr_s, 256, shuffle=True)\n",
    "val_loader   = make_loader(X_va_s, y_va_s, 256)\n",
    "test_loader  = make_loader(X_te_s, y_te_s, 256)\n",
    "\n",
    "print(f'Train: {len(X_tr_s):>6}개  ({len(X_tr_s)/len(X_all)*100:.0f}%)')\n",
    "print(f'Val  : {len(X_va_s):>6}개  ({len(X_va_s)/len(X_all)*100:.0f}%)')\n",
    "print(f'Test : {len(X_te_s):>6}개  ({len(X_te_s)/len(X_all)*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md3",
   "metadata": {},
   "source": [
    "### 4-3. `nn.Linear` 모델 정의\n",
    "\n",
    "`nn.Linear(in, out)`은 내부적으로 `y = xW^T + b` 를 수행합니다.\n",
    "\n",
    "```python\n",
    "# 직접 구현하면 이렇게 됩니다:\n",
    "y = x @ W.T + b   # W: (out, in),  b: (out,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \"\"\"단순 선형 회귀: y = Xw + b\"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)   # 출력 1개 (회귀)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x).squeeze(1)          # (batch,1) → (batch,)\n",
    "\n",
    "model = LinearRegression(in_features=8)\n",
    "print(model)\n",
    "print(f'\\n파라미터 수: {sum(p.numel() for p in model.parameters())}')\n",
    "print(f'  가중치 w : {model.linear.weight.shape}')\n",
    "print(f'  편향   b : {model.linear.bias.shape}')\n",
    "\n",
    "x_sample = torch.tensor(X_tr_s[:4])\n",
    "with torch.no_grad():\n",
    "    y_sample = model(x_sample)\n",
    "print(f'\\n초기 예측 (랜덤 가중치): {y_sample.numpy().round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md4",
   "metadata": {},
   "source": [
    "### 4-4. 학습 루프 (Train / Validate)\n",
    "\n",
    "매 에폭마다 아래 5단계를 반복합니다:\n",
    "\n",
    "```\n",
    "1. optimizer.zero_grad()   ← 이전 그래디언트 초기화\n",
    "2. y_hat = model(x)        ← 순전파\n",
    "3. loss  = criterion(...)  ← 손실 계산\n",
    "4. loss.backward()         ← 역전파\n",
    "5. optimizer.step()        ← 파라미터 업데이트\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()          # 1\n",
    "        y_hat = model(xb)              # 2\n",
    "        loss  = criterion(y_hat, yb)   # 3\n",
    "        loss.backward()                # 4\n",
    "        optimizer.step()               # 5\n",
    "        total_loss += loss.item() * len(xb)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        total += criterion(model(xb), yb).item() * len(xb)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "model_pt  = LinearRegression(8)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_pt.parameters(), lr=1e-2)\n",
    "\n",
    "EPOCHS = 100\n",
    "train_losses_pt, val_losses_pt = [], []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr = train_one_epoch(model_pt, train_loader, criterion, optimizer)\n",
    "    va = evaluate(model_pt, val_loader, criterion)\n",
    "    train_losses_pt.append(tr)\n",
    "    val_losses_pt.append(va)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:3d} | Train MSE: {tr:.4f} | Val MSE: {va:.4f}')\n",
    "print('\\n학습 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(train_losses_pt, label='Train', color='steelblue')\n",
    "axes[0].plot(val_losses_pt,   label='Val',   color='tomato')\n",
    "axes[0].set_title('학습 / 검증 손실 곡선')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE (표준화 공간)')\n",
    "axes[0].legend()\n",
    "\n",
    "model_pt.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_s = model_pt(torch.tensor(X_te_s)).numpy()\n",
    "y_pred_orig = scaler_y.inverse_transform(y_pred_s.reshape(-1,1)).ravel()\n",
    "\n",
    "axes[1].scatter(y_te, y_pred_orig, alpha=0.3, s=10, color='steelblue')\n",
    "lo = min(y_te.min(), y_pred_orig.min())\n",
    "hi = max(y_te.max(), y_pred_orig.max())\n",
    "axes[1].plot([lo,hi],[lo,hi],'r--',lw=2,label='완벽한 예측선')\n",
    "axes[1].set_title('예측값 vs 실제값')\n",
    "axes[1].set_xlabel('실제 집값 (×$100K)')\n",
    "axes[1].set_ylabel('예측 집값 (×$100K)')\n",
    "axes[1].legend()\n",
    "\n",
    "residuals = y_te - y_pred_orig\n",
    "axes[2].hist(residuals, bins=50, color='steelblue', edgecolor='white')\n",
    "axes[2].axvline(0, color='red', lw=2)\n",
    "axes[2].axvline(residuals.mean(), color='orange', lw=2,\n",
    "                label=f'평균={residuals.mean():.3f}')\n",
    "axes[2].set_title('잔차 분포')\n",
    "axes[2].set_xlabel('잔차 (실제 - 예측)')\n",
    "axes[2].legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "regression_metrics(y_te, y_pred_orig, '\\n[ Test 성능 — PyTorch 선형 회귀 ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 가중치 해석\n",
    "w_learned = model_pt.linear.weight.detach().numpy().ravel()\n",
    "b_learned = model_pt.linear.bias.detach().item()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "colors_w = ['tomato' if w > 0 else 'steelblue' for w in w_learned]\n",
    "plt.barh(housing.feature_names, w_learned, color=colors_w)\n",
    "plt.axvline(0, color='black', lw=0.8)\n",
    "plt.title('학습된 선형 회귀 가중치\\n(빨강=양의 영향, 파랑=음의 영향)')\n",
    "plt.xlabel('가중치 크기 (표준화 공간)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('가중치 해석:')\n",
    "for feat, w in sorted(zip(housing.feature_names, w_learned), key=lambda x: abs(x[1]), reverse=True):\n",
    "    sign = '(+) 집값 상승' if w > 0 else '(-) 집값 하락'\n",
    "    print(f'  {feat:<12}: {w:+.4f}  {sign}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md5",
   "metadata": {},
   "source": [
    "### 4-5. 옵티마이저 비교 (SGD vs Adam vs RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code7",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    ('SGD (lr=0.05)',     lambda p: torch.optim.SGD(p,     lr=0.05)),\n",
    "    ('Adam (lr=0.01)',    lambda p: torch.optim.Adam(p,    lr=0.01)),\n",
    "    ('RMSprop (lr=0.01)', lambda p: torch.optim.RMSprop(p, lr=0.01)),\n",
    "]\n",
    "colors_opt = ['steelblue', 'tomato', 'seagreen']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for (name, opt_fn), color in zip(configs, colors_opt):\n",
    "    m   = LinearRegression(8)\n",
    "    opt = opt_fn(m.parameters())\n",
    "    vh  = [evaluate(m, val_loader, criterion)]\n",
    "    for _ in range(60):\n",
    "        train_one_epoch(m, train_loader, criterion, opt)\n",
    "        vh.append(evaluate(m, val_loader, criterion))\n",
    "    plt.plot(vh, label=name, color=color, lw=2)\n",
    "plt.title('옵티마이저별 검증 손실 수렴 비교')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val MSE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p5-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5. 모델 개선\n",
    "\n",
    "### 5-1. 다항 특성 (Polynomial Features)\n",
    "\n",
    "선형 모델이지만 **입력 특성을 고차항으로 확장**하면 비선형 관계를 포착할 수 있습니다.\n",
    "\n",
    "$$x \\rightarrow [1,\\; x,\\; x^2,\\; x^3, \\ldots, x^d]$$\n",
    "\n",
    "단, 차수가 높아질수록 **과적합(overfitting)** 위험이 증가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x_poly = np.linspace(-3, 3, 80)\n",
    "y_poly = 0.5*x_poly**3 - 2*x_poly**2 + x_poly + 3 + np.random.randn(80)*2\n",
    "x_eval = np.linspace(-3, 3, 300)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, deg in zip(axes, [1, 3, 9]):\n",
    "    Xp = np.column_stack([x_poly**d for d in range(deg+1)])\n",
    "    wp = np.linalg.lstsq(Xp, y_poly, rcond=None)[0]\n",
    "    Xe = np.column_stack([x_eval**d for d in range(deg+1)])\n",
    "    ye = Xe @ wp\n",
    "    r2 = r2_score(y_poly, Xp@wp)\n",
    "    ax.scatter(x_poly, y_poly, alpha=0.5, s=20, color='steelblue')\n",
    "    ax.plot(x_eval, ye, color='tomato', lw=2)\n",
    "    ax.set_ylim(-25, 20)\n",
    "    ax.set_title(f'{deg}차 다항식  R²={r2:.3f}')\n",
    "    label = '과소적합' if deg==1 else ('적합' if deg==3 else '과적합')\n",
    "    ax.text(0, -22, label, ha='center', fontsize=12,\n",
    "            color='red' if label!='적합' else 'green', fontweight='bold')\n",
    "plt.suptitle('다항 특성 — 과소적합 vs 적합 vs 과적합', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p5-md2",
   "metadata": {},
   "source": [
    "### 5-2. 정규화 (Regularization) — Ridge & Lasso\n",
    "\n",
    "과적합을 방지하기 위해 손실 함수에 **페널티 항**을 추가합니다.\n",
    "\n",
    "$$\\text{Ridge (L2)}: \\quad \\mathcal{L} + \\lambda \\|\\mathbf{w}\\|_2^2$$\n",
    "\n",
    "$$\\text{Lasso (L1)}: \\quad \\mathcal{L} + \\lambda \\|\\mathbf{w}\\|_1$$\n",
    "\n",
    "| | Ridge | Lasso |\n",
    "|---|---|---|\n",
    "| 패널티 | $\\lambda\\|w\\|_2^2$ | $\\lambda\\|w\\|_1$ |\n",
    "| 효과 | 가중치 축소 | 가중치를 0으로 (특성 선택) |\n",
    "| PyTorch | `weight_decay` 파라미터 | 수동 구현 필요 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고차 다항식 + Ridge 정규화\n",
    "DEG = 9\n",
    "Xhi = np.column_stack([x_poly**d for d in range(DEG+1)])\n",
    "Xhi = (Xhi - Xhi.mean(0)) / (Xhi.std(0) + 1e-8)\n",
    "Xev = np.column_stack([x_eval**d for d in range(DEG+1)])\n",
    "Xev = (Xev - Xev.mean(0)) / (Xev.std(0) + 1e-8)\n",
    "\n",
    "lambdas    = [0.0, 0.01, 1.0, 10.0]\n",
    "colors_reg = plt.cm.Blues(np.linspace(0.4, 1.0, 4))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(x_poly, y_poly, alpha=0.5, s=20, color='gray', zorder=3)\n",
    "plt.plot(x_eval, 0.5*x_eval**3-2*x_eval**2+x_eval+3, 'k--', lw=1.5,\n",
    "         label='실제 함수', zorder=4)\n",
    "for lam, color in zip(lambdas, colors_reg):\n",
    "    I = np.eye(Xhi.shape[1])\n",
    "    wr = np.linalg.solve(Xhi.T@Xhi + lam*I, Xhi.T@y_poly)\n",
    "    plt.plot(x_eval, Xev@wr, color=color, lw=2, label=f'Ridge l={lam}')\n",
    "plt.ylim(-25, 20)\n",
    "plt.title('Ridge 정규화 — λ 값에 따른 과적합 억제')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch에서 Ridge 정규화 = weight_decay\n",
    "print('[ 정규화 효과 비교 ]')\n",
    "print(f'{\"방법\":<16}  {\"Val MSE\":>9}  {\"Test MSE\":>9}  {\"||w||2\":>8}')\n",
    "print('-' * 50)\n",
    "for name, wd in [('No Reg', 0.0), ('Ridge 1e-3', 1e-3), ('Ridge 1e-1', 1e-1)]:\n",
    "    m   = LinearRegression(8)\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=0.01, weight_decay=wd)\n",
    "    for _ in range(80):\n",
    "        train_one_epoch(m, train_loader, criterion, opt)\n",
    "    va = evaluate(m, val_loader, criterion)\n",
    "    te = evaluate(m, test_loader, criterion)\n",
    "    wn = m.linear.weight.detach().norm().item()\n",
    "    print(f'{name:<16}  {va:>9.4f}  {te:>9.4f}  {wn:>8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise\n",
    "\n",
    "### Exercise 1. 정규 방정식 직접 구현\n",
    "\n",
    "California Housing 데이터의 **처음 2개 특성** (`MedInc`, `HouseAge`)만 사용해  \n",
    "정규 방정식 $\\mathbf{w}^* = (X^\\top X)^{-1} X^\\top \\mathbf{y}$를 직접 구현하고  \n",
    "MSE와 R²를 출력하세요. (`np.linalg.lstsq` 미사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 2. Mini-batch 배치 크기 실험\n",
    "\n",
    "**batch_size = 1, 16, 64, 전체(n)** 4가지 경우의 수렴 곡선을 하나의 그래프에 그리고  \n",
    "최종 손실을 비교하세요.  \n",
    "데이터는 위에서 사용한 1D 예제 (`X_norm`, `y_norm`)를 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 3. PyTorch 파이프라인 완성\n",
    "\n",
    "아래 클래스와 학습 루프를 완성하세요.\n",
    "\n",
    "- 모델: `nn.Linear(8, 1)` + **He 초기화** (`nn.init.kaiming_uniform_`) 적용\n",
    "- 옵티마이저: `Adam`, `lr=0.01`\n",
    "- 스케줄러: `StepLR(optimizer, step_size=30, gamma=0.5)` — 30 에폭마다 lr 절반\n",
    "- 100 에폭 학습 후 **Test MSE와 R²** 출력\n",
    "- 학습 곡선 (train loss, val loss)과 **학습률 변화** 함께 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionV2(nn.Module):\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "        # Your code here: He 초기화 적용\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "# Your code here: 학습 루프 + 스케줄러 + 시각화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| 개념 | 핵심 내용 |\n",
    "|---|---|\n",
    "| **선형 회귀** | $\\hat{y} = \\mathbf{w}^\\top\\mathbf{x} + b$, 연속값 예측 |\n",
    "| **정규 방정식** | $(X^\\top X)^{-1}X^\\top\\mathbf{y}$, 소규모 데이터에 적합 |\n",
    "| **MSE vs MAE** | MSE는 이상치에 민감, MAE는 강건 |\n",
    "| **Batch GD** | 안정적이지만 느림, 전체 데이터 사용 |\n",
    "| **SGD** | 빠르지만 진동, 1개 샘플 사용 |\n",
    "| **Mini-batch** | 실제 딥러닝의 표준 방식 |\n",
    "| **학습률 η** | 가장 중요한 하이퍼파라미터 |\n",
    "| **autograd** | `requires_grad=True` → `.backward()` → `.grad` |\n",
    "| **nn.Linear** | `Linear(in, out)` 내부: `xW^T + b` |\n",
    "| **Ridge** | L2 정규화, `weight_decay`로 적용 |\n",
    "| **Lasso** | L1 정규화, 특성 선택 효과 |\n",
    "\n",
    "---\n",
    "\n",
    "**다음 강의 (Week 6):** 분류 — Logistic Regression, 결정 경계, Precision/Recall, ROC-AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
