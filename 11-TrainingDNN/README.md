# Week 11 — Training Deep Networks

## Contents
- Optimizers: SGD with momentum, RMSProp, Adam
- Batch normalization and layer normalization
- Dropout and L1/L2 regularization
- Learning rate scheduling

## Lab
**MNIST with MLP (PyTorch)** — Train a deep MLP on MNIST with batch norm and dropout, and compare different optimizers.

---
[← Back to Course](../README.md)
