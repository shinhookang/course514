{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 05 — Linear Models\n\n> **강의 시간:** 약 2시간  \n> **주제:** 선형 회귀 이론부터 PyTorch 구현까지\n\n---\n\n## 학습 목표\n\n| # | 목표 | 예상 시간 |\n|---|---|---|\n| 1 | 선형 회귀 수식과 Closed-form Solution 이해 | 25분 |\n| 2 | MSE / MAE 손실 함수의 특성 비교 | 20분 |\n| 3 | Batch / SGD / Mini-batch 경사 하강법 구현 및 비교 | 35분 |\n| 4 | PyTorch `autograd`, `nn.Linear`로 집값 예측 모델 구현 | 30분 |\n| 5 | 다항 특성, Ridge / Lasso 정규화 개념 | 10분 |\n\n---\n\n**데이터셋:** California Housing (sklearn) — 캘리포니아 지역별 주택 가격",
   "id": "c-title"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# 한글 폰트\n_fp = '/System/Library/Fonts/AppleGothic.ttf'\nfm.fontManager.addfont(_fp)\nplt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\nplt.rcParams['axes.unicode_minus'] = False\nsns.set_theme(style='whitegrid')\n\nrng = np.random.default_rng(42)\ntorch.manual_seed(42)\nprint('PyTorch:', torch.__version__)",
   "execution_count": null,
   "outputs": [],
   "id": "c-setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 1. 선형 회귀 이론\n\n### 1-1. 문제 정의\n\n**회귀(Regression)** 란 연속적인 수치 출력을 예측하는 지도 학습 문제입니다.\n\n선형 회귀는 입력 특성 $\\mathbf{x}$와 출력 $y$ 사이를 **선형 함수**로 모델링합니다.\n\n$$\\hat{y} = w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d + b = \\mathbf{w}^\\top \\mathbf{x} + b$$\n\n- $\\mathbf{w}$ : 가중치(weight) — 각 특성이 예측에 미치는 영향\n- $b$ : 편향(bias) — 절편\n\n전체 데이터셋 행렬 표기:\n\n$$\\hat{\\mathbf{y}} = X\\mathbf{w} + b, \\quad X \\in \\mathbb{R}^{n \\times d}$$",
   "id": "c-p1-md1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 1D 예제: 방 크기 → 집값\n# 실제 관계: price = 3.0 * size + 5.0 + noise\nn = 80\nsize  = rng.uniform(20, 100, n)\nprice = 3.0 * size + 5.0 + rng.normal(0, 8, n)\n\nplt.figure(figsize=(7, 4))\nplt.scatter(size, price, alpha=0.7, edgecolors='k', s=50)\nplt.xlabel('방 크기 (m²)')\nplt.ylabel('집값 (만 달러)')\nplt.title('1D 선형 회귀 예제 — 방 크기 vs 집값')\nplt.tight_layout()\nplt.show()\nprint(f'샘플 수: {n}')\nprint('실제 관계: price = 3.0 * size + 5.0 + noise')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p1-code1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1-2. Closed-form Solution (정규 방정식)\n\nMSE 손실을 최소화하는 $\\mathbf{w}$를 **해석적으로** 구할 수 있습니다.\n\n$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{n} \\|X\\mathbf{w} - \\mathbf{y}\\|^2$$\n\n$\\nabla_\\mathbf{w} \\mathcal{L} = 0$ 으로 놓으면:\n\n$$\\boxed{\\mathbf{w}^* = (X^\\top X)^{-1} X^\\top \\mathbf{y}}$$\n\n> **언제 쓸 수 있나?** 특성 수 $d$가 작을 때 ($d < 10{,}000$).  \n> 역행렬 계산이 $\\mathcal{O}(d^3)$이므로 고차원에서는 비실용적.",
   "id": "c-p1-md2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 정규 방정식 직접 구현\n# 편향 항을 위해 X에 1열 추가: X_b = [1 | x]\nX_b = np.column_stack([np.ones(n), size])   # (80, 2)\ny   = price\n\n# w* = (X^T X)^{-1} X^T y\nw_star = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\nb_hat, w_hat = w_star[0], w_star[1]\n\nprint(f'추정 기울기 w : {w_hat:.4f}  (실제: 3.0)')\nprint(f'추정 절편   b : {b_hat:.4f}  (실제: 5.0)')\n\n# 예측 및 시각화\nx_line = np.linspace(20, 100, 200)\ny_line = w_hat * x_line + b_hat\ny_pred_ne = w_hat * size + b_hat\nmse_ne    = np.mean((y_pred_ne - price)**2)\nr2_ne     = 1 - np.sum((price - y_pred_ne)**2) / np.sum((price - price.mean())**2)\n\nplt.figure(figsize=(7, 4))\nplt.scatter(size, price, alpha=0.6, edgecolors='k', s=40, label='데이터')\nplt.plot(x_line, y_line, color='tomato', lw=2.5,\n         label=f'정규 방정식: y = {w_hat:.2f}x + {b_hat:.2f}')\nfor xi, yi, ypi in zip(size, price, y_pred_ne):\n    plt.plot([xi, xi], [yi, ypi], color='gray', alpha=0.3, lw=0.8)\nplt.xlabel('방 크기 (m²)')\nplt.ylabel('집값 (만 달러)')\nplt.title(f'Closed-form Solution  MSE={mse_ne:.2f}  R²={r2_ne:.4f}')\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p1-code2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1-3. 다중 선형 회귀 (Multiple Linear Regression)\n\n실제 집값에는 여러 특성이 영향을 미칩니다.  \nCalifornia Housing 데이터셋을 불러와 다중 회귀를 수행합니다.",
   "id": "c-p1-md3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "housing = fetch_california_housing(as_frame=True)\ndf = housing.frame.copy()\n\nprint('특성 수  :', len(housing.feature_names))\nprint('샘플 수  :', len(df))\nprint('\\n특성 설명:')\ndescriptions = {\n    'MedInc'    : '중위 소득 (만 달러)',\n    'HouseAge'  : '주택 연령 (년)',\n    'AveRooms'  : '평균 방 수',\n    'AveBedrms' : '평균 침실 수',\n    'Population': '인구 수',\n    'AveOccup'  : '평균 거주자 수',\n    'Latitude'  : '위도',\n    'Longitude' : '경도',\n}\nfor feat, desc in descriptions.items():\n    print(f'  {feat:<12}: {desc}')\nprint('\\n목표값: MedHouseVal — 중위 주택 가격 (×$100,000)')\ndf.head()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p1-code3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 분포 및 상관관계\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\naxes[0].hist(df['MedHouseVal'], bins=50, color='steelblue', edgecolor='white')\naxes[0].axvline(df['MedHouseVal'].median(), color='tomato', lw=2,\n                label=f\"중앙값 {df['MedHouseVal'].median():.2f}\")\naxes[0].set_title('주택 가격 분포')\naxes[0].set_xlabel('중위 주택 가격 (×$100K)')\naxes[0].legend()\n\ncorr = df.corr().round(2)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n            vmin=-1, vmax=1, ax=axes[1], linewidths=0.3, annot_kws={'size': 7})\naxes[1].set_title('특성 간 상관관계')\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p1-code4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 정규 방정식으로 다중 선형 회귀\nX_multi = df[housing.feature_names].values\ny_multi = df['MedHouseVal'].values\nX_multi_b = np.column_stack([np.ones(len(X_multi)), X_multi])\n\n# lstsq: 수치적으로 더 안정적인 최소제곱법\nw_multi = np.linalg.lstsq(X_multi_b, y_multi, rcond=None)[0]\n\nprint('다중 회귀 계수:')\nprint(f'  편향(b)   : {w_multi[0]:.4f}')\nfor feat, w in zip(housing.feature_names, w_multi[1:]):\n    print(f'  {feat:<12}: {w:+.4f}')\n\ny_pred_multi = X_multi_b @ w_multi\nprint(f'\\nTrain MSE : {mean_squared_error(y_multi, y_pred_multi):.4f}')\nprint(f'Train R²  : {r2_score(y_multi, y_pred_multi):.4f}')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p1-code5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 2. 손실 함수 (Loss Functions)\n\n### 2-1. MSE vs MAE vs Huber\n\n| 손실 함수 | 수식 | 특징 |\n|---|---|---|\n| **MSE** | $\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$ | 이상치에 민감, 미분 가능 |\n| **MAE** | $\\frac{1}{n}\\sum|y_i - \\hat{y}_i|$ | 이상치에 강건 |\n| **Huber** | MSE + MAE 결합 | 두 장점 모두 |\n| **RMSE** | $\\sqrt{\\text{MSE}}$ | 단위가 $y$와 동일 |\n| **R²** | $1 - \\frac{\\text{SS}_{res}}{\\text{SS}_{tot}}$ | 1에 가까울수록 좋음 |",
   "id": "c-p2-md1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 이상치 존재 시 MSE vs MAE 민감도 비교\nnp.random.seed(0)\nx_toy = np.linspace(0, 10, 30)\ny_toy = 2*x_toy + 1 + np.random.randn(30)*2\nx_out = np.append(x_toy, [2, 4])\ny_out = np.append(y_toy, [30, -10])   # 극단 이상치\n\ndef fit_line(x, y):\n    Xb = np.column_stack([np.ones_like(x), x])\n    return np.linalg.lstsq(Xb, y, rcond=None)[0]\n\nw_clean   = fit_line(x_toy, y_toy)\nx_line2   = np.linspace(0, 10, 200)\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\nfor ax, (x_d, y_d, title) in zip(axes, [\n    (x_toy, y_toy, '이상치 없음'),\n    (x_out, y_out, '이상치 포함'),\n]):\n    ax.scatter(x_d[:30], y_d[:30], color='steelblue', s=40, zorder=3)\n    if len(x_d) > 30:\n        ax.scatter(x_d[30:], y_d[30:], color='red', s=80, marker='*', zorder=4, label='이상치')\n    w = fit_line(x_d, y_d)\n    ax.plot(x_line2, w[1]*x_line2+w[0], 'tomato', lw=2, label=f'MSE 최적선 (w={w[1]:.2f})')\n    ax.plot(x_line2, w_clean[1]*x_line2+w_clean[0], 'green', lw=1.5, linestyle='--',\n            label=f'클린 선 (w={w_clean[1]:.2f})')\n    ax.set_title(title)\n    ax.legend(fontsize=8)\nplt.suptitle('MSE는 이상치에 민감하다', y=1.02)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p2-code1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 손실 함수 시각화\nerr   = np.linspace(-4, 4, 300)\ndelta = 1.0\nmse_v   = err**2\nmae_v   = np.abs(err)\nhuber_v = np.where(np.abs(err) <= delta, 0.5*err**2, delta*(np.abs(err)-0.5*delta))\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\naxes[0].plot(err, mse_v,   label='MSE',            color='steelblue', lw=2)\naxes[0].plot(err, mae_v,   label='MAE',            color='tomato',    lw=2)\naxes[0].plot(err, huber_v, label=f'Huber (d={delta})', color='seagreen', lw=2)\naxes[0].set_title('손실 함수 비교')\naxes[0].set_xlabel('오차 (y - ŷ)')\naxes[0].set_ylabel('손실')\naxes[0].set_ylim(-0.5, 8)\naxes[0].legend()\n\ngrad_mse   = 2*err\ngrad_mae   = np.sign(err)\ngrad_huber = np.where(np.abs(err) <= delta, err, delta*np.sign(err))\naxes[1].plot(err, grad_mse,   label='dMSE/de',   color='steelblue', lw=2)\naxes[1].plot(err, grad_mae,   label='dMAE/de',   color='tomato',    lw=2)\naxes[1].plot(err, grad_huber, label='dHuber/de', color='seagreen',  lw=2)\naxes[1].set_title('그래디언트 크기 비교')\naxes[1].set_xlabel('오차 (y - ŷ)')\naxes[1].set_ylabel('그래디언트')\naxes[1].set_ylim(-3, 3)\naxes[1].legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p2-code2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# PyTorch 손실 함수 사용\ny_true_t = torch.tensor([3.0, 5.0, 2.5, 7.0])\ny_pred_t = torch.tensor([2.8, 5.5, 2.0, 6.5])\n\nprint(f'MSE   Loss : {nn.MSELoss()(y_pred_t, y_true_t).item():.4f}')\nprint(f'MAE   Loss : {nn.L1Loss()(y_pred_t, y_true_t).item():.4f}')\nprint(f'Huber Loss : {nn.HuberLoss(delta=1.0)(y_pred_t, y_true_t).item():.4f}')\n\ndef regression_metrics(y_true, y_pred, label=''):\n    mse  = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae  = mean_absolute_error(y_true, y_pred)\n    r2   = r2_score(y_true, y_pred)\n    print(f'{label}')\n    print(f'  MSE  : {mse:.4f}')\n    print(f'  RMSE : {rmse:.4f}')\n    print(f'  MAE  : {mae:.4f}')\n    print(f'  R²   : {r2:.4f}')\n    return dict(mse=mse, rmse=rmse, mae=mae, r2=r2)\n\nregression_metrics(y_multi, y_pred_multi, '다중 선형 회귀 (정규 방정식)')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p2-code3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 3. 경사 하강법 (Gradient Descent)\n\n### 3-1. 핵심 아이디어\n\n정규 방정식은 고차원에서 역행렬 계산이 비실용적입니다.  \n**경사 하강법**은 손실을 반복적으로 줄이며 최적해를 찾습니다.\n\n$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\cdot \\nabla_\\mathbf{w} \\mathcal{L}$$\n\nMSE의 그래디언트:\n\n$$\\nabla_\\mathbf{w} \\mathcal{L}_{\\text{MSE}} = \\frac{2}{n} X^\\top (X\\mathbf{w} - \\mathbf{y})$$\n\n| 방법 | 배치 크기 | 장점 | 단점 |\n|---|---|---|---|\n| **Batch GD** | 전체 $n$ | 안정적 수렴 | 느림, 메모리 큼 |\n| **SGD** | 1개 | 빠름, 지역 최소 탈출 | 불안정, 진동 큼 |\n| **Mini-batch GD** | $k$개 | 두 장점 균형 | 배치 크기 튜닝 필요 |",
   "id": "c-p3-md1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 1D 예제로 3가지 경사 하강법 구현\nX_gd    = size.reshape(-1, 1)\ny_gd    = price\nx_mean, x_std = X_gd.mean(), X_gd.std()\ny_mean, y_std = y_gd.mean(),  y_gd.std()\nX_norm  = (X_gd - x_mean) / x_std\ny_norm  = (y_gd - y_mean) / y_std\nn_gd    = len(y_norm)\n\ndef gd_step(w, b, X, y, lr):\n    n_   = len(y)\n    err  = X.ravel()*w + b - y\n    dw   = (2/n_) * (err * X.ravel()).sum()\n    db   = (2/n_) * err.sum()\n    return w - lr*dw, b - lr*db\n\ndef mse_val(w, b, X, y):\n    return np.mean((X.ravel()*w + b - y)**2)\n\nEPOCHS, LR, BATCH = 200, 0.05, 16\n\n# Batch GD\nw_b, b_b = 0.0, 0.0\nloss_batch = []\nfor _ in range(EPOCHS):\n    w_b, b_b = gd_step(w_b, b_b, X_norm, y_norm, LR)\n    loss_batch.append(mse_val(w_b, b_b, X_norm, y_norm))\n\n# SGD\nw_s, b_s = 0.0, 0.0\nloss_sgd  = []\nfor ep in range(EPOCHS):\n    idx = rng.permutation(n_gd)\n    for i in idx:\n        w_s, b_s = gd_step(w_s, b_s, X_norm[i:i+1], y_norm[i:i+1], LR)\n    loss_sgd.append(mse_val(w_s, b_s, X_norm, y_norm))\n\n# Mini-batch GD\nw_m, b_m = 0.0, 0.0\nloss_mini = []\nfor ep in range(EPOCHS):\n    idx = rng.permutation(n_gd)\n    for start in range(0, n_gd, BATCH):\n        bi = idx[start:start+BATCH]\n        w_m, b_m = gd_step(w_m, b_m, X_norm[bi], y_norm[bi], LR)\n    loss_mini.append(mse_val(w_m, b_m, X_norm, y_norm))\n\nprint(f'최종 손실 - Batch:{loss_batch[-1]:.5f}  SGD:{loss_sgd[-1]:.5f}  Mini:{loss_mini[-1]:.5f}')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p3-code1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\naxes[0].plot(loss_batch, label='Batch GD',   color='steelblue', lw=2)\naxes[0].plot(loss_mini,  label='Mini-batch', color='seagreen',  lw=2)\naxes[0].plot(loss_sgd,   label='SGD',        color='tomato',    lw=1.5, alpha=0.8)\naxes[0].set_title('3가지 경사 하강법 손실 수렴 비교')\naxes[0].set_xlabel('에폭')\naxes[0].set_ylabel('MSE Loss')\naxes[0].set_yscale('log')\naxes[0].legend()\n\naxes[1].plot(loss_batch[:30], label='Batch GD',   color='steelblue', lw=2)\naxes[1].plot(loss_mini[:30],  label='Mini-batch', color='seagreen',  lw=2)\naxes[1].plot(loss_sgd[:30],   label='SGD',        color='tomato',    lw=1.5, alpha=0.8)\naxes[1].set_title('초기 수렴 속도 (첫 30 에폭)')\naxes[1].set_xlabel('에폭')\naxes[1].set_ylabel('MSE Loss')\naxes[1].legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p3-code2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3-2. 학습률(Learning Rate) 영향\n\n학습률 $\\eta$는 가장 중요한 하이퍼파라미터입니다.\n\n- $\\eta$ **너무 큼** → 발산(divergence)\n- $\\eta$ **너무 작음** → 느린 수렴\n- $\\eta$ **적절** → 안정적, 빠른 수렴",
   "id": "c-p3-md2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "learning_rates = [0.001, 0.05, 0.2, 0.8]\ncolors_lr      = ['purple', 'steelblue', 'seagreen', 'tomato']\nEPOCHS_LR      = 100\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\nfor lr, color in zip(learning_rates, colors_lr):\n    w, b = 0.0, 0.0\n    losses_lr = []\n    for _ in range(EPOCHS_LR):\n        w, b = gd_step(w, b, X_norm, y_norm, lr)\n        loss = mse_val(w, b, X_norm, y_norm)\n        if loss > 1e6 or np.isnan(loss):\n            losses_lr.extend([np.nan] * (EPOCHS_LR - len(losses_lr)))\n            break\n        losses_lr.append(loss)\n    axes[0].plot(losses_lr, label=f'lr={lr}', color=color, lw=2)\n\naxes[0].set_title('학습률에 따른 수렴 특성')\naxes[0].set_xlabel('에폭')\naxes[0].set_ylabel('MSE Loss')\naxes[0].set_ylim(-0.1, 3)\naxes[0].legend()\n\n# 손실 등고선 + 경사 하강 경로\nW_r = np.linspace(-2, 2, 80)\nB_r = np.linspace(-2, 2, 80)\nWW, BB = np.meshgrid(W_r, B_r)\nLL = np.array([[mse_val(wi, bi, X_norm, y_norm) for wi, bi in zip(Wr, Br)]\n               for Wr, Br in zip(WW, BB)])\naxes[1].contourf(WW, BB, LL, levels=25, cmap='RdYlGn_r', alpha=0.8)\nfor lr, color in zip([0.05, 0.2, 0.8], ['steelblue', 'seagreen', 'tomato']):\n    w, b = -1.5, -1.5\n    wh, bh = [w], [b]\n    for _ in range(20):\n        w, b = gd_step(w, b, X_norm, y_norm, lr)\n        if abs(w) > 5 or abs(b) > 5: break\n        wh.append(w); bh.append(b)\n    axes[1].plot(wh, bh, 'o-', color=color, markersize=4, lw=1.5, label=f'lr={lr}')\naxes[1].set_title('손실 등고선 + 경사 하강 경로 (20스텝)')\naxes[1].set_xlabel('w'); axes[1].set_ylabel('b')\naxes[1].legend(fontsize=8)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p3-code3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 4. PyTorch로 선형 회귀 구현\n\n### 4-1. PyTorch `autograd` 기초\n\nPyTorch는 **자동 미분(autograd)** 을 제공합니다.  \n`requires_grad=True`로 설정된 텐서는 연산 그래프를 기록하고,  \n`.backward()` 호출 시 연쇄 법칙으로 그래디언트를 자동 계산합니다.\n\n```\n순전파: x → y_pred → loss\n역전파: loss.backward() → w.grad, b.grad 자동 계산\n업데이트: w = w - lr * w.grad\n```",
   "id": "c-p4-md1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# autograd 동작 원리\nx_ag  = torch.tensor([2.0, 3.0, 4.0])\nw_ag  = torch.tensor(1.5, requires_grad=True)\nb_ag  = torch.tensor(0.0, requires_grad=True)\ny_tgt = torch.tensor([5.0, 7.0, 9.0])   # w=2, b=1이면 완벽\n\n# 순전파\ny_hat_ag = w_ag * x_ag + b_ag\nloss_ag  = ((y_hat_ag - y_tgt)**2).mean()\nprint(f'예측: {y_hat_ag.tolist()}')\nprint(f'손실: {loss_ag.item():.4f}')\n\n# 역전파\nloss_ag.backward()\nprint(f'\\ndL/dw = {w_ag.grad.item():.4f}  (이 방향으로 w를 줄이면 손실 감소)')\nprint(f'dL/db = {b_ag.grad.item():.4f}')\n\n# 수동 검증\ne = y_hat_ag.detach().numpy() - y_tgt.numpy()\ndw_manual = 2/3 * (e * x_ag.numpy()).sum()\nprint(f'\\n수동 계산 dL/dw = {dw_manual:.4f}  (일치 확인)')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4-2. 데이터 준비",
   "id": "c-p4-md2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "X_all = df[housing.feature_names].values.astype(np.float32)\ny_all = df['MedHouseVal'].values.astype(np.float32)\n\nX_tv, X_te, y_tv, y_te = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\nX_tr, X_va, y_tr, y_va = train_test_split(X_tv,  y_tv,  test_size=0.25, random_state=42)\n\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\nX_tr_s = scaler_X.fit_transform(X_tr)\nX_va_s = scaler_X.transform(X_va)\nX_te_s = scaler_X.transform(X_te)\ny_tr_s = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()\ny_va_s = scaler_y.transform(y_va.reshape(-1,1)).ravel()\ny_te_s = scaler_y.transform(y_te.reshape(-1,1)).ravel()\n\ndef make_loader(X, y, batch_size=256, shuffle=False):\n    ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\ntrain_loader = make_loader(X_tr_s, y_tr_s, 256, shuffle=True)\nval_loader   = make_loader(X_va_s, y_va_s, 256)\ntest_loader  = make_loader(X_te_s, y_te_s, 256)\n\nprint(f'Train: {len(X_tr_s):>6}개  ({len(X_tr_s)/len(X_all)*100:.0f}%)')\nprint(f'Val  : {len(X_va_s):>6}개  ({len(X_va_s)/len(X_all)*100:.0f}%)')\nprint(f'Test : {len(X_te_s):>6}개  ({len(X_te_s)/len(X_all)*100:.0f}%)')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4-3. `nn.Linear` 모델 정의\n\n`nn.Linear(in, out)`은 내부적으로 `y = xW^T + b` 를 수행합니다.\n\n```python\n# 직접 구현하면 이렇게 됩니다:\ny = x @ W.T + b   # W: (out, in),  b: (out,)\n```",
   "id": "c-p4-md3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class LinearRegression(nn.Module):\n    \"\"\"단순 선형 회귀: y = Xw + b\"\"\"\n    def __init__(self, in_features: int):\n        super().__init__()\n        self.linear = nn.Linear(in_features, 1)   # 출력 1개 (회귀)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.linear(x).squeeze(1)          # (batch,1) → (batch,)\n\nmodel = LinearRegression(in_features=8)\nprint(model)\nprint(f'\\n파라미터 수: {sum(p.numel() for p in model.parameters())}')\nprint(f'  가중치 w : {model.linear.weight.shape}')\nprint(f'  편향   b : {model.linear.bias.shape}')\n\nx_sample = torch.tensor(X_tr_s[:4])\nwith torch.no_grad():\n    y_sample = model(x_sample)\nprint(f'\\n초기 예측 (랜덤 가중치): {y_sample.numpy().round(3)}')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4-4. 학습 루프 (Train / Validate)\n\n매 에폭마다 아래 5단계를 반복합니다:\n\n```\n1. optimizer.zero_grad()   ← 이전 그래디언트 초기화\n2. y_hat = model(x)        ← 순전파\n3. loss  = criterion(...)  ← 손실 계산\n4. loss.backward()         ← 역전파\n5. optimizer.step()        ← 파라미터 업데이트\n```",
   "id": "c-p4-md4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def train_one_epoch(model, loader, criterion, optimizer):\n    model.train()\n    total_loss = 0\n    for xb, yb in loader:\n        optimizer.zero_grad()          # 1\n        y_hat = model(xb)              # 2\n        loss  = criterion(y_hat, yb)   # 3\n        loss.backward()                # 4\n        optimizer.step()               # 5\n        total_loss += loss.item() * len(xb)\n    return total_loss / len(loader.dataset)\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion):\n    model.eval()\n    total = 0\n    for xb, yb in loader:\n        total += criterion(model(xb), yb).item() * len(xb)\n    return total / len(loader.dataset)\n\nmodel_pt  = LinearRegression(8)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model_pt.parameters(), lr=1e-2)\n\nEPOCHS = 100\ntrain_losses_pt, val_losses_pt = [], []\nfor epoch in range(1, EPOCHS+1):\n    tr = train_one_epoch(model_pt, train_loader, criterion, optimizer)\n    va = evaluate(model_pt, val_loader, criterion)\n    train_losses_pt.append(tr)\n    val_losses_pt.append(va)\n    if epoch % 20 == 0:\n        print(f'Epoch {epoch:3d} | Train MSE: {tr:.4f} | Val MSE: {va:.4f}')\nprint('\\n학습 완료')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 결과 시각화\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(train_losses_pt, label='Train', color='steelblue')\naxes[0].plot(val_losses_pt,   label='Val',   color='tomato')\naxes[0].set_title('학습 / 검증 손실 곡선')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('MSE (표준화 공간)')\naxes[0].legend()\n\nmodel_pt.eval()\nwith torch.no_grad():\n    y_pred_s = model_pt(torch.tensor(X_te_s)).numpy()\ny_pred_orig = scaler_y.inverse_transform(y_pred_s.reshape(-1,1)).ravel()\n\naxes[1].scatter(y_te, y_pred_orig, alpha=0.3, s=10, color='steelblue')\nlo = min(y_te.min(), y_pred_orig.min())\nhi = max(y_te.max(), y_pred_orig.max())\naxes[1].plot([lo,hi],[lo,hi],'r--',lw=2,label='완벽한 예측선')\naxes[1].set_title('예측값 vs 실제값')\naxes[1].set_xlabel('실제 집값 (×$100K)')\naxes[1].set_ylabel('예측 집값 (×$100K)')\naxes[1].legend()\n\nresiduals = y_te - y_pred_orig\naxes[2].hist(residuals, bins=50, color='steelblue', edgecolor='white')\naxes[2].axvline(0, color='red', lw=2)\naxes[2].axvline(residuals.mean(), color='orange', lw=2,\n                label=f'평균={residuals.mean():.3f}')\naxes[2].set_title('잔차 분포')\naxes[2].set_xlabel('잔차 (실제 - 예측)')\naxes[2].legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\nregression_metrics(y_te, y_pred_orig, '\\n[ Test 성능 — PyTorch 선형 회귀 ]')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 학습된 가중치 해석\nw_learned = model_pt.linear.weight.detach().numpy().ravel()\nb_learned = model_pt.linear.bias.detach().item()\n\nplt.figure(figsize=(8, 4))\ncolors_w = ['tomato' if w > 0 else 'steelblue' for w in w_learned]\nplt.barh(housing.feature_names, w_learned, color=colors_w)\nplt.axvline(0, color='black', lw=0.8)\nplt.title('학습된 선형 회귀 가중치\\n(빨강=양의 영향, 파랑=음의 영향)')\nplt.xlabel('가중치 크기 (표준화 공간)')\nplt.tight_layout()\nplt.show()\n\nprint('가중치 해석:')\nfor feat, w in sorted(zip(housing.feature_names, w_learned), key=lambda x: abs(x[1]), reverse=True):\n    sign = '(+) 집값 상승' if w > 0 else '(-) 집값 하락'\n    print(f'  {feat:<12}: {w:+.4f}  {sign}')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4-5. 옵티마이저 비교 (SGD vs Adam vs RMSprop)",
   "id": "c-p4-md5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "configs = [\n    ('SGD (lr=0.05)',     lambda p: torch.optim.SGD(p,     lr=0.05)),\n    ('Adam (lr=0.01)',    lambda p: torch.optim.Adam(p,    lr=0.01)),\n    ('RMSprop (lr=0.01)', lambda p: torch.optim.RMSprop(p, lr=0.01)),\n]\ncolors_opt = ['steelblue', 'tomato', 'seagreen']\n\nplt.figure(figsize=(8, 4))\nfor (name, opt_fn), color in zip(configs, colors_opt):\n    m   = LinearRegression(8)\n    opt = opt_fn(m.parameters())\n    vh  = [evaluate(m, val_loader, criterion)]\n    for _ in range(60):\n        train_one_epoch(m, train_loader, criterion, opt)\n        vh.append(evaluate(m, val_loader, criterion))\n    plt.plot(vh, label=name, color=color, lw=2)\nplt.title('옵티마이저별 검증 손실 수렴 비교')\nplt.xlabel('Epoch')\nplt.ylabel('Val MSE')\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p4-code7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 5. 모델 개선\n\n### 5-1. 다항 특성 (Polynomial Features)\n\n선형 모델이지만 **입력 특성을 고차항으로 확장**하면 비선형 관계를 포착할 수 있습니다.\n\n$$x \\rightarrow [1,\\; x,\\; x^2,\\; x^3, \\ldots, x^d]$$\n\n단, 차수가 높아질수록 **과적합(overfitting)** 위험이 증가합니다.",
   "id": "c-p5-md1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "np.random.seed(1)\nx_poly = np.linspace(-3, 3, 80)\ny_poly = 0.5*x_poly**3 - 2*x_poly**2 + x_poly + 3 + np.random.randn(80)*2\nx_eval = np.linspace(-3, 3, 300)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfor ax, deg in zip(axes, [1, 3, 9]):\n    Xp = np.column_stack([x_poly**d for d in range(deg+1)])\n    wp = np.linalg.lstsq(Xp, y_poly, rcond=None)[0]\n    Xe = np.column_stack([x_eval**d for d in range(deg+1)])\n    ye = Xe @ wp\n    r2 = r2_score(y_poly, Xp@wp)\n    ax.scatter(x_poly, y_poly, alpha=0.5, s=20, color='steelblue')\n    ax.plot(x_eval, ye, color='tomato', lw=2)\n    ax.set_ylim(-25, 20)\n    ax.set_title(f'{deg}차 다항식  R²={r2:.3f}')\n    label = '과소적합' if deg==1 else ('적합' if deg==3 else '과적합')\n    ax.text(0, -22, label, ha='center', fontsize=12,\n            color='red' if label!='적합' else 'green', fontweight='bold')\nplt.suptitle('다항 특성 — 과소적합 vs 적합 vs 과적합', y=1.02)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p5-code1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5-2. 정규화 (Regularization) — Ridge & Lasso\n\n과적합을 방지하기 위해 손실 함수에 **페널티 항**을 추가합니다.\n\n$$\\text{Ridge (L2)}: \\quad \\mathcal{L} + \\lambda \\|\\mathbf{w}\\|_2^2$$\n\n$$\\text{Lasso (L1)}: \\quad \\mathcal{L} + \\lambda \\|\\mathbf{w}\\|_1$$\n\n| | Ridge | Lasso |\n|---|---|---|\n| 패널티 | $\\lambda\\|w\\|_2^2$ | $\\lambda\\|w\\|_1$ |\n| 효과 | 가중치 축소 | 가중치를 0으로 (특성 선택) |\n| PyTorch | `weight_decay` 파라미터 | 수동 구현 필요 |",
   "id": "c-p5-md2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 고차 다항식 + Ridge 정규화\nDEG = 9\nXhi = np.column_stack([x_poly**d for d in range(DEG+1)])\nXhi = (Xhi - Xhi.mean(0)) / (Xhi.std(0) + 1e-8)\nXev = np.column_stack([x_eval**d for d in range(DEG+1)])\nXev = (Xev - Xev.mean(0)) / (Xev.std(0) + 1e-8)\n\nlambdas    = [0.0, 0.01, 1.0, 10.0]\ncolors_reg = plt.cm.Blues(np.linspace(0.4, 1.0, 4))\n\nplt.figure(figsize=(10, 4))\nplt.scatter(x_poly, y_poly, alpha=0.5, s=20, color='gray', zorder=3)\nplt.plot(x_eval, 0.5*x_eval**3-2*x_eval**2+x_eval+3, 'k--', lw=1.5,\n         label='실제 함수', zorder=4)\nfor lam, color in zip(lambdas, colors_reg):\n    I = np.eye(Xhi.shape[1])\n    wr = np.linalg.solve(Xhi.T@Xhi + lam*I, Xhi.T@y_poly)\n    plt.plot(x_eval, Xev@wr, color=color, lw=2, label=f'Ridge l={lam}')\nplt.ylim(-25, 20)\nplt.title('Ridge 정규화 — λ 값에 따른 과적합 억제')\nplt.xlabel('x'); plt.ylabel('y')\nplt.legend(fontsize=9)\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": [],
   "id": "c-p5-code2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# PyTorch에서 Ridge 정규화 = weight_decay\nprint('[ 정규화 효과 비교 ]')\nprint(f'{\"방법\":<16}  {\"Val MSE\":>9}  {\"Test MSE\":>9}  {\"||w||2\":>8}')\nprint('-' * 50)\nfor name, wd in [('No Reg', 0.0), ('Ridge 1e-3', 1e-3), ('Ridge 1e-1', 1e-1)]:\n    m   = LinearRegression(8)\n    opt = torch.optim.Adam(m.parameters(), lr=0.01, weight_decay=wd)\n    for _ in range(80):\n        train_one_epoch(m, train_loader, criterion, opt)\n    va = evaluate(m, val_loader, criterion)\n    te = evaluate(m, test_loader, criterion)\n    wn = m.linear.weight.detach().norm().item()\n    print(f'{name:<16}  {va:>9.4f}  {te:>9.4f}  {wn:>8.4f}')",
   "execution_count": null,
   "outputs": [],
   "id": "c-p5-code3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise\n\n### Exercise 1. 정규 방정식 직접 구현\n\nCalifornia Housing 데이터의 **처음 2개 특성** (`MedInc`, `HouseAge`)만 사용해  \n정규 방정식 $\\mathbf{w}^* = (X^\\top X)^{-1} X^\\top \\mathbf{y}$를 직접 구현하고  \nMSE와 R²를 출력하세요. (`np.linalg.lstsq` 미사용)",
   "id": "c-ex-md1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Your code here\n",
   "execution_count": null,
   "outputs": [],
   "id": "c-ex1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2. Mini-batch 배치 크기 실험\n\n**batch_size = 1, 16, 64, 전체(n)** 4가지 경우의 수렴 곡선을 하나의 그래프에 그리고  \n최종 손실을 비교하세요.  \n데이터는 위에서 사용한 1D 예제 (`X_norm`, `y_norm`)를 사용하세요.",
   "id": "c-ex-md2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Your code here\n",
   "execution_count": null,
   "outputs": [],
   "id": "c-ex2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3. PyTorch 파이프라인 완성\n\n아래 클래스와 학습 루프를 완성하세요.\n\n- 모델: `nn.Linear(8, 1)` + **He 초기화** (`nn.init.kaiming_uniform_`) 적용\n- 옵티마이저: `Adam`, `lr=0.01`\n- 스케줄러: `StepLR(optimizer, step_size=30, gamma=0.5)` — 30 에폭마다 lr 절반\n- 100 에폭 학습 후 **Test MSE와 R²** 출력\n- 학습 곡선 (train loss, val loss)과 **학습률 변화** 함께 시각화",
   "id": "c-ex-md3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class LinearRegressionV2(nn.Module):\n    def __init__(self, in_features: int):\n        super().__init__()\n        self.linear = nn.Linear(in_features, 1)\n        # Your code here: He 초기화 적용\n\n    def forward(self, x):\n        # Your code here\n        pass\n\n\n# Your code here: 학습 루프 + 스케줄러 + 시각화\n",
   "execution_count": null,
   "outputs": [],
   "id": "c-ex3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n| 개념 | 핵심 내용 |\n|---|---|\n| **선형 회귀** | $\\hat{y} = \\mathbf{w}^\\top\\mathbf{x} + b$, 연속값 예측 |\n| **정규 방정식** | $(X^\\top X)^{-1}X^\\top\\mathbf{y}$, 소규모 데이터에 적합 |\n| **MSE vs MAE** | MSE는 이상치에 민감, MAE는 강건 |\n| **Batch GD** | 안정적이지만 느림, 전체 데이터 사용 |\n| **SGD** | 빠르지만 진동, 1개 샘플 사용 |\n| **Mini-batch** | 실제 딥러닝의 표준 방식 |\n| **학습률 η** | 가장 중요한 하이퍼파라미터 |\n| **autograd** | `requires_grad=True` → `.backward()` → `.grad` |\n| **nn.Linear** | `Linear(in, out)` 내부: `xW^T + b` |\n| **Ridge** | L2 정규화, `weight_decay`로 적용 |\n| **Lasso** | L1 정규화, 특성 선택 효과 |\n\n---\n\n**다음 강의 (Week 6):** 분류 — Logistic Regression, 결정 경계, Precision/Recall, ROC-AUC",
   "id": "c-summary"
  }
 ]
}