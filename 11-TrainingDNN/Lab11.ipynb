{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 11 — Training Deep Networks\n",
    "\n",
    "\n",
    "> **주제:** 딥 신경망 학습: 옵티마이저, 정규화, 배치 정규화, 학습률 스케줄링\n",
    "\n",
    "---\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| # | 목표 |  \n",
    "|---|---| \n",
    "| 1 | 옵티마이저 비교: SGD, Momentum, RMSProp, Adam |  \n",
    "| 2 | 정규화 기법: L1/L2, Dropout |  \n",
    "| 3 | 배치 정규화 & 레이어 정규화 |  \n",
    "| 4 | 학습률 스케줄링 |  \n",
    "| 5 | Exercise |  \n",
    "\n",
    "---\n",
    "\n",
    "**데이터셋:**\n",
    "- 시각화: 합성 2D 데이터 (최적화 궤적, 과적합 비교)\n",
    "- 분류: Digits (sklearn) — 64개 특성, 10개 클래스 (손글씨 숫자 0–9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device   : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. 옵티마이저 (Optimizers)\n",
    "\n",
    "### 1-1. 왜 다양한 옵티마이저가 필요한가?\n",
    "\n",
    "기본 경사 하강법(SGD)은 단순하지만, 실제 손실 지형(loss landscape)에서는 비효율적입니다.\n",
    "\n",
    "| 옵티마이저 | 업데이트 규칙 | 특징 |\n",
    "|---|---|---|\n",
    "| **SGD** | $w \\leftarrow w - \\eta \\nabla L$ | 단순, 느림, 하이퍼파라미터 민감 |\n",
    "| **Momentum** | $v \\leftarrow \\beta v + \\nabla L$; $w \\leftarrow w - \\eta v$ | 관성으로 지그재그 감소 |\n",
    "| **RMSProp** | $s \\leftarrow \\beta s + (1-\\beta)g^2$; $w \\leftarrow w - \\frac{\\eta}{\\sqrt{s+\\varepsilon}}g$ | 적응형 lr, 온라인 학습에 강함 |\n",
    "| **Adam** | Momentum + RMSProp 결합, 편향 보정 포함 | 대부분 상황에서 잘 작동 |\n",
    "\n",
    "### 1-2. 손실 지형 시각화\n",
    "\n",
    "$f(x, y) = x^2 + 10y^2$ (길쭉한 이차 함수)에서 각 옵티마이저의 최솟값 탐색 경로를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 궤적 시각화: f(x, y) = x² + 10y²\n",
    "def f(x, y):     return x**2 + 10*y**2\n",
    "def grad_f(x, y): return 2*x, 20*y\n",
    "\n",
    "def run_sgd(x0, y0, lr, n):\n",
    "    x, y = x0, y0; path = [(x, y)]\n",
    "    for _ in range(n):\n",
    "        gx, gy = grad_f(x, y); x -= lr*gx; y -= lr*gy; path.append((x, y))\n",
    "    return path\n",
    "\n",
    "def run_momentum(x0, y0, lr, beta, n):\n",
    "    x, y, vx, vy = x0, y0, 0., 0.; path = [(x, y)]\n",
    "    for _ in range(n):\n",
    "        gx, gy = grad_f(x, y)\n",
    "        vx = beta*vx + gx; vy = beta*vy + gy\n",
    "        x -= lr*vx; y -= lr*vy; path.append((x, y))\n",
    "    return path\n",
    "\n",
    "def run_rmsprop(x0, y0, lr, beta, eps, n):\n",
    "    x, y, sx, sy = x0, y0, 0., 0.; path = [(x, y)]\n",
    "    for _ in range(n):\n",
    "        gx, gy = grad_f(x, y)\n",
    "        sx = beta*sx + (1-beta)*gx**2; sy = beta*sy + (1-beta)*gy**2\n",
    "        x -= lr*gx/(np.sqrt(sx)+eps); y -= lr*gy/(np.sqrt(sy)+eps)\n",
    "        path.append((x, y))\n",
    "    return path\n",
    "\n",
    "def run_adam(x0, y0, lr, b1, b2, eps, n):\n",
    "    x, y, mx, my, vx, vy = x0, y0, 0., 0., 0., 0.; path = [(x, y)]\n",
    "    for t in range(1, n+1):\n",
    "        gx, gy = grad_f(x, y)\n",
    "        mx = b1*mx+(1-b1)*gx;  my = b1*my+(1-b1)*gy\n",
    "        vx = b2*vx+(1-b2)*gx**2; vy = b2*vy+(1-b2)*gy**2\n",
    "        mxh = mx/(1-b1**t); myh = my/(1-b1**t)\n",
    "        vxh = vx/(1-b2**t); vyh = vy/(1-b2**t)\n",
    "        x -= lr*mxh/(np.sqrt(vxh)+eps); y -= lr*myh/(np.sqrt(vyh)+eps)\n",
    "        path.append((x, y))\n",
    "    return path\n",
    "\n",
    "x0, y0, n_steps = 2.0, 2.0, 60\n",
    "paths = {\n",
    "    'SGD (lr=0.05)':        run_sgd(x0, y0, 0.05, n_steps),\n",
    "    'Momentum (β=0.9)':     run_momentum(x0, y0, 0.05, 0.9, n_steps),\n",
    "    'RMSProp':              run_rmsprop(x0, y0, 0.1, 0.9, 1e-8, n_steps),\n",
    "    'Adam':                 run_adam(x0, y0, 0.3, 0.9, 0.999, 1e-8, n_steps),\n",
    "}\n",
    "\n",
    "xr = np.linspace(-2.5, 2.5, 300); yr = np.linspace(-2.5, 2.5, 300)\n",
    "Xg, Yg = np.meshgrid(xr, yr); Zg = f(Xg, Yg)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "colors = ['steelblue', 'tomato', 'seagreen', 'darkorange']\n",
    "levels = [0.5, 2, 5, 10, 20, 40]\n",
    "\n",
    "# 궤적 그림\n",
    "for ax_i, (pair, title) in enumerate([\n",
    "    ([0,1], 'SGD vs Momentum'),\n",
    "    ([2,3], 'RMSProp vs Adam'),\n",
    "]):\n",
    "    ax = axes[ax_i]\n",
    "    cs = ax.contour(Xg, Yg, Zg, levels=levels, cmap='Blues', alpha=0.6)\n",
    "    ax.clabel(cs, fmt='%.0f', fontsize=8)\n",
    "    ax.plot(0, 0, 'r*', ms=15, zorder=10, label='최솟값')\n",
    "    for i in pair:\n",
    "        name = list(paths.keys())[i]\n",
    "        pts = paths[name]\n",
    "        xs, ys = zip(*pts)\n",
    "        ax.plot(xs, ys, 'o-', color=colors[i], ms=3, lw=1.5, label=name, alpha=0.85)\n",
    "        ax.plot(xs[0], ys[0], 's', color=colors[i], ms=10, zorder=5)\n",
    "    ax.set_title(title); ax.legend(fontsize=9)\n",
    "    ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "\n",
    "# 수렴 속도 비교 (로그 스케일)\n",
    "ax = axes[2]\n",
    "for (name, path), color in zip(paths.items(), colors):\n",
    "    losses = [f(x, y) for x, y in path]\n",
    "    ax.semilogy(losses, color=color, lw=2, label=name)\n",
    "ax.set_title('수렴 속도 (로그 스케일)')\n",
    "ax.set_xlabel('스텝'); ax.set_ylabel('f(x,y)'); ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('최적화 궤적 비교: f(x,y) = x² + 10y²', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('=== 최종 손실 비교 ===')\n",
    "for name, path in paths.items():\n",
    "    print(f'  {name:<20}: f = {f(*path[-1]):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digits 데이터셋 준비\n",
    "digits = load_digits()\n",
    "X = digits.data.astype(np.float32)\n",
    "y = digits.target\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr).astype(np.float32)\n",
    "X_te_s = scaler.transform(X_te).astype(np.float32)\n",
    "\n",
    "X_tr_t = torch.tensor(X_tr_s)\n",
    "y_tr_t = torch.tensor(y_tr, dtype=torch.long)\n",
    "X_te_t = torch.tensor(X_te_s)\n",
    "y_te_t = torch.tensor(y_te, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "print('=== Digits 데이터셋 ===')\n",
    "print(f'  총 샘플: {len(X):,}개')\n",
    "print(f'  특성 수: {X.shape[1]} (8×8 픽셀 → 64차원)')\n",
    "print(f'  클래스: {list(digits.target_names)}  (10개)')\n",
    "print(f'  Train  : {len(X_tr)}개  |  Test: {len(X_te)}개')\n",
    "\n",
    "# 샘플 시각화\n",
    "fig, axes = plt.subplots(2, 10, figsize=(14, 3))\n",
    "for d in range(10):\n",
    "    idx  = np.where(y == d)[0][0]\n",
    "    idx2 = np.where(y == d)[0][1]\n",
    "    axes[0, d].imshow(digits.images[idx],  cmap='gray_r'); axes[0, d].set_title(str(d), fontsize=10)\n",
    "    axes[1, d].imshow(digits.images[idx2], cmap='gray_r')\n",
    "    for r in range(2): axes[r, d].axis('off')\n",
    "plt.suptitle('Digits 샘플 (클래스별 2개)', fontsize=11)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 클래스 및 학습 함수 (이 셀은 이후 모든 실험에서 공유됩니다)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"배치 정규화, 레이어 정규화, 드롭아웃을 지원하는 MLP\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim,\n",
    "                 dropout=0.0, batch_norm=False, layer_norm=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            elif layer_norm:\n",
    "                layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_model(model, loader, X_val, y_val,\n",
    "                n_epochs=100, lr=0.001, optimizer_type='adam',\n",
    "                weight_decay=0.0, l1_lambda=0.0,\n",
    "                scheduler_type=None, verbose=True):\n",
    "    \"\"\"\n",
    "    optimizer_type: 'sgd', 'momentum', 'rmsprop', 'adam'\n",
    "    scheduler_type: None, 'step', 'cosine', 'plateau', 'exp'\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    opt_map = {\n",
    "        'sgd':      lambda: optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay),\n",
    "        'momentum': lambda: optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay),\n",
    "        'rmsprop':  lambda: optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay),\n",
    "        'adam':     lambda: optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay),\n",
    "    }\n",
    "    optimizer = opt_map[optimizer_type]()\n",
    "\n",
    "    scheduler = None\n",
    "    if scheduler_type == 'step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    elif scheduler_type == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    elif scheduler_type == 'plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    elif scheduler_type == 'exp':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    train_losses, val_losses, train_accs, val_accs, lr_hist = [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        ep_loss, correct, total = 0., 0, 0\n",
    "        for Xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            # L1 패널티 수동 추가\n",
    "            if l1_lambda > 0:\n",
    "                l1 = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + l1_lambda * l1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ep_loss  += loss.item() * len(Xb)\n",
    "            correct  += (logits.argmax(1) == yb).sum().item()\n",
    "            total    += len(yb)\n",
    "\n",
    "        train_losses.append(ep_loss / total)\n",
    "        train_accs.append(correct / total)\n",
    "        lr_hist.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            lv = model(X_val)\n",
    "            val_loss = criterion(lv, y_val).item()\n",
    "            val_acc  = (lv.argmax(1) == y_val).float().mean().item()\n",
    "        val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss) if scheduler_type == 'plateau' else scheduler.step()\n",
    "\n",
    "        if verbose and (epoch + 1) % 25 == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} | '\n",
    "                  f'Train {train_losses[-1]:.4f}/{train_accs[-1]:.4f} | '\n",
    "                  f'Val {val_loss:.4f}/{val_acc:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, lr_hist\n",
    "\n",
    "\n",
    "# 모델 확인\n",
    "demo = MLP(64, [128, 64], 10)\n",
    "print('=== MLP 구조 (64 → 128 → 64 → 10) ===')\n",
    "print(demo)\n",
    "print(f'\\n총 파라미터: {sum(p.numel() for p in demo.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4가지 옵티마이저 비교 (Digits 10-class Classification)\n",
    "opt_configs = [\n",
    "    ('SGD (lr=0.01)',      'sgd',      0.01),\n",
    "    ('Momentum (β=0.9)',   'momentum', 0.01),\n",
    "    ('RMSProp (lr=0.001)', 'rmsprop',  0.001),\n",
    "    ('Adam (lr=0.001)',    'adam',     0.001),\n",
    "]\n",
    "colors_opt = ['steelblue', 'tomato', 'seagreen', 'darkorange']\n",
    "\n",
    "opt_results = {}\n",
    "print('=== 옵티마이저 비교 (100 에포크) ===\\n')\n",
    "for name, opt_type, lr in opt_configs:\n",
    "    torch.manual_seed(42)\n",
    "    m = MLP(64, [128, 64], 10)\n",
    "    t_l, v_l, t_a, v_a, _ = train_model(\n",
    "        m, train_loader, X_te_t, y_te_t,\n",
    "        n_epochs=100, lr=lr, optimizer_type=opt_type, verbose=False\n",
    "    )\n",
    "    opt_results[name] = (t_l, v_l, t_a, v_a)\n",
    "    print(f'{name:<25}: Val Acc={v_a[-1]:.4f}  Val Loss={v_l[-1]:.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for (name, (_, v_l, _, v_a)), color in zip(opt_results.items(), colors_opt):\n",
    "    axes[0].plot(v_l, color=color, lw=2, label=name)\n",
    "    axes[1].plot(v_a, color=color, lw=2, label=f'{name} ({v_a[-1]:.4f})')\n",
    "\n",
    "for ax, title, ylabel in [\n",
    "    (axes[0], '옵티마이저별 Val Loss',     'CrossEntropy Loss'),\n",
    "    (axes[1], '옵티마이저별 Val Accuracy', 'Accuracy'),\n",
    "]:\n",
    "    ax.set_title(title); ax.set_xlabel('에포크')\n",
    "    ax.set_ylabel(ylabel); ax.legend(fontsize=9)\n",
    "axes[1].set_ylim(0.5, 1.02)\n",
    "\n",
    "plt.suptitle('옵티마이저 비교 (Digits)', fontsize=12)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. 정규화 (Regularization)\n",
    "\n",
    "### 2-1. 왜 정규화가 필요한가?\n",
    "\n",
    "모델이 너무 크거나 학습 데이터가 적으면 **과적합(Overfitting)** 이 발생합니다.\n",
    "정규화는 모델의 복잡도를 제한하여 **일반화(Generalization) 성능**을 높입니다.\n",
    "\n",
    "### 2-2. 주요 정규화 기법\n",
    "\n",
    "| 기법 | 설명 | PyTorch |\n",
    "|---|---|---|\n",
    "| **L2 (Weight Decay)** | $L_{\\text{total}} = L + \\lambda \\sum w^2$ | `optimizer(..., weight_decay=λ)` |\n",
    "| **L1** | $L_{\\text{total}} = L + \\lambda \\sum |w|$ | 직접 구현 필요 |\n",
    "| **Dropout** | 학습 시 뉴런을 확률 $p$로 무작위 비활성화 | `nn.Dropout(p)` |\n",
    "\n",
    "**Dropout 핵심:**\n",
    "- **학습 시**: 각 뉴런을 확률 $p$로 0으로 만들고, 남은 출력을 $\\frac{1}{1-p}$로 스케일\n",
    "- **추론 시**: 모든 뉴런을 사용 (`model.eval()` 호출 시 자동 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합 시나리오: 작은 학습 데이터 + 큰 모델\n",
    "np.random.seed(42)\n",
    "small_idx = np.random.choice(len(X_tr_t), 150, replace=False)\n",
    "X_small_t = X_tr_t[small_idx]\n",
    "y_small_t = y_tr_t[small_idx]\n",
    "\n",
    "small_ds     = TensorDataset(X_small_t, y_small_t)\n",
    "small_loader = DataLoader(small_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_overfit = MLP(64, [256, 256, 128], 10)\n",
    "print(f'=== 과적합 시나리오: 150개 학습 샘플, 큰 모델 ===')\n",
    "print(f'모델 파라미터: {sum(p.numel() for p in model_overfit.parameters()):,}개')\n",
    "\n",
    "t_l_ov, v_l_ov, t_a_ov, v_a_ov, _ = train_model(\n",
    "    model_overfit, small_loader, X_te_t, y_te_t,\n",
    "    n_epochs=200, lr=0.001, verbose=False\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "ep = range(1, 201)\n",
    "\n",
    "axes[0].plot(ep, t_l_ov, 'b-', lw=2, label=f'Train Loss')\n",
    "axes[0].plot(ep, v_l_ov, 'r-', lw=2, label=f'Val Loss')\n",
    "axes[0].set_title('과적합: Loss 곡선'); axes[0].set_xlabel('에포크'); axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(ep, t_a_ov, 'b-', lw=2, label=f'Train Acc ({t_a_ov[-1]:.4f})')\n",
    "axes[1].plot(ep, v_a_ov, 'r-', lw=2, label=f'Val Acc   ({v_a_ov[-1]:.4f})')\n",
    "axes[1].set_title('과적합: Accuracy 곡선'); axes[1].set_xlabel('에포크'); axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('과적합(Overfitting): Train ↑, Val ↓ — 일반화 실패!', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "gap = t_a_ov[-1] - v_a_ov[-1]\n",
    "print(f'\\nTrain Acc  : {t_a_ov[-1]:.4f}')\n",
    "print(f'Val   Acc  : {v_a_ov[-1]:.4f}')\n",
    "print(f'일반화 Gap : {gap:.4f}  (클수록 과적합 심각)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 기법 비교 (동일한 작은 데이터셋, 큰 모델)\n",
    "reg_configs = [\n",
    "    ('정규화 없음',        dict(dropout=0.0), 0.0,  0.0),\n",
    "    ('L2 (wd=0.01)',      dict(dropout=0.0), 0.01, 0.0),\n",
    "    ('L1 (λ=1e-4)',       dict(dropout=0.0), 0.0,  1e-4),\n",
    "    ('Dropout (p=0.4)',   dict(dropout=0.4), 0.0,  0.0),\n",
    "    ('L2 + Dropout',     dict(dropout=0.4), 0.01, 0.0),\n",
    "]\n",
    "colors_reg = ['gray', 'steelblue', 'purple', 'tomato', 'seagreen']\n",
    "\n",
    "reg_results = {}\n",
    "print('=== 정규화 기법 비교 ===\\n')\n",
    "print(f'{\"기법\":<20} {\"Train Acc\":>10} {\"Val Acc\":>10} {\"Gap\":>8}')\n",
    "print('-' * 52)\n",
    "\n",
    "for name, model_kw, wd, l1 in reg_configs:\n",
    "    torch.manual_seed(42)\n",
    "    m = MLP(64, [256, 256, 128], 10, **model_kw)\n",
    "    t_l, v_l, t_a, v_a, _ = train_model(\n",
    "        m, small_loader, X_te_t, y_te_t,\n",
    "        n_epochs=200, lr=0.001, weight_decay=wd, l1_lambda=l1, verbose=False\n",
    "    )\n",
    "    reg_results[name] = (t_l, v_l, t_a, v_a)\n",
    "    gap = t_a[-1] - v_a[-1]\n",
    "    print(f'{name:<20} {t_a[-1]:>10.4f} {v_a[-1]:>10.4f} {gap:>8.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ep = range(1, 201)\n",
    "for (name, (_, v_l, t_a, v_a)), color in zip(reg_results.items(), colors_reg):\n",
    "    axes[0].plot(ep, v_l, color=color, lw=2, label=name)\n",
    "    axes[1].plot(ep, v_a, color=color, lw=2, label=f'{name} ({v_a[-1]:.4f})')\n",
    "\n",
    "for ax, title, ylabel in [\n",
    "    (axes[0], 'Val Loss — 정규화 효과',     'Loss'),\n",
    "    (axes[1], 'Val Accuracy — 정규화 효과', 'Accuracy'),\n",
    "]:\n",
    "    ax.set_title(title); ax.set_xlabel('에포크')\n",
    "    ax.set_ylabel(ylabel); ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('정규화 기법 비교 (과적합 환경, 150개 학습 샘플)', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Dropout 동작 확인 (train vs eval 모드)\n",
    "model_test = MLP(64, [8], 10, dropout=0.5)\n",
    "x_test = torch.randn(1, 64)\n",
    "model_test.train()\n",
    "out_train = model_test(x_test).detach()\n",
    "model_test.eval()\n",
    "out_eval  = model_test(x_test).detach()\n",
    "print('\\nDropout 동작 확인 (출력 최댓값):')\n",
    "print(f'  train 모드 (Dropout 활성):  {out_train.max().item():.4f}')\n",
    "print(f'  eval  모드 (Dropout 비활성): {out_eval.max().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. 배치 정규화 & 레이어 정규화\n",
    "\n",
    "### 3-1. 배치 정규화 (Batch Normalization)\n",
    "\n",
    "**내부 공변량 이동(Internal Covariate Shift)**: 학습 중 각 층의 입력 분포가 계속 변하는 현상.\n",
    "이로 인해 학습 속도가 느려지고 높은 학습률 사용이 어렵습니다.\n",
    "\n",
    "**BN 해결책:** 각 미니배치에서 정규화 수행\n",
    "\n",
    "$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}, \\quad y = \\gamma \\hat{x} + \\beta$$\n",
    "\n",
    "| 항목 | 내용 |\n",
    "|---|---|\n",
    "| $\\mu_B, \\sigma_B^2$ | 미니배치의 평균과 분산 |\n",
    "| $\\gamma, \\beta$ | 학습 가능한 스케일 & 시프트 파라미터 |\n",
    "| **학습 시** | 미니배치 통계 사용 |\n",
    "| **추론 시** | 학습 중 누적한 이동 평균 통계 사용 |\n",
    "\n",
    "### 3-2. 레이어 정규화 (Layer Normalization)\n",
    "\n",
    "BN은 배치 전체로 정규화하지만, **LayerNorm** 은 샘플 하나의 특성 차원으로 정규화합니다.\n",
    "배치 크기가 작거나 RNN/Transformer에서 주로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm vs No-BN: 수렴 속도 및 안정성 비교\n",
    "bn_configs = [\n",
    "    ('No BN  (lr=0.01)',  False, False, 0.01),\n",
    "    ('No BN  (lr=0.001)', False, False, 0.001),\n",
    "    ('BatchNorm (lr=0.01)',  True,  False, 0.01),\n",
    "    ('BatchNorm (lr=0.001)', True,  False, 0.001),\n",
    "]\n",
    "colors_bn = ['steelblue', 'steelblue', 'tomato', 'tomato']\n",
    "styles_bn  = ['--', '-', '--', '-']\n",
    "\n",
    "bn_results = {}\n",
    "print('=== BatchNorm 효과 비교 ===\\n')\n",
    "for name, use_bn, use_ln, lr in bn_configs:\n",
    "    torch.manual_seed(42)\n",
    "    m = MLP(64, [128, 64], 10, batch_norm=use_bn, layer_norm=use_ln)\n",
    "    n_params = sum(p.numel() for p in m.parameters())\n",
    "    t_l, v_l, t_a, v_a, _ = train_model(\n",
    "        m, train_loader, X_te_t, y_te_t,\n",
    "        n_epochs=100, lr=lr, verbose=False\n",
    "    )\n",
    "    bn_results[name] = (v_l, v_a)\n",
    "    print(f'{name:<26}  params={n_params:,}  Val Acc={v_a[-1]:.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for (name, (v_l, v_a)), color, ls in zip(bn_results.items(), colors_bn, styles_bn):\n",
    "    label_acc = f'{name} ({v_a[-1]:.4f})'\n",
    "    axes[0].plot(v_l, color=color, lw=2, ls=ls, label=name)\n",
    "    axes[1].plot(v_a, color=color, lw=2, ls=ls, label=label_acc)\n",
    "\n",
    "for ax, title, ylabel in [\n",
    "    (axes[0], 'BatchNorm 효과: Val Loss',     'Loss'),\n",
    "    (axes[1], 'BatchNorm 효과: Val Accuracy', 'Accuracy'),\n",
    "]:\n",
    "    ax.set_title(title); ax.set_xlabel('에포크')\n",
    "    ax.set_ylabel(ylabel); ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Batch Normalization: 더 높은 lr에서도 안정적 학습!', fontsize=12)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN vs LayerNorm vs No-Norm 비교\n",
    "norm_configs = [\n",
    "    ('No Norm',    False, False),\n",
    "    ('BatchNorm',  True,  False),\n",
    "    ('LayerNorm',  False, True),\n",
    "]\n",
    "colors_norm = ['steelblue', 'tomato', 'seagreen']\n",
    "\n",
    "norm_results = {}\n",
    "print('=== BN vs LayerNorm vs No-Norm (lr=0.01) ===\\n')\n",
    "for name, bn, ln in norm_configs:\n",
    "    torch.manual_seed(42)\n",
    "    m = MLP(64, [128, 64], 10, batch_norm=bn, layer_norm=ln)\n",
    "    t_l, v_l, t_a, v_a, _ = train_model(\n",
    "        m, train_loader, X_te_t, y_te_t,\n",
    "        n_epochs=100, lr=0.01, verbose=False\n",
    "    )\n",
    "    norm_results[name] = (v_l, v_a)\n",
    "    print(f'{name:<12}: Val Acc={v_a[-1]:.4f}  최고 Val Acc={max(v_a):.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "for (name, (v_l, v_a)), color in zip(norm_results.items(), colors_norm):\n",
    "    axes[0].plot(v_l, color=color, lw=2, label=name)\n",
    "    axes[1].plot(v_a, color=color, lw=2, label=f'{name} ({v_a[-1]:.4f})')\n",
    "\n",
    "for ax, title, ylabel in [\n",
    "    (axes[0], '정규화별 Val Loss',     'Loss'),\n",
    "    (axes[1], '정규화별 Val Accuracy', 'Accuracy'),\n",
    "]:\n",
    "    ax.set_title(title); ax.set_xlabel('에포크')\n",
    "    ax.set_ylabel(ylabel); ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('No Norm vs BatchNorm vs LayerNorm (lr=0.01)', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\n[참고]')\n",
    "print('  BatchNorm: 배치 차원으로 정규화 → CNN, MLP에 적합')\n",
    "print('  LayerNorm: 특성 차원으로 정규화 → RNN, Transformer에 적합')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. 학습률 스케줄링 (Learning Rate Scheduling)\n",
    "\n",
    "### 왜 학습률을 조정하는가?\n",
    "\n",
    "- **초기 학습**: 큰 학습률 → 빠른 수렴\n",
    "- **후반 학습**: 작은 학습률 → 세밀한 최적화, 진동 방지\n",
    "\n",
    "| 스케줄러 | 방식 | 특징 |\n",
    "|---|---|---|\n",
    "| **StepLR** | N 에포크마다 lr × γ | 단순, 예측 가능 |\n",
    "| **CosineAnnealingLR** | 코사인 함수로 lr 감소 | 부드러운 감소, warm restart 응용 |\n",
    "| **ExponentialLR** | 매 에포크마다 lr × γ | 지수적 감소 |\n",
    "| **ReduceLROnPlateau** | Val Loss가 개선되지 않을 때 lr 감소 | 적응형, 실전에서 많이 사용 |\n",
    "\n",
    "```python\n",
    "# 사용 예시\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "for epoch in range(n_epochs):\n",
    "    # ... 학습 루프 ...\n",
    "    scheduler.step()          # 에포크 끝에 호출\n",
    "    # ReduceLROnPlateau는: scheduler.step(val_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 스케줄러 시각화 (실제 학습 없이 lr 변화만 확인)\n",
    "n_epochs_vis = 100\n",
    "init_lr = 0.01\n",
    "\n",
    "dummy_p = nn.Parameter(torch.tensor(0.0))  # 더미 파라미터\n",
    "\n",
    "schedulers_vis = {\n",
    "    '고정 lr':            (None, {}),\n",
    "    'StepLR (s=30, γ=0.5)': ('step',   {'step_size': 30, 'gamma': 0.5}),\n",
    "    'CosineAnnealingLR':   ('cosine', {'T_max': n_epochs_vis}),\n",
    "    'ExponentialLR (γ=0.95)': ('exp',  {'gamma': 0.95}),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "colors_sch = ['gray', 'steelblue', 'tomato', 'seagreen']\n",
    "\n",
    "for (name, (sch_type, sch_kw)), color in zip(schedulers_vis.items(), colors_sch):\n",
    "    opt_v = optim.SGD([dummy_p], lr=init_lr)\n",
    "    if sch_type == 'step':\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt_v, **sch_kw)\n",
    "    elif sch_type == 'cosine':\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt_v, **sch_kw)\n",
    "    elif sch_type == 'exp':\n",
    "        sch = torch.optim.lr_scheduler.ExponentialLR(opt_v, **sch_kw)\n",
    "    else:\n",
    "        sch = None\n",
    "    lrs = []\n",
    "    for ep in range(n_epochs_vis):\n",
    "        lrs.append(opt_v.param_groups[0]['lr'])\n",
    "        if sch: sch.step()\n",
    "    ax.plot(range(1, n_epochs_vis+1), lrs, color=color, lw=2, label=name)\n",
    "\n",
    "ax.set_title(f'학습률 스케줄러별 lr 변화 (초기 lr={init_lr})')\n",
    "ax.set_xlabel('에포크'); ax.set_ylabel('Learning Rate')\n",
    "ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('=== 학습률 핵심 포인트 ===')\n",
    "print('  - 너무 크면: 발산(loss 폭발)')\n",
    "print('  - 너무 작으면: 수렴 매우 느림')\n",
    "print('  - 스케줄링: 초반 크게 → 후반 작게 = 속도 + 정밀도')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케줄러 학습 비교\n",
    "sch_configs = [\n",
    "    ('Adam (고정 lr=0.001)',    'adam', None,     0.001),\n",
    "    ('Adam + StepLR',           'adam', 'step',   0.01),\n",
    "    ('Adam + CosineAnneal',     'adam', 'cosine', 0.01),\n",
    "    ('Adam + ExponentialLR',    'adam', 'exp',    0.01),\n",
    "]\n",
    "colors_sch2 = ['gray', 'steelblue', 'tomato', 'seagreen']\n",
    "\n",
    "sch_results = {}\n",
    "print('=== 학습률 스케줄링 비교 (100 에포크) ===\\n')\n",
    "for name, opt_type, sch_type, lr in sch_configs:\n",
    "    torch.manual_seed(42)\n",
    "    m = MLP(64, [128, 64], 10)\n",
    "    t_l, v_l, t_a, v_a, lr_hist = train_model(\n",
    "        m, train_loader, X_te_t, y_te_t,\n",
    "        n_epochs=100, lr=lr, optimizer_type=opt_type,\n",
    "        scheduler_type=sch_type, verbose=False\n",
    "    )\n",
    "    sch_results[name] = (v_a, lr_hist)\n",
    "    print(f'{name:<30}: 최종 Val Acc={v_a[-1]:.4f}  최고 Val Acc={max(v_a):.4f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "for (name, (v_a, lr_hist)), color in zip(sch_results.items(), colors_sch2):\n",
    "    axes[0].plot(range(1, 101), v_a,     color=color, lw=2, label=f'{name} ({v_a[-1]:.4f})')\n",
    "    axes[1].plot(range(1, 101), lr_hist, color=color, lw=2, label=name)\n",
    "\n",
    "axes[0].set_title('스케줄러별 Val Accuracy')\n",
    "axes[0].set_xlabel('에포크'); axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "axes[1].set_title('스케줄러별 lr 변화')\n",
    "axes[1].set_xlabel('에포크'); axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('학습률 스케줄링 비교', fontsize=12)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise\n",
    "\n",
    "### Exercise 1. 정규화 전략 심층 비교\n",
    "\n",
    "전체 학습 데이터(X_tr_t, y_tr_t)를 사용하여 아래 4가지 설정을 비교하세요.\n",
    "\n",
    "| 설정 | 내용 |\n",
    "|---|---|\n",
    "| 기준 | No regularization (dropout=0, weight_decay=0) |\n",
    "| A | Dropout p=0.3 |\n",
    "| B | L2 weight_decay=0.001 |\n",
    "| C | Dropout p=0.3 + L2 weight_decay=0.001 |\n",
    "\n",
    "**요구사항:**\n",
    "- 모델 구조: `hidden_dims=[128, 64]`, `n_epochs=100`, Adam, `lr=0.001`\n",
    "- Val Accuracy 학습 곡선 시각화 (4개 선)\n",
    "- 최종 Val Accuracy 및 Train Accuracy 출력 (일반화 Gap 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: 정규화 전략 비교\n",
    "configs_ex1 = {\n",
    "    '기준 (No Reg)':             dict(dropout=0.0, weight_decay=0.0),\n",
    "    'A: Dropout p=0.3':          dict(dropout=0.3, weight_decay=0.0),\n",
    "    'B: L2 wd=0.001':            dict(dropout=0.0, weight_decay=0.001),\n",
    "    'C: Dropout + L2':           dict(dropout=0.3, weight_decay=0.001),\n",
    "}\n",
    "\n",
    "ex1_results = {}\n",
    "\n",
    "for name, cfg in configs_ex1.items():\n",
    "    # Your code here: MLP 생성, train_model 호출, 결과 저장\n",
    "    pass\n",
    "\n",
    "# Your code here: Val Accuracy 학습 곡선 시각화\n",
    "\n",
    "# Your code here: 최종 Val Acc, Train Acc, Gap 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 2. BatchNorm + Dropout 조합 최적화\n",
    "\n",
    "아래 4가지 조합에서 최적의 설정을 찾으세요.\n",
    "\n",
    "| 모델 | batch_norm | dropout |\n",
    "|---|---|---|\n",
    "| Vanilla | False | 0.0 |\n",
    "| BN only | True | 0.0 |\n",
    "| Dropout only | False | 0.3 |\n",
    "| BN + Dropout | True | 0.3 |\n",
    "\n",
    "**요구사항:**\n",
    "- 구조: `hidden_dims=[256, 128, 64]`, `n_epochs=100`, Adam, `lr=0.01`\n",
    "- Val Loss + Val Accuracy 학습 곡선 시각화\n",
    "- 최종 성능 표 출력 (Val Acc, 파라미터 수 포함)\n",
    "\n",
    "**힌트:** BatchNorm 레이어도 파라미터를 추가합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: BN + Dropout 조합\n",
    "bn_drop_configs = {\n",
    "    'Vanilla':      dict(batch_norm=False, dropout=0.0),\n",
    "    'BN only':      dict(batch_norm=True,  dropout=0.0),\n",
    "    'Dropout only': dict(batch_norm=False, dropout=0.3),\n",
    "    'BN + Dropout': dict(batch_norm=True,  dropout=0.3),\n",
    "}\n",
    "\n",
    "ex2_results = {}\n",
    "\n",
    "for name, cfg in bn_drop_configs.items():\n",
    "    # Your code here: MLP 생성 (hidden_dims=[256, 128, 64], output_dim=10, lr=0.01)\n",
    "\n",
    "    # Your code here: train_model 호출, 결과 저장\n",
    "    pass\n",
    "\n",
    "# Your code here: Val Loss, Val Accuracy 곡선 시각화\n",
    "\n",
    "# Your code here: 최종 성능 표 (Val Acc, 파라미터 수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 3. (도전) ReduceLROnPlateau + Early Stopping\n",
    "\n",
    "**Early Stopping**: 검증 손실이 일정 에포크 동안 개선되지 않으면 학습을 조기 종료합니다.\n",
    "\n",
    "```\n",
    "최고 val_loss 갱신 시 → 모델 저장, 카운터 초기화\n",
    "개선 없을 때         → 카운터 +1\n",
    "카운터 ≥ patience   → 학습 종료\n",
    "```\n",
    "\n",
    "**요구사항:**\n",
    "1. `train_with_early_stopping()` 함수 구현\n",
    "   - `ReduceLROnPlateau` 스케줄러 사용 (patience=5, factor=0.5)\n",
    "   - Early stopping: patience=15\n",
    "2. 모델: `MLP(64, [128, 64], 10)`, Adam, 초기 `lr=0.01`, `max_epochs=300`\n",
    "3. 결과 출력: 실제 학습 에포크 수, 최종 Val Acc, lr 변화 곡선\n",
    "\n",
    "**힌트:**\n",
    "```python\n",
    "import copy\n",
    "best_model_state = copy.deepcopy(model.state_dict())  # 모델 상태 저장\n",
    "model.load_state_dict(best_model_state)                # 최고 상태로 복원\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_with_early_stopping(model, loader, X_val, y_val,\n",
    "                               max_epochs=300, lr=0.01, patience=15):\n",
    "    \"\"\"\n",
    "    ReduceLROnPlateau + Early Stopping 구현\n",
    "    Returns: val_accs, lr_history, actual_epochs\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Your code here: ReduceLROnPlateau 스케줄러 생성 (patience=5, factor=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    val_accs  = []\n",
    "    lr_history = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Your code here: 학습 루프 (train_loader 사용)\n",
    "\n",
    "        # Your code here: 검증 (val_loss, val_acc 계산)\n",
    "\n",
    "        # Your code here: val_accs, lr_history 기록\n",
    "\n",
    "        # Your code here: scheduler.step(val_loss)\n",
    "\n",
    "        # Your code here: Early stopping 체크\n",
    "        #   - val_loss < best_val_loss → best 갱신, 카운터 초기화, 모델 저장\n",
    "        #   - 그렇지 않으면 카운터 증가 → patience 초과 시 break\n",
    "        pass\n",
    "\n",
    "    # Your code here: 최고 모델 복원\n",
    "\n",
    "    return val_accs, lr_history, epoch + 1\n",
    "\n",
    "\n",
    "# Your code here: 실험 실행 및 결과 시각화\n",
    "torch.manual_seed(42)\n",
    "model_es = MLP(64, [128, 64], 10)\n",
    "\n",
    "# val_accs, lr_history, actual_ep = train_with_early_stopping(\n",
    "#     model_es, train_loader, X_te_t, y_te_t, max_epochs=300, lr=0.01, patience=15\n",
    "# )\n",
    "# print(f'실제 학습 에포크: {actual_ep} / 300')\n",
    "# print(f'최종 Val Accuracy: {val_accs[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| 개념 | 핵심 내용 |\n",
    "|---|---|\n",
    "| **SGD** | 기본 경사 하강법, 학습률에 민감, 지그재그 수렴 |\n",
    "| **Momentum** | 관성 누적으로 지그재그 감소, 빠른 수렴 |\n",
    "| **RMSProp** | 방향별 적응적 학습률, 비정상(non-stationary) 데이터에 강함 |\n",
    "| **Adam** | Momentum + RMSProp, 대부분 상황에서 첫 번째 선택 |\n",
    "| **L1 규제** | $\\sum|w|$ 패널티, 희소(sparse) 가중치 유도 |\n",
    "| **L2 규제** | $\\sum w^2$ 패널티, 가중치를 0 근처로 수축 |\n",
    "| **Dropout** | 학습 시 무작위 뉴런 비활성, 앙상블 효과 |\n",
    "| **BatchNorm** | 미니배치 정규화, 학습 안정성↑, 높은 lr 허용 |\n",
    "| **LayerNorm** | 샘플별 특성 정규화, RNN/Transformer에 적합 |\n",
    "| **StepLR** | N 에포크마다 lr × γ, 예측 가능한 감소 |\n",
    "| **CosineAnnealingLR** | 코사인 곡선으로 부드러운 감소 |\n",
    "| **ReduceLROnPlateau** | Val Loss 정체 시 자동 감소, 실전에서 많이 사용 |\n",
    "| **Early Stopping** | Val Loss 개선 없으면 조기 종료, 과적합 방지 |\n",
    "\n",
    "---\n",
    "\n",
    "**다음 강의 (Week 12):** CNN — 합성곱 신경망, 풀링, 이미지 분류"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
