{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 10 — Neural Networks: PyTorch 입문\n",
    "\n",
    "> **강의 시간:** 약 2시간  \n",
    "> **주제:** 퍼셉트론부터 다층 신경망(MLP) 학습까지\n",
    "\n",
    "---\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| # | 목표 | 예상 시간 |\n",
    "|---|---|---|\n",
    "| 1 | 퍼셉트론 원리와 NumPy 구현 | 25분 |\n",
    "| 2 | 활성화 함수 (ReLU, Sigmoid, Tanh) 특성 분석 | 15분 |\n",
    "| 3 | 순전파(Forward Pass) 직접 계산 | 20분 |\n",
    "| 4 | PyTorch 기초: Tensor, Autograd, nn.Module | 25분 |\n",
    "| 5 | MLP 학습 루프 및 성능 분석 | 30분 |\n",
    "\n",
    "---\n",
    "\n",
    "**데이터셋:**\n",
    "- 시각화용: 합성 2D 데이터 (make_moons)\n",
    "- 분류: Breast Cancer Wisconsin (sklearn) — 30개 특성, 이진 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 한글 폰트\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available : {torch.cuda.is_available()}')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device   : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. 퍼셉트론 (Perceptron)\n",
    "\n",
    "### 1-1. 퍼셉트론이란?\n",
    "\n",
    "퍼셉트론(Perceptron)은 인공 신경망의 가장 기본 단위입니다.\n",
    "\n",
    "```\n",
    "입력 (x₁, x₂, ..., xₙ)\n",
    "  × 가중치 (w₁, w₂, ..., wₙ)\n",
    "       ↓\n",
    "  가중합: z = Σ(wᵢ · xᵢ) + b\n",
    "       ↓\n",
    "  활성화 함수: ŷ = f(z)\n",
    "       ↓\n",
    "  출력 (ŷ)\n",
    "```\n",
    "\n",
    "| 구성 요소 | 역할 |\n",
    "|---|---|\n",
    "| **입력 (x)** | 특성 벡터 |\n",
    "| **가중치 (w)** | 각 입력의 중요도 |\n",
    "| **편향 (b)** | 결정 경계 이동 |\n",
    "| **활성화 함수 (f)** | 비선형성 추가 |\n",
    "\n",
    "**학습 규칙:** 예측 오류에 비례하여 가중치를 조정\n",
    "$$w \\leftarrow w + \\eta \\cdot (y - \\hat{y}) \\cdot x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy로 구현한 퍼셉트론\n",
    "class Perceptron:\n",
    "    \"\"\"NumPy로 구현한 단층 퍼셉트론\"\"\"\n",
    "\n",
    "    def __init__(self, n_features, lr=0.1, n_epochs=50):\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0.0\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.errors_ = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = X @ self.w + self.b\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.errors_ = []\n",
    "        for epoch in range(self.n_epochs):\n",
    "            errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                y_hat = self.predict(xi.reshape(1, -1))[0]\n",
    "                update = self.lr * (yi - y_hat)\n",
    "                self.w += update * xi\n",
    "                self.b += update\n",
    "                errors += int(update != 0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "\n",
    "# AND 게이트 학습\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "pct = Perceptron(n_features=2, lr=0.1, n_epochs=20)\n",
    "pct.fit(X_and, y_and)\n",
    "\n",
    "print('=== AND 게이트 학습 결과 ===')\n",
    "print(f'{\"입력 (x1, x2)\":>20} | {\"정답\":>5} | {\"예측\":>5}')\n",
    "print('-' * 38)\n",
    "for xi, yi in zip(X_and, y_and):\n",
    "    y_hat = pct.predict(xi.reshape(1, -1))[0]\n",
    "    status = '✓' if yi == y_hat else '✗'\n",
    "    print(f'{str(xi.astype(int)):>20} | {yi:>5} | {y_hat:>5} {status}')\n",
    "print(f'\\n학습된 가중치: w={pct.w.round(3)}, b={pct.b:.3f}')\n",
    "\n",
    "# 학습 과정 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(range(1, len(pct.errors_) + 1), pct.errors_, 'o-', color='steelblue', lw=2)\n",
    "axes[0].set_title('에포크별 오분류 수 (AND 게이트)')\n",
    "axes[0].set_xlabel('에포크')\n",
    "axes[0].set_ylabel('오분류 수')\n",
    "\n",
    "x1_range = np.linspace(-0.5, 1.5, 100)\n",
    "if abs(pct.w[1]) > 1e-6:\n",
    "    boundary = -(pct.w[0] * x1_range + pct.b) / pct.w[1]\n",
    "    axes[1].plot(x1_range, boundary, 'g--', lw=2.5, label='결정 경계')\n",
    "axes[1].scatter(X_and[y_and == 0, 0], X_and[y_and == 0, 1],\n",
    "                s=200, c='steelblue', label='0 (False)', zorder=5, marker='s')\n",
    "axes[1].scatter(X_and[y_and == 1, 0], X_and[y_and == 1, 1],\n",
    "                s=200, c='tomato', label='1 (True)', zorder=5, marker='^')\n",
    "for xi, yi in zip(X_and, y_and):\n",
    "    axes[1].annotate(f'({int(xi[0])},{int(xi[1])})', xy=(xi[0] + 0.05, xi[1] + 0.05), fontsize=11)\n",
    "axes[1].set_title('AND 게이트 — 학습된 결정 경계')\n",
    "axes[1].set_xlabel('x₁'); axes[1].set_ylabel('x₂')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(-0.5, 1.5); axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md2",
   "metadata": {},
   "source": [
    "### 1-2. 퍼셉트론의 한계 — XOR 문제\n",
    "\n",
    "단일 퍼셉트론은 **선형으로 분리되지 않는** 문제를 풀 수 없습니다.\n",
    "\n",
    "| 입력 | AND | OR | **XOR** |\n",
    "|---|---|---|---|\n",
    "| (0, 0) | 0 | 0 | **0** |\n",
    "| (0, 1) | 0 | 1 | **1** |\n",
    "| (1, 0) | 0 | 1 | **1** |\n",
    "| (1, 1) | 1 | 1 | **0** |\n",
    "\n",
    "XOR은 하나의 직선으로 분리할 수 없습니다. → **다층 퍼셉트론(MLP)** 이 필요합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR에서 퍼셉트론 실패 시각화\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "pct_xor = Perceptron(n_features=2, lr=0.1, n_epochs=100)\n",
    "pct_xor.fit(X_xor, y_xor)\n",
    "\n",
    "print('=== XOR 게이트 퍼셉트론 결과 ===')\n",
    "for xi, yi in zip(X_xor, y_xor):\n",
    "    y_hat = pct_xor.predict(xi.reshape(1, -1))[0]\n",
    "    status = '✓' if yi == y_hat else '✗'\n",
    "    print(f'  입력 {xi.astype(int)} → 정답: {yi}, 예측: {y_hat} {status}')\n",
    "print(f'\\n100 에포크 후 마지막 에포크 오분류: {pct_xor.errors_[-1]}개  → 수렴 불가!')\n",
    "\n",
    "# AND vs XOR 결정 경계 비교\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "x1_range = np.linspace(-0.5, 1.5, 100)\n",
    "\n",
    "# AND - 선형 분리 가능\n",
    "for xi, yi in zip(X_and, y_and):\n",
    "    axes[0].scatter(xi[0], xi[1], c='tomato' if yi == 1 else 'steelblue',\n",
    "                    s=300, zorder=5, marker='^' if yi == 1 else 's', edgecolors='black', lw=1.5)\n",
    "    axes[0].annotate(f'({int(xi[0])},{int(xi[1])})={yi}', xy=(xi[0] + 0.07, xi[1] + 0.07), fontsize=11)\n",
    "if abs(pct.w[1]) > 1e-6:\n",
    "    boundary = -(pct.w[0] * x1_range + pct.b) / pct.w[1]\n",
    "    axes[0].plot(x1_range, boundary, 'g-', lw=2.5, label='결정 경계')\n",
    "axes[0].set_title('AND — 선형 분리 가능 ✓\\n(하나의 직선으로 분리됨)', fontsize=11)\n",
    "axes[0].set_xlim(-0.5, 1.5); axes[0].set_ylim(-0.5, 1.5)\n",
    "axes[0].legend()\n",
    "\n",
    "# XOR - 선형 분리 불가능\n",
    "for xi, yi in zip(X_xor, y_xor):\n",
    "    axes[1].scatter(xi[0], xi[1], c='tomato' if yi == 1 else 'steelblue',\n",
    "                    s=300, zorder=5, marker='^' if yi == 1 else 's', edgecolors='black', lw=1.5)\n",
    "    axes[1].annotate(f'({int(xi[0])},{int(xi[1])})={yi}', xy=(xi[0] + 0.07, xi[1] + 0.07), fontsize=11)\n",
    "for angle in np.linspace(0, np.pi, 5):\n",
    "    x_l = np.array([-0.5, 1.5])\n",
    "    if np.cos(angle) != 0:\n",
    "        y_l = 0.5 + np.tan(angle) * (x_l - 0.5)\n",
    "        axes[1].plot(x_l, y_l, 'gray', lw=1, alpha=0.4, linestyle='--')\n",
    "axes[1].set_title('XOR — 선형 분리 불가 ✗\\n(어떤 직선으로도 분리 불가)', fontsize=11)\n",
    "axes[1].set_xlim(-0.5, 1.5); axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.suptitle('퍼셉트론의 한계: XOR은 단일 직선으로 분리 불가\\n→ 다층 퍼셉트론(MLP)이 필요!', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. 활성화 함수 (Activation Functions)\n",
    "\n",
    "### 2-1. 왜 활성화 함수가 필요한가?\n",
    "\n",
    "활성화 함수가 없다면 아무리 깊은 신경망도 **선형 변환**에 불과합니다.\n",
    "\n",
    "$$W_3(W_2(W_1 x)) = W_{\\text{합성}} x \\quad (\\text{선형})$$\n",
    "\n",
    "비선형 활성화 함수 $f$를 추가하면 복잡한 함수를 근사할 수 있습니다.\n",
    "\n",
    "### 2-2. 주요 활성화 함수 비교\n",
    "\n",
    "| 함수 | 식 | 출력 범위 | 특징 |\n",
    "|---|---|---|---|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | (0, 1) | 확률 해석 용이, 기울기 소실 |\n",
    "| **Tanh** | $\\tanh(z)$ | (-1, 1) | 0 중심, 기울기 소실 |\n",
    "| **ReLU** | $\\max(0, z)$ | $[0, \\infty)$ | 빠름, 기울기 소실 없음 |\n",
    "| **Leaky ReLU** | $\\max(0.01z, z)$ | $(-\\infty, \\infty)$ | 죽은 ReLU 개선 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 시각화\n",
    "z = np.linspace(-5, 5, 300)\n",
    "\n",
    "sigmoid   = 1 / (1 + np.exp(-z))\n",
    "tanh_val  = np.tanh(z)\n",
    "relu_val  = np.maximum(0, z)\n",
    "lrelu_val = np.where(z > 0, z, 0.01 * z)\n",
    "\n",
    "d_sigmoid  = sigmoid * (1 - sigmoid)\n",
    "d_tanh     = 1 - tanh_val ** 2\n",
    "d_relu     = np.where(z > 0, 1.0, 0.0)\n",
    "d_lrelu    = np.where(z > 0, 1.0, 0.01)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "funcs = [\n",
    "    ('Sigmoid',         sigmoid,   d_sigmoid,  'steelblue'),\n",
    "    ('Tanh',            tanh_val,  d_tanh,     'tomato'),\n",
    "    ('ReLU',            relu_val,  d_relu,     'seagreen'),\n",
    "    ('Leaky ReLU (α=0.01)', lrelu_val, d_lrelu, 'darkorange'),\n",
    "]\n",
    "\n",
    "for col, (name, f_val, df_val, color) in enumerate(funcs):\n",
    "    axes[0, col].plot(z, f_val, lw=2.5, color=color)\n",
    "    axes[0, col].axhline(0, color='k', lw=0.8, linestyle=':')\n",
    "    axes[0, col].axvline(0, color='k', lw=0.8, linestyle=':')\n",
    "    axes[0, col].set_title(name, fontsize=11)\n",
    "    axes[0, col].set_xlabel('z')\n",
    "    axes[0, col].set_ylabel('f(z)' if col == 0 else '')\n",
    "\n",
    "    axes[1, col].plot(z, df_val, lw=2.5, color=color, linestyle='--')\n",
    "    axes[1, col].axhline(0, color='k', lw=0.8, linestyle=':')\n",
    "    axes[1, col].axvline(0, color='k', lw=0.8, linestyle=':')\n",
    "    axes[1, col].set_title(f\"{name} 미분\", fontsize=11)\n",
    "    axes[1, col].set_xlabel('z')\n",
    "    axes[1, col].set_ylabel(\"f'(z)\" if col == 0 else '')\n",
    "    axes[1, col].set_ylim(-0.1, 1.2)\n",
    "\n",
    "plt.suptitle('활성화 함수와 미분 비교\\n(미분이 0에 가까우면 기울기 소실 발생)', fontsize=12, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('=== 최대 기울기 비교 (z=0에서) ===')\n",
    "print(f'  Sigmoid 최대 기울기: {d_sigmoid.max():.4f}')\n",
    "print(f'  Tanh    최대 기울기: {d_tanh.max():.4f}')\n",
    "print(f'  ReLU    최대 기울기: {d_relu.max():.4f}  (z>0 구간)')\n",
    "print()\n",
    "print('→ Sigmoid/Tanh: 포화 영역에서 기울기 ≈ 0  → 기울기 소실(Vanishing Gradient)!')\n",
    "print('→ ReLU: z>0에서 기울기 = 1  → 깊은 신경망에 적합')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 소실(Vanishing Gradient) 시뮬레이션\n",
    "# 여러 층을 통과할 때 기울기가 어떻게 곱해지는지 확인\n",
    "\n",
    "n_layers = 10\n",
    "\n",
    "def simulate_gradient(activation, z_init, n_layers):\n",
    "    grad = 1.0\n",
    "    history = [1.0]\n",
    "    z = z_init\n",
    "    for _ in range(n_layers):\n",
    "        if activation == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-z))\n",
    "            grad *= s * (1 - s)\n",
    "        elif activation == 'tanh':\n",
    "            t = np.tanh(z)\n",
    "            grad *= (1 - t ** 2)\n",
    "        elif activation == 'relu':\n",
    "            grad *= 1.0  # z>0 가정\n",
    "        history.append(grad)\n",
    "    return history\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "layers = list(range(n_layers + 1))\n",
    "\n",
    "# 포화 영역 (z=2.0)\n",
    "for act, color, label in [\n",
    "    ('sigmoid', 'steelblue', 'Sigmoid'),\n",
    "    ('tanh',    'tomato',    'Tanh'),\n",
    "    ('relu',    'seagreen',  'ReLU'),\n",
    "]:\n",
    "    grads = simulate_gradient(act, 2.0, n_layers)\n",
    "    axes[0].semilogy(layers, grads, 'o-', color=color, lw=2, label=label)\n",
    "\n",
    "axes[0].set_title('층별 누적 기울기 (포화 영역, z=2.0, 로그 스케일)')\n",
    "axes[0].set_xlabel('층 수')\n",
    "axes[0].set_ylabel('누적 기울기 (로그)')\n",
    "axes[0].legend()\n",
    "\n",
    "# 활성 영역 (z=0.5)\n",
    "for act, color, label in [\n",
    "    ('sigmoid', 'steelblue', 'Sigmoid'),\n",
    "    ('tanh',    'tomato',    'Tanh'),\n",
    "    ('relu',    'seagreen',  'ReLU'),\n",
    "]:\n",
    "    grads = simulate_gradient(act, 0.5, n_layers)\n",
    "    axes[1].plot(layers, grads, 'o-', color=color, lw=2, label=label)\n",
    "\n",
    "axes[1].set_title('층별 누적 기울기 (활성 영역, z=0.5)')\n",
    "axes[1].set_xlabel('층 수')\n",
    "axes[1].set_ylabel('누적 기울기')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('기울기 소실(Vanishing Gradient): 층이 깊어질수록 기울기가 사라진다', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "g_sig = simulate_gradient('sigmoid', 2.0, 10)\n",
    "g_relu = simulate_gradient('relu', 2.0, 10)\n",
    "print(f'Sigmoid 10층 누적 기울기 (포화): {g_sig[-1]:.2e}')\n",
    "print(f'ReLU    10층 누적 기울기:        {g_relu[-1]:.2e}')\n",
    "print('\\n→ ReLU가 깊은 신경망 학습에 유리한 이유!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. 다층 퍼셉트론(MLP) — 순전파\n",
    "\n",
    "### 3-1. MLP 아키텍처\n",
    "\n",
    "```\n",
    "입력층          은닉층 1        은닉층 2        출력층\n",
    "  x₁ ──┐\n",
    "  x₂ ──┼──→ [h₁¹] ──→ [h₁²] ──→ [ŷ]\n",
    "  ...  └── [h₂¹] ──→ [h₂²]\n",
    "              ...        ...\n",
    "```\n",
    "\n",
    "각 층의 계산:\n",
    "$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}, \\quad a^{(l)} = f\\left(z^{(l)}\\right)$$\n",
    "\n",
    "### 3-2. 순전파 직접 계산\n",
    "\n",
    "2개 입력 → 4개 은닉 → 4개 은닉 → 1개 출력 구조를 NumPy로 직접 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 순전파 직접 계산 (NumPy)\n",
    "# 아키텍처: 2 → 4 → 4 → 1\n",
    "np.random.seed(42)\n",
    "\n",
    "W1 = np.random.randn(4, 2) * 0.5   # 은닉층 1 가중치\n",
    "b1 = np.zeros(4)\n",
    "W2 = np.random.randn(4, 4) * 0.5   # 은닉층 2 가중치\n",
    "b2 = np.zeros(4)\n",
    "W3 = np.random.randn(1, 4) * 0.5   # 출력층 가중치\n",
    "b3 = np.zeros(1)\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def sigmoid_np(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def forward_pass(x):\n",
    "    \"\"\"2→4→4→1 MLP 순전파\"\"\"\n",
    "    z1 = W1 @ x + b1        # 은닉층 1 가중합\n",
    "    a1 = relu(z1)            # 은닉층 1 출력\n",
    "\n",
    "    z2 = W2 @ a1 + b2       # 은닉층 2 가중합\n",
    "    a2 = relu(z2)            # 은닉층 2 출력\n",
    "\n",
    "    z3 = W3 @ a2 + b3       # 출력층 가중합\n",
    "    a3 = sigmoid_np(z3)      # 출력층 (이진 분류)\n",
    "    return a1, a2, a3, z1, z2, z3\n",
    "\n",
    "\n",
    "x_sample = np.array([1.5, -0.5])\n",
    "a1, a2, a3, z1, z2, z3 = forward_pass(x_sample)\n",
    "\n",
    "print('=== MLP 순전파 (2 → 4 → 4 → 1) ===')\n",
    "print(f'\\n입력: {x_sample}')\n",
    "print(f'\\n[은닉층 1]')\n",
    "print(f'  z1 = W1 @ x + b1 = {z1.round(4)}')\n",
    "print(f'  a1 = ReLU(z1)     = {a1.round(4)}')\n",
    "print(f'\\n[은닉층 2]')\n",
    "print(f'  z2 = W2 @ a1 + b2 = {z2.round(4)}')\n",
    "print(f'  a2 = ReLU(z2)      = {a2.round(4)}')\n",
    "print(f'\\n[출력층]')\n",
    "print(f'  z3 = W3 @ a2 + b3 = {z3.round(4)}')\n",
    "print(f'  a3 = σ(z3)         = {a3.round(4)}')\n",
    "print(f'\\n예측 클래스: {int(a3[0] >= 0.5)}  (확률: {a3[0]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md2",
   "metadata": {},
   "source": [
    "### 3-3. MLP로 XOR 해결\n",
    "\n",
    "퍼셉트론이 풀 수 없었던 XOR 문제를 sklearn의 MLP로 해결합니다.\n",
    "\n",
    "**핵심 아이디어:** 은닉층이 입력을 비선형 변환하여 선형 분리 가능한 공간으로 매핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# XOR 데이터\n",
    "X_xor_t = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)\n",
    "y_xor_t = np.array([0, 1, 1, 0])\n",
    "\n",
    "# 퍼셉트론 (한계 확인)\n",
    "pct_xor2 = Perceptron(n_features=2, lr=0.1, n_epochs=100)\n",
    "pct_xor2.fit(X_xor_t, y_xor_t)\n",
    "acc_pct = accuracy_score(y_xor_t, pct_xor2.predict(X_xor_t))\n",
    "\n",
    "# MLP (해결)\n",
    "mlp_xor = MLPClassifier(hidden_layer_sizes=(4, 4), activation='relu',\n",
    "                         max_iter=10000, random_state=42)\n",
    "mlp_xor.fit(X_xor_t, y_xor_t)\n",
    "acc_mlp = accuracy_score(y_xor_t, mlp_xor.predict(X_xor_t))\n",
    "\n",
    "print('=== XOR 해결 비교 ===')\n",
    "print(f'퍼셉트론 정확도: {acc_pct:.2f}  (선형 분리 불가)')\n",
    "print(f'MLP       정확도: {acc_mlp:.2f}  (비선형 변환으로 해결!)')\n",
    "print()\n",
    "for xi, yi in zip(X_xor_t, y_xor_t):\n",
    "    y_hat_mlp = mlp_xor.predict(xi.reshape(1, -1))[0]\n",
    "    status = '✓' if yi == y_hat_mlp else '✗'\n",
    "    print(f'  MLP: 입력 {xi.astype(int)} → 정답: {yi}, 예측: {y_hat_mlp} {status}')\n",
    "\n",
    "# 결정 경계 비교 시각화\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 200), np.linspace(-0.5, 1.5, 200))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for ax, model, title in [\n",
    "    (axes[0], pct_xor2, f'퍼셉트론 (Acc={acc_pct:.2f})'),\n",
    "    (axes[1], mlp_xor,  f'MLP  4→4 (Acc={acc_mlp:.2f})'),\n",
    "]:\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    ax.contour(xx, yy, Z, colors='k', linewidths=1.5)\n",
    "    for xi, yi in zip(X_xor_t, y_xor_t):\n",
    "        ax.scatter(xi[0], xi[1], c='tomato' if yi == 1 else 'steelblue',\n",
    "                   s=300, zorder=5, marker='^' if yi == 1 else 's',\n",
    "                   edgecolors='black', lw=1.5)\n",
    "        ax.annotate(f'({int(xi[0])},{int(xi[1])})={yi}',\n",
    "                    xy=(xi[0] + 0.07, xi[1] + 0.07), fontsize=12)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel('x₁'); ax.set_ylabel('x₂')\n",
    "    ax.set_xlim(-0.5, 1.5); ax.set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.suptitle('XOR 문제: 퍼셉트론 vs MLP', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. PyTorch 기초 — Tensor & Autograd\n",
    "\n",
    "### 4-1. 텐서(Tensor)\n",
    "\n",
    "PyTorch의 핵심 자료구조입니다. NumPy 배열과 유사하지만 **GPU 가속** 및 **자동 미분(Autograd)** 을 지원합니다.\n",
    "\n",
    "| 작업 | NumPy | PyTorch |\n",
    "|---|---|---|\n",
    "| 배열 생성 | `np.array(...)` | `torch.tensor(...)` |\n",
    "| 난수 | `np.random.randn(...)` | `torch.randn(...)` |\n",
    "| 차원 변경 | `.reshape(...)` | `.view(...)` / `.reshape(...)` |\n",
    "| 행렬 곱 | `@` | `@` / `torch.matmul` |\n",
    "| NumPy 변환 | — | `.numpy()` |\n",
    "\n",
    "### 4-2. Autograd — 자동 미분\n",
    "\n",
    "`requires_grad=True`를 설정하면 PyTorch가 계산 그래프를 자동으로 구성합니다.  \n",
    "`.backward()`를 호출하면 체인 룰(Chain Rule)로 모든 파라미터의 기울기를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 텐서 기본 연산\n",
    "print('=== PyTorch 텐서 기본 연산 ===\\n')\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(f'a = {a}')\n",
    "print(f'b = {b}')\n",
    "print(f'a + b  = {a + b}')\n",
    "print(f'a * b  = {a * b}')\n",
    "print(f'a · b  = {torch.dot(a, b).item()}')\n",
    "\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "print(f'\\nA @ B =\\n{A @ B}')\n",
    "\n",
    "# NumPy 호환성\n",
    "np_arr = np.array([1.0, 2.0, 3.0])\n",
    "t_from_np = torch.from_numpy(np_arr)\n",
    "print(f'\\nNumPy → Tensor: {t_from_np}')\n",
    "print(f'Tensor → NumPy: {t_from_np.numpy()}')\n",
    "\n",
    "# 텐서 형태 변환\n",
    "x = torch.randn(3, 4)\n",
    "print(f'\\nx.shape          = {x.shape}')\n",
    "print(f'x.view(2,6).shape = {x.view(2, 6).shape}')\n",
    "print(f'x.reshape(12).shape = {x.reshape(12).shape}')\n",
    "print(f'x.T.shape        = {x.T.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd — 자동 미분\n",
    "print('=== Autograd: 기울기 자동 계산 ===\\n')\n",
    "\n",
    "# z = x² + 3y + 5 의 기울기 계산\n",
    "x = torch.tensor(2.0, requires_grad=True)   # ∂z/∂x = 2x = 4\n",
    "y = torch.tensor(3.0, requires_grad=True)   # ∂z/∂y = 3\n",
    "\n",
    "z = x ** 2 + 3 * y + 5\n",
    "\n",
    "print(f'x = {x.item()}, y = {y.item()}')\n",
    "print(f'z = x² + 3y + 5 = {z.item()}')\n",
    "\n",
    "z.backward()  # 역전파 실행\n",
    "\n",
    "print(f'\\n∂z/∂x = 2x = {x.grad.item():.1f}  (이론: {2 * 2.0:.1f})')\n",
    "print(f'∂z/∂y = 3  = {y.grad.item():.1f}  (이론: 3.0)')\n",
    "\n",
    "print('\\n=== 신경망 파라미터 기울기 계산 ===')\n",
    "# 단층 신경망: y_pred = σ(w·x + b)\n",
    "w = torch.tensor(0.5, requires_grad=True)\n",
    "b_param = torch.tensor(0.1, requires_grad=True)\n",
    "x_in = torch.tensor(2.0)\n",
    "y_true = torch.tensor(1.0)\n",
    "\n",
    "z_net = w * x_in + b_param\n",
    "y_pred = torch.sigmoid(z_net)\n",
    "loss = (y_pred - y_true) ** 2   # MSE 손실\n",
    "\n",
    "print(f'w={w.item():.2f}, b={b_param.item():.2f}, x={x_in.item():.1f}')\n",
    "print(f'z = w·x + b = {z_net.item():.4f}')\n",
    "print(f'ŷ = σ(z)   = {y_pred.item():.4f}')\n",
    "print(f'loss = (ŷ - 1)² = {loss.item():.4f}')\n",
    "\n",
    "loss.backward()\n",
    "print(f'\\n∂loss/∂w = {w.grad.item():.6f}')\n",
    "print(f'∂loss/∂b = {b_param.grad.item():.6f}')\n",
    "print('\\n→ PyTorch가 체인 룰을 자동으로 적용!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module로 신경망 구성\n",
    "print('=== nn.Module 기초 ===')\n",
    "\n",
    "# 방법 1: nn.Sequential (간단한 경우)\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print('nn.Sequential:')\n",
    "print(model_seq)\n",
    "\n",
    "# 파라미터 개수 확인\n",
    "total_params = sum(p.numel() for p in model_seq.parameters())\n",
    "print(f'\\n총 파라미터 수: {total_params:,}')\n",
    "\n",
    "# 순전파 테스트\n",
    "x_test = torch.randn(5, 4)  # 배치 크기 5, 특성 4\n",
    "y_test = model_seq(x_test)\n",
    "print(f'\\n입력 shape: {x_test.shape}')\n",
    "print(f'출력 shape: {y_test.shape}')\n",
    "print(f'출력값 (예측 확률): {y_test.detach().numpy().flatten().round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p5-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5. PyTorch로 MLP 학습\n",
    "\n",
    "### 5-1. PyTorch 학습 파이프라인\n",
    "\n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()                     # 학습 모드\n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()         # 기울기 초기화\n",
    "        y_pred = model(X_batch)       # 순전파\n",
    "        loss = criterion(y_pred, y_batch)  # 손실 계산\n",
    "        loss.backward()               # 역전파 (기울기 계산)\n",
    "        optimizer.step()              # 파라미터 업데이트\n",
    "```\n",
    "\n",
    "### 5-2. 데이터 준비 — Breast Cancer Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "X_bc = cancer.data.values.astype(np.float32)\n",
    "y_bc = cancer.target.values.astype(np.float32)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_bc, y_bc, test_size=0.2, random_state=42, stratify=y_bc\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr).astype(np.float32)\n",
    "X_te_s = scaler.transform(X_te).astype(np.float32)\n",
    "\n",
    "# PyTorch 텐서 변환\n",
    "X_tr_t = torch.tensor(X_tr_s)\n",
    "y_tr_t = torch.tensor(y_tr).unsqueeze(1)\n",
    "X_te_t = torch.tensor(X_te_s)\n",
    "y_te_t = torch.tensor(y_te).unsqueeze(1)\n",
    "\n",
    "# DataLoader (미니배치)\n",
    "train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "print('Breast Cancer Wisconsin 데이터셋')\n",
    "print(f'  특성 수   : {X_bc.shape[1]}')\n",
    "print(f'  Train     : {len(X_tr)}개  (양성={int(y_tr.sum())}, 악성={len(y_tr)-int(y_tr.sum())})')\n",
    "print(f'  Test      : {len(X_te)}개  (양성={int(y_te.sum())}, 악성={len(y_te)-int(y_te.sum())})')\n",
    "print(f'\\n텐서 형태: X_tr_t={X_tr_t.shape}, y_tr_t={y_tr_t.shape}')\n",
    "print(f'배치 크기: 32  →  {len(train_loader)}개 배치/에포크')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"다층 퍼셉트론 — PyTorch nn.Module 기반\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1,\n",
    "                 activation='relu', dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        act_map = {'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid(), 'tanh': nn.Tanh()}\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(act_map.get(activation, nn.ReLU()))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# 모델 구조 확인\n",
    "model_demo = MLP(input_dim=30, hidden_dims=[64, 32], output_dim=1,\n",
    "                 activation='relu', dropout=0.2)\n",
    "print('=== MLP 모델 구조 (30 → 64 → 32 → 1) ===')\n",
    "print(model_demo)\n",
    "total_params = sum(p.numel() for p in model_demo.parameters())\n",
    "print(f'\\n총 파라미터 수: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_model(model, train_loader, X_val, y_val,\n",
    "                n_epochs=100, lr=0.001, optimizer_name='adam'):\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # 학습 단계\n",
    "        model.train()\n",
    "        epoch_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(X_batch)\n",
    "            correct += ((y_pred >= 0.5).float() == y_batch).sum().item()\n",
    "            total += len(y_batch)\n",
    "\n",
    "        train_losses.append(epoch_loss / total)\n",
    "        train_accs.append(correct / total)\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val).item()\n",
    "            val_acc = ((y_val_pred >= 0.5).float() == y_val).float().mean().item()\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} | '\n",
    "                  f'Train Loss: {train_losses[-1]:.4f} | '\n",
    "                  f'Val Loss: {val_loss:.4f} | '\n",
    "                  f'Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "\n",
    "# MLP 학습 실행\n",
    "torch.manual_seed(42)\n",
    "model = MLP(input_dim=30, hidden_dims=[64, 32], output_dim=1,\n",
    "            activation='relu', dropout=0.2)\n",
    "print('=== MLP (30→64→32→1) 학습 시작 ===')\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model, train_loader, X_te_t, y_te_t, n_epochs=100, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "axes[0].plot(epochs, train_losses, 'b-', lw=2, label='Train Loss')\n",
    "axes[0].plot(epochs, val_losses,   'r-', lw=2, label='Val Loss')\n",
    "axes[0].set_title('Loss 곡선')\n",
    "axes[0].set_xlabel('에포크')\n",
    "axes[0].set_ylabel('BCE Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(epochs, train_accs, 'b-', lw=2, label='Train Accuracy')\n",
    "axes[1].plot(epochs, val_accs,   'r-', lw=2, label='Val Accuracy')\n",
    "axes[1].set_title('Accuracy 곡선')\n",
    "axes[1].set_xlabel('에포크')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0.7, 1.02)\n",
    "\n",
    "plt.suptitle('MLP 학습 곡선 (Breast Cancer)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최종 성능\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_final = model(X_te_t).numpy()\n",
    "y_labels = (y_pred_final >= 0.5).astype(int).flatten()\n",
    "test_acc = accuracy_score(y_te, y_labels)\n",
    "test_auc = roc_auc_score(y_te, y_pred_final.flatten())\n",
    "print(f'최종 Test Accuracy : {test_acc:.4f}')\n",
    "print(f'최종 Test ROC-AUC  : {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 비교: Adam vs SGD vs RMSprop\n",
    "print('=== 옵티마이저 비교 (각 100 에포크) ===\\n')\n",
    "\n",
    "optimizer_configs = [\n",
    "    ('Adam',    'adam',    0.001),\n",
    "    ('SGD',     'sgd',     0.01),\n",
    "    ('RMSprop', 'rmsprop', 0.001),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "colors_opt = ['steelblue', 'tomato', 'seagreen']\n",
    "epochs_range = range(1, 101)\n",
    "\n",
    "for (name, opt_name, lr), color in zip(optimizer_configs, colors_opt):\n",
    "    torch.manual_seed(42)\n",
    "    m = MLP(input_dim=30, hidden_dims=[64, 32], output_dim=1, activation='relu')\n",
    "    t_l, v_l, t_a, v_a = train_model(m, train_loader, X_te_t, y_te_t,\n",
    "                                      n_epochs=100, lr=lr, optimizer_name=opt_name)\n",
    "    axes[0].plot(epochs_range, v_l, color=color, lw=2, label=f'{name} (lr={lr})')\n",
    "    axes[1].plot(epochs_range, v_a, color=color, lw=2,\n",
    "                 label=f'{name} (최종: {v_a[-1]:.4f})')\n",
    "    print(f'{name:>8}: 최종 Val Acc={v_a[-1]:.4f}  Val Loss={v_l[-1]:.4f}')\n",
    "\n",
    "axes[0].set_title('옵티마이저별 Val Loss')\n",
    "axes[0].set_xlabel('에포크'); axes[0].set_ylabel('BCE Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('옵티마이저별 Val Accuracy')\n",
    "axes[1].set_xlabel('에포크'); axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0.85, 1.02)\n",
    "\n",
    "plt.suptitle('옵티마이저 비교', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p5-code6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 모델과 성능 비교\n",
    "sklearn_models = {\n",
    "    'Logistic Regression':   LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest':         RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting':     GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "results_cmp = {}\n",
    "for name, m in sklearn_models.items():\n",
    "    m.fit(X_tr_s, y_tr)\n",
    "    y_pred = m.predict(X_te_s)\n",
    "    y_prob = m.predict_proba(X_te_s)[:, 1]\n",
    "    results_cmp[name] = (accuracy_score(y_te, y_pred), roc_auc_score(y_te, y_prob))\n",
    "\n",
    "# MLP (PyTorch) 결과 추가\n",
    "results_cmp['MLP (PyTorch)'] = (test_acc, test_auc)\n",
    "\n",
    "# 출력\n",
    "print(f'{\"모델\":<28} {\"Accuracy\":>10} {\"ROC-AUC\":>10}')\n",
    "print('-' * 52)\n",
    "for name, (acc, auc) in sorted(results_cmp.items(), key=lambda x: x[1][1]):\n",
    "    best_mark = ' ★' if auc == max(v[1] for v in results_cmp.values()) else ''\n",
    "    print(f'{name:<28} {acc:>10.4f} {auc:>10.4f}{best_mark}')\n",
    "\n",
    "# 시각화\n",
    "sorted_items = sorted(results_cmp.items(), key=lambda x: x[1][1])\n",
    "names_cmp = [n for n, _ in sorted_items]\n",
    "accs_cmp  = [v[0] for _, v in sorted_items]\n",
    "aucs_cmp  = [v[1] for _, v in sorted_items]\n",
    "bar_colors = ['#aed6f1' if 'MLP' not in n else 'steelblue' for n in names_cmp]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "for ax, vals, title in [(axes[0], accs_cmp, 'Test Accuracy'),\n",
    "                         (axes[1], aucs_cmp, 'Test ROC-AUC')]:\n",
    "    bars = ax.barh(names_cmp, vals, color=bar_colors, edgecolor='k', alpha=0.85)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(v + 0.001, bar.get_y() + bar.get_height() / 2,\n",
    "                f'{v:.4f}', va='center', fontsize=9)\n",
    "    ax.set_title(f'모델별 {title}')\n",
    "    ax.set_xlabel(title)\n",
    "    ax.set_xlim(0.9, 1.02)\n",
    "\n",
    "plt.suptitle('MLP vs sklearn 모델 성능 비교 (Breast Cancer)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise\n",
    "\n",
    "### Exercise 1. PyTorch로 OR 게이트 학습\n",
    "\n",
    "`nn.Sequential`을 사용하여 OR 게이트를 학습하는 단층 퍼셉트론을 구현하세요.\n",
    "\n",
    "| 입력 (x1, x2) | OR 출력 |\n",
    "|---|---|\n",
    "| (0, 0) | 0 |\n",
    "| (0, 1) | 1 |\n",
    "| (1, 0) | 1 |\n",
    "| (1, 1) | 1 |\n",
    "\n",
    "**요구사항:**\n",
    "- `nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())` 구성\n",
    "- BCE 손실 함수 / SGD 옵티마이저 (lr=0.5)\n",
    "- 200 에포크 학습\n",
    "- 에포크별 손실 시각화\n",
    "- 4개 입력 모두에 대한 예측 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: OR 게이트 학습\n",
    "X_or = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "y_or = torch.tensor([[0.], [1.], [1.], [1.]])\n",
    "\n",
    "# Your code here: 모델 정의 (nn.Sequential)\n",
    "model_or = None\n",
    "\n",
    "# Your code here: 손실 함수, 옵티마이저 정의\n",
    "\n",
    "# Your code here: 200 에포크 학습\n",
    "\n",
    "# Your code here: 에포크별 손실 시각화\n",
    "\n",
    "# Your code here: 결과 출력 (4개 입력에 대한 예측)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 2. 은닉층 크기 효과 분석\n",
    "\n",
    "Breast Cancer 데이터에서 아래 4가지 아키텍처를 비교하세요.\n",
    "\n",
    "| 모델 이름 | hidden_dims |\n",
    "|---|---|\n",
    "| Shallow | `[16]` |\n",
    "| Medium | `[64, 32]` |\n",
    "| Deep | `[128, 64, 32]` |\n",
    "| Wide | `[256]` |\n",
    "\n",
    "**요구사항:**\n",
    "- 각 모델: `n_epochs=100`, `lr=0.001`, Adam 옵티마이저\n",
    "- Val Loss 및 Val Accuracy 학습 곡선 시각화 (4개 모델 비교)\n",
    "- 최종 Val Accuracy 비교 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: 아키텍처 비교\n",
    "architectures = {\n",
    "    'Shallow [16]':        [16],\n",
    "    'Medium [64, 32]':     [64, 32],\n",
    "    'Deep [128, 64, 32]':  [128, 64, 32],\n",
    "    'Wide [256]':          [256],\n",
    "}\n",
    "\n",
    "arch_results = {}\n",
    "\n",
    "for name, hidden_dims in architectures.items():\n",
    "    # Your code here: 각 아키텍처 학습 및 결과 저장\n",
    "    pass\n",
    "\n",
    "# Your code here: Val Loss & Val Accuracy 학습 곡선 시각화\n",
    "\n",
    "# Your code here: 최종 Val Accuracy 비교 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 3. (도전) 학습률 스케줄링\n",
    "\n",
    "Adam 옵티마이저에 학습률 스케줄러를 적용하여 세 가지 전략을 비교하세요.\n",
    "\n",
    "| 전략 | 설명 |\n",
    "|---|---|\n",
    "| **고정 lr** | `lr=0.001` 고정 |\n",
    "| **StepLR** | `step_size=30, gamma=0.5` (30 에포크마다 절반) |\n",
    "| **CosineAnnealingLR** | `T_max=100` (코사인 형태로 감소) |\n",
    "\n",
    "**요구사항:**\n",
    "- 은닉층: `[64, 32]`, `n_epochs=100`, 초기 `lr=0.001`\n",
    "- Val Accuracy 비교 곡선 시각화\n",
    "- 학습률 변화 곡선 시각화\n",
    "\n",
    "**힌트:**\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "# 에포크 끝마다:\n",
    "scheduler.step()\n",
    "# 현재 학습률:\n",
    "current_lr = optimizer.param_groups[0]['lr']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: 학습률 스케줄링\n",
    "def train_with_scheduler(hidden_dims, n_epochs, lr, scheduler_type='none'):\n",
    "    \"\"\"\n",
    "    scheduler_type: 'none', 'step', 'cosine'\n",
    "    Returns: val_accs, lr_history\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    model_sch = MLP(input_dim=30, hidden_dims=hidden_dims, output_dim=1, activation='relu')\n",
    "    criterion_sch = nn.BCELoss()\n",
    "    optimizer_sch = optim.Adam(model_sch.parameters(), lr=lr)\n",
    "\n",
    "    # Your code here: 스케줄러 생성 (scheduler_type에 따라 다르게)\n",
    "\n",
    "    val_accs = []\n",
    "    lr_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Your code here: 학습 루프\n",
    "\n",
    "        # Your code here: 검증 및 val_acc 기록\n",
    "\n",
    "        # Your code here: 학습률 기록 및 scheduler.step() 호출\n",
    "        pass\n",
    "\n",
    "    return val_accs, lr_history\n",
    "\n",
    "\n",
    "# Your code here: 세 가지 전략으로 학습 실행\n",
    "\n",
    "# Your code here: Val Accuracy 비교 및 학습률 곡선 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| 개념 | 핵심 내용 |\n",
    "|---|---|\n",
    "| **퍼셉트론** | 입력 × 가중치 + 편향 → 활성화 함수 → 출력 |\n",
    "| **퍼셉트론 한계** | 선형 분리 불가능 문제(XOR) 해결 불가 |\n",
    "| **MLP** | 은닉층 추가 → 비선형 문제 해결 가능 |\n",
    "| **Sigmoid** | (0,1) 범위, 이진 출력, 포화 시 기울기 소실 |\n",
    "| **ReLU** | z>0이면 기울기=1, 기울기 소실 방지, 현재 가장 많이 사용 |\n",
    "| **순전파** | $z = Wx + b \\to a = f(z)$, 층별 반복 |\n",
    "| **역전파** | 체인 룰로 기울기 계산, 출력→입력 방향 |\n",
    "| **PyTorch Tensor** | NumPy + GPU + Autograd |\n",
    "| **Autograd** | `requires_grad=True` → `.backward()` → `.grad` |\n",
    "| **nn.Module** | 모델 정의, `forward()` 구현 |\n",
    "| **학습 루프** | `zero_grad → forward → loss → backward → step` |\n",
    "| **Adam** | 모멘텀 + 적응형 학습률, 가장 널리 사용 |\n",
    "\n",
    "---\n",
    "\n",
    "**다음 강의 (Week 11):** CNN — 합성곱 신경망, 풀링, 이미지 분류"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
