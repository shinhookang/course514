{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 06 — Classification\n",
    "\n",
    "> **강의 시간:** 약 1시간  \n",
    "> **주제:** 로지스틱 회귀부터 PyTorch 소프트맥스 분류기까지\n",
    "\n",
    "---\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| # | 목표 |  \n",
    "|---|---| \n",
    "| 1 | Sigmoid / 결정 경계 이해 및 시각화 |  \n",
    "| 2 | 소프트맥스와 크로스 엔트로피 손실 |  \n",
    "| 3 | Iris 데이터셋으로 PyTorch 분류기 구현 |  \n",
    "| 4 | 평가 지표: 정확도, 정밀도, 재현율, F1, 혼동 행렬 |  \n",
    "| 5 | Exercise: MNIST 소프트맥스 분류기 |  \n",
    "\n",
    "---\n",
    "\n",
    "**데이터셋:** Iris (sklearn) — 붓꽃 3종 분류 / MNIST — 손글씨 숫자 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# 한글 폰트\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "print('PyTorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. 로지스틱 회귀와 결정 경계\n",
    "\n",
    "### 1-1. 왜 선형 회귀로 분류를 못 할까?\n",
    "\n",
    "**분류(Classification)** 는 출력이 **이산적인 클래스 레이블**입니다.  \n",
    "선형 회귀($\\hat{y} = \\mathbf{w}^\\top\\mathbf{x}$)는 $(-\\infty, +\\infty)$ 범위의 값을 내놓기 때문에  \n",
    "확률(0~1)을 나타내기에 부적합합니다.\n",
    "\n",
    "**로지스틱 회귀**는 선형 출력을 **시그모이드 함수**로 압축합니다:\n",
    "\n",
    "$$\\hat{p} = \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad z = \\mathbf{w}^\\top\\mathbf{x} + b$$\n",
    "\n",
    "- $\\hat{p} \\geq 0.5$ → 클래스 1로 예측\n",
    "- $\\hat{p} < 0.5$ → 클래스 0으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수 시각화\n",
    "z = np.linspace(-6, 6, 300)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "grad    = sigmoid * (1 - sigmoid)   # 도함수\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(z, sigmoid, color='steelblue', lw=2.5)\n",
    "axes[0].axhline(0.5, color='tomato', linestyle='--', lw=1.5, label='임계값 0.5')\n",
    "axes[0].axvline(0.0, color='gray', linestyle='--', lw=1.0)\n",
    "axes[0].fill_between(z, 0.5, sigmoid, where=(sigmoid >= 0.5),\n",
    "                     alpha=0.15, color='steelblue', label='클래스 1 영역')\n",
    "axes[0].fill_between(z, sigmoid, 0.5, where=(sigmoid < 0.5),\n",
    "                     alpha=0.15, color='tomato', label='클래스 0 영역')\n",
    "axes[0].set_title('시그모이드 함수 $\\\\sigma(z)$')\n",
    "axes[0].set_xlabel('z (선형 출력)')\n",
    "axes[0].set_ylabel('$\\\\hat{p}$ (클래스 1 확률)')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "axes[1].plot(z, grad, color='seagreen', lw=2.5, label=\"$\\\\sigma'(z) = \\\\sigma(1-\\\\sigma)$\")\n",
    "axes[1].axvline(0, color='gray', linestyle='--', lw=1.0)\n",
    "axes[1].set_title('시그모이드 도함수')\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].set_ylabel('그래디언트')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('σ(0) =', 1/(1+np.exp(0)))\n",
    "print('σ(-∞) → 0,  σ(+∞) → 1')\n",
    "print('z=0일 때 그래디언트 최대:', 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md2",
   "metadata": {},
   "source": [
    "### 1-2. 이진 분류 — Iris 데이터 (Setosa vs Non-Setosa)\n",
    "\n",
    "Iris 데이터셋에서 **꽃잎 길이(petal length), 꽃잎 너비(petal width)** 2개 특성으로  \n",
    "**Setosa(0) vs Non-Setosa(1)** 를 분류합니다.\n",
    "\n",
    "**결정 경계(Decision Boundary):** $\\mathbf{w}^\\top\\mathbf{x} + b = 0$ 인 선  \n",
    "→ 이 선을 기준으로 두 클래스가 나뉩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터 로드 및 이진 분류 준비\n",
    "iris   = load_iris()\n",
    "X_iris = iris.data[:, 2:4].astype(np.float32)   # petal length, petal width\n",
    "y_bin  = (iris.target != 0).astype(np.float32)   # Setosa=0, 나머지=1\n",
    "\n",
    "scaler  = StandardScaler()\n",
    "X_s     = scaler.fit_transform(X_iris)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_s, y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",
    ")\n",
    "\n",
    "print(f'전체 샘플: {len(X_iris)}')\n",
    "print(f'클래스 분포 — Setosa: {(y_bin==0).sum()}, Non-Setosa: {(y_bin==1).sum()}')\n",
    "\n",
    "# 산점도\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "colors_cls = ['steelblue', 'tomato']\n",
    "labels_cls = ['Setosa (0)', 'Non-Setosa (1)']\n",
    "for c, color, label in zip([0, 1], colors_cls, labels_cls):\n",
    "    mask = y_bin == c\n",
    "    ax.scatter(X_iris[mask, 0], X_iris[mask, 1],\n",
    "               color=color, label=label, s=60, edgecolors='k', alpha=0.8)\n",
    "ax.set_xlabel('꽃잎 길이 (cm)')\n",
    "ax.set_ylabel('꽃잎 너비 (cm)')\n",
    "ax.set_title('Iris — Setosa vs Non-Setosa')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md3",
   "metadata": {},
   "source": [
    "### 1-3. 이진 크로스 엔트로피 손실 (BCE Loss)\n",
    "\n",
    "MSE 대신 **Binary Cross-Entropy**를 사용합니다:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n}\\sum_{i=1}^{n}\\left[ y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i) \\right]$$\n",
    "\n",
    "- $y=1$이면 $-\\log\\hat{p}$: 예측 확률이 높을수록 손실↓\n",
    "- $y=0$이면 $-\\log(1-\\hat{p})$: 예측 확률이 낮을수록 손실↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE Loss 시각화\n",
    "p = np.linspace(1e-6, 1 - 1e-6, 300)\n",
    "bce_y1 = -np.log(p)        # y=1일 때\n",
    "bce_y0 = -np.log(1 - p)    # y=0일 때\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(p, bce_y1, color='steelblue', lw=2.5, label='y=1: $-\\\\log(\\\\hat{p})$')\n",
    "ax.plot(p, bce_y0, color='tomato',    lw=2.5, label='y=0: $-\\\\log(1-\\\\hat{p})$')\n",
    "ax.axvline(0.5, color='gray', linestyle='--', lw=1.2)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_xlabel('예측 확률 $\\\\hat{p}$')\n",
    "ax.set_ylabel('손실')\n",
    "ax.set_title('Binary Cross-Entropy Loss')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PyTorch BCE 예시\n",
    "y_true_ex = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "y_pred_ex = torch.tensor([0.9, 0.1, 0.4, 0.8])\n",
    "bce = nn.BCELoss()(y_pred_ex, y_true_ex)\n",
    "print(f'BCE Loss 예시: {bce.item():.4f}')\n",
    "print(f'  예측 맞음 (0.9→1, 0.1→0): 손실 작음')\n",
    "print(f'  예측 틀림 (0.4→1, 0.8→0): 손실 큼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch로 로지스틱 회귀 학습\n",
    "X_tr_t = torch.tensor(X_tr)\n",
    "y_tr_t = torch.tensor(y_tr)\n",
    "X_te_t = torch.tensor(X_te)\n",
    "y_te_t = torch.tensor(y_te)\n",
    "\n",
    "log_reg = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "optimizer = torch.optim.Adam(log_reg.parameters(), lr=0.05)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    log_reg.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred  = log_reg(X_tr_t).squeeze()\n",
    "    loss  = criterion(pred, y_tr_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# 결정 경계 시각화\n",
    "log_reg.eval()\n",
    "with torch.no_grad():\n",
    "    # 그리드 생성\n",
    "    x0_min, x0_max = X_s[:, 0].min()-0.5, X_s[:, 0].max()+0.5\n",
    "    x1_min, x1_max = X_s[:, 1].min()-0.5, X_s[:, 1].max()+0.5\n",
    "    xx0, xx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200),\n",
    "                           np.linspace(x1_min, x1_max, 200))\n",
    "    grid = torch.tensor(np.c_[xx0.ravel(), xx1.ravel()], dtype=torch.float32)\n",
    "    zz   = log_reg(grid).squeeze().numpy().reshape(xx0.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# 손실 곡선\n",
    "axes[0].plot(losses, color='steelblue', lw=2)\n",
    "axes[0].set_title('BCE 손실 수렴')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "# 결정 경계\n",
    "axes[1].contourf(xx0, xx1, zz, levels=50, cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[1].contour(xx0, xx1, zz, levels=[0.5], colors='black', linewidths=2)\n",
    "for c, color, label in zip([0, 1], colors_cls, labels_cls):\n",
    "    mask = y_bin == c\n",
    "    axes[1].scatter(X_s[mask, 0], X_s[mask, 1],\n",
    "                    color=color, label=label, s=50, edgecolors='k', alpha=0.9, zorder=3)\n",
    "axes[1].set_title('로지스틱 회귀 결정 경계\\n(검은 선: $\\\\hat{p}=0.5$)')\n",
    "axes[1].set_xlabel('꽃잎 길이 (표준화)')\n",
    "axes[1].set_ylabel('꽃잎 너비 (표준화)')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 정확도\n",
    "with torch.no_grad():\n",
    "    pred_te = (log_reg(X_te_t).squeeze() >= 0.5).float()\n",
    "acc = (pred_te == y_te_t).float().mean().item()\n",
    "print(f'Test Accuracy: {acc:.4f} ({acc*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. 다중 클래스 분류 — 소프트맥스와 크로스 엔트로피\n",
    "\n",
    "### 2-1. 소프트맥스 (Softmax)\n",
    "\n",
    "클래스가 3개 이상이면 **소프트맥스**로 확률 분포를 만듭니다:\n",
    "\n",
    "$$\\text{softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}, \\quad k=1,\\ldots,K$$\n",
    "\n",
    "특성:\n",
    "- 모든 출력 합 = 1 (확률 분포)\n",
    "- 가장 큰 로짓이 가장 높은 확률\n",
    "- $K=2$이면 시그모이드와 동일\n",
    "\n",
    "### 2-2. 크로스 엔트로피 손실 (Cross-Entropy Loss)\n",
    "\n",
    "$$\\mathcal{L}_{\\text{CE}} = -\\frac{1}{n}\\sum_{i=1}^{n} \\log \\hat{p}_{i, y_i}$$\n",
    "\n",
    "정답 클래스의 예측 확률이 높을수록 손실이 작아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 동작 확인\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1],    # 클래스 0이 높음\n",
    "                        [0.5, 2.5, 0.3],    # 클래스 1이 높음\n",
    "                        [0.1, 0.2, 3.0]])   # 클래스 2가 높음\n",
    "\n",
    "probs = F.softmax(logits, dim=1)\n",
    "print('로짓 → 소프트맥스 확률:')\n",
    "for i, (l, p) in enumerate(zip(logits, probs)):\n",
    "    print(f'  샘플{i}: logits={l.tolist()}  →  probs={p.detach().numpy().round(3).tolist()}')\n",
    "    print(f'         예측 클래스={p.argmax().item()},  합={p.sum().item():.4f}')\n",
    "\n",
    "# 크로스 엔트로피 손실\n",
    "targets = torch.tensor([0, 1, 2])   # 각 샘플의 정답\n",
    "ce_loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "print(f'\\nCross-Entropy Loss: {ce_loss.item():.4f}')\n",
    "\n",
    "# 시각화: 소프트맥스 확률 분포\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "class_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "for i, (l, p) in enumerate(zip(logits, probs)):\n",
    "    colors_bar = ['steelblue', 'tomato', 'seagreen']\n",
    "    bars = axes[i].bar(class_names, p.detach().numpy(),\n",
    "                       color=colors_bar, edgecolor='k', alpha=0.8)\n",
    "    axes[i].set_ylim(0, 1.1)\n",
    "    axes[i].set_title(f'샘플 {i} (정답: {class_names[targets[i]]})')\n",
    "    axes[i].set_ylabel('확률')\n",
    "    # 정답 막대 강조\n",
    "    bars[targets[i]].set_edgecolor('gold')\n",
    "    bars[targets[i]].set_linewidth(3)\n",
    "    for bar, val in zip(bars, p.detach().numpy()):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, val + 0.02,\n",
    "                     f'{val:.2f}', ha='center', fontsize=9)\n",
    "plt.suptitle('소프트맥스 확률 분포 (금색 테두리 = 정답 클래스)', y=1.04)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. PyTorch로 Iris 3-클래스 분류기 구현\n",
    "\n",
    "### 3-1. 데이터 준비\n",
    "\n",
    "Iris 전체 특성(4개)으로 3종류 붓꽃을 분류합니다:\n",
    "- **Setosa (0)**: 꽃잎 작고 선명히 구분\n",
    "- **Versicolor (1)**: 중간\n",
    "- **Virginica (2)**: 꽃잎 크고 Versicolor와 겹침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Iris 데이터 (4특성, 3클래스)\n",
    "X_full = iris.data.astype(np.float32)\n",
    "y_full = iris.target.astype(np.int64)\n",
    "\n",
    "X_tr3, X_te3, y_tr3, y_te3 = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "sc3 = StandardScaler()\n",
    "X_tr3_s = sc3.fit_transform(X_tr3).astype(np.float32)\n",
    "X_te3_s = sc3.transform(X_te3).astype(np.float32)\n",
    "\n",
    "# 텐서 변환\n",
    "X_tr3_t = torch.tensor(X_tr3_s)\n",
    "y_tr3_t = torch.tensor(y_tr3)\n",
    "X_te3_t = torch.tensor(X_te3_s)\n",
    "y_te3_t = torch.tensor(y_te3)\n",
    "\n",
    "print(f'Train: {len(X_tr3_s)}개,  Test: {len(X_te3_s)}개')\n",
    "print(f'클래스: {iris.target_names.tolist()}')\n",
    "\n",
    "# 특성 쌍 산점도\n",
    "df_iris = pd.DataFrame(X_full, columns=iris.feature_names)\n",
    "df_iris['species'] = [iris.target_names[t] for t in y_full]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "palette = {'setosa':'steelblue', 'versicolor':'tomato', 'virginica':'seagreen'}\n",
    "for ax, (fx, fy) in zip(axes, [('petal length (cm)', 'petal width (cm)'),\n",
    "                                 ('sepal length (cm)', 'sepal width (cm)')]):\n",
    "    for sp, color in palette.items():\n",
    "        m = df_iris['species'] == sp\n",
    "        ax.scatter(df_iris.loc[m, fx], df_iris.loc[m, fy],\n",
    "                   color=color, label=sp, s=50, edgecolors='k', alpha=0.8)\n",
    "    ax.set_xlabel(fx)\n",
    "    ax.set_ylabel(fy)\n",
    "    ax.legend(fontsize=8)\n",
    "plt.suptitle('Iris 3-클래스 분포', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md2",
   "metadata": {},
   "source": [
    "### 3-2. 소프트맥스 분류기 모델\n",
    "\n",
    "```\n",
    "입력 (4) → Linear(4→16) → ReLU → Linear(16→3)\n",
    "                                    ↓\n",
    "                              CrossEntropyLoss (내부에 Softmax 포함)\n",
    "```\n",
    "\n",
    "> **주의:** `nn.CrossEntropyLoss`는 내부적으로 소프트맥스를 포함하므로  \n",
    "> 모델 마지막 레이어에 `Softmax`를 **붙이지 않습니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(nn.Module):\n",
    "    \"\"\"4 → 16 → 3 분류기\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3),    # 소프트맥스 없음 (CrossEntropyLoss에 포함)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)       # 로짓(logits) 반환\n",
    "\n",
    "\n",
    "model3   = SoftmaxClassifier()\n",
    "crit3    = nn.CrossEntropyLoss()\n",
    "opt3     = torch.optim.Adam(model3.parameters(), lr=0.02)\n",
    "\n",
    "train_losses3, train_accs3 = [], []\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model3.train()\n",
    "    opt3.zero_grad()\n",
    "    logits3 = model3(X_tr3_t)\n",
    "    loss3   = crit3(logits3, y_tr3_t)\n",
    "    loss3.backward()\n",
    "    opt3.step()\n",
    "\n",
    "    # 정확도 계산\n",
    "    with torch.no_grad():\n",
    "        preds_tr = logits3.argmax(dim=1)\n",
    "        acc_tr   = (preds_tr == y_tr3_t).float().mean().item()\n",
    "    train_losses3.append(loss3.item())\n",
    "    train_accs3.append(acc_tr)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch:3d} | Loss: {loss3.item():.4f} | Train Acc: {acc_tr:.4f}')\n",
    "\n",
    "# Test 정확도\n",
    "model3.eval()\n",
    "with torch.no_grad():\n",
    "    preds_te3 = model3(X_te3_t).argmax(dim=1)\n",
    "test_acc3 = (preds_te3 == y_te3_t).float().mean().item()\n",
    "print(f'\\nTest Accuracy: {test_acc3:.4f} ({test_acc3*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses3, color='steelblue', lw=2)\n",
    "axes[0].set_title('학습 손실 (Cross-Entropy)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(train_accs3, color='seagreen', lw=2)\n",
    "axes[1].axhline(test_acc3, color='tomato', linestyle='--', lw=2,\n",
    "                label=f'Test Acc = {test_acc3:.2f}')\n",
    "axes[1].set_title('학습 정확도')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. 평가 지표 (Evaluation Metrics)\n",
    "\n",
    "### 4-1. 혼동 행렬 (Confusion Matrix)\n",
    "\n",
    "분류 결과를 정답 vs 예측 행렬로 표현합니다.\n",
    "\n",
    "| | 예측 양성(1) | 예측 음성(0) |\n",
    "|---|---|---|\n",
    "| **실제 양성(1)** | TP (진양성) | FN (위음성) |\n",
    "| **실제 음성(0)** | FP (위양성) | TN (진음성) |\n",
    "\n",
    "### 4-2. 주요 지표\n",
    "\n",
    "| 지표 | 수식 | 의미 |\n",
    "|---|---|---|\n",
    "| **Accuracy** | $(TP+TN)/(P+N)$ | 전체 중 올바른 예측 비율 |\n",
    "| **Precision** | $TP/(TP+FP)$ | 양성 예측 중 진짜 양성 비율 |\n",
    "| **Recall** | $TP/(TP+FN)$ | 실제 양성 중 올바르게 예측한 비율 |\n",
    "| **F1** | $2 \\cdot P \\cdot R / (P+R)$ | Precision과 Recall의 조화 평균 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 3-클래스 평가 지표\n",
    "y_pred_np = preds_te3.numpy()\n",
    "y_true_np = y_te3_t.numpy()\n",
    "\n",
    "# 혼동 행렬\n",
    "cm = confusion_matrix(y_true_np, y_pred_np)\n",
    "\n",
    "# 지표 출력\n",
    "print('[ 분류 평가 지표 ]')\n",
    "print(f'  Accuracy : {accuracy_score(y_true_np, y_pred_np):.4f}')\n",
    "print(f'  Precision (macro): {precision_score(y_true_np, y_pred_np, average=\"macro\"):.4f}')\n",
    "print(f'  Recall    (macro): {recall_score(y_true_np, y_pred_np, average=\"macro\"):.4f}')\n",
    "print(f'  F1        (macro): {f1_score(y_true_np, y_pred_np, average=\"macro\"):.4f}')\n",
    "\n",
    "print('\\n[ 클래스별 지표 ]')\n",
    "print(f'{\"\":15} Precision  Recall    F1')\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    p = precision_score(y_true_np, y_pred_np, average=None)[i]\n",
    "    r = recall_score(y_true_np, y_pred_np, average=None)[i]\n",
    "    f = f1_score(y_true_np, y_pred_np, average=None)[i]\n",
    "    print(f'  {name:<13} {p:.4f}     {r:.4f}    {f:.4f}')\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names,\n",
    "            ax=axes[0], linewidths=0.5, linecolor='gray')\n",
    "axes[0].set_title('혼동 행렬 (Confusion Matrix)')\n",
    "axes[0].set_xlabel('예측 클래스')\n",
    "axes[0].set_ylabel('실제 클래스')\n",
    "\n",
    "# 정규화 혼동 행렬 (비율)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names,\n",
    "            ax=axes[1], linewidths=0.5, linecolor='gray',\n",
    "            vmin=0, vmax=1)\n",
    "axes[1].set_title('정규화 혼동 행렬 (비율)')\n",
    "axes[1].set_xlabel('예측 클래스')\n",
    "axes[1].set_ylabel('실제 클래스')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p4-md2",
   "metadata": {},
   "source": [
    "### 4-3. Precision vs Recall 트레이드오프와 ROC 곡선\n",
    "\n",
    "임계값(threshold)을 바꾸면 Precision과 Recall이 반대 방향으로 움직입니다.\n",
    "\n",
    "- **임계값 ↑** → Precision↑, Recall↓ (확신하는 것만 양성 예측)\n",
    "- **임계값 ↓** → Precision↓, Recall↑ (더 많이 양성 예측)\n",
    "\n",
    "**ROC-AUC**: 모든 임계값에서의 성능을 요약\n",
    "- **AUC = 1.0**: 완벽한 분류기\n",
    "- **AUC = 0.5**: 랜덤 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p4-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 (Setosa vs Non-Setosa)로 ROC 곡선 분석\n",
    "log_reg.eval()\n",
    "with torch.no_grad():\n",
    "    probs_te = log_reg(X_te_t).squeeze().numpy()   # 클래스 1 확률\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_te, probs_te)\n",
    "auc = roc_auc_score(y_te, probs_te)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ROC 곡선\n",
    "axes[0].plot(fpr, tpr, color='steelblue', lw=2.5, label=f'ROC (AUC={auc:.4f})')\n",
    "axes[0].plot([0,1],[0,1], 'k--', lw=1.5, label='랜덤 분류기 (AUC=0.5)')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.15, color='steelblue')\n",
    "axes[0].set_title('ROC 곡선 (Setosa vs Non-Setosa)')\n",
    "axes[0].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[0].set_ylabel('True Positive Rate (Recall)')\n",
    "axes[0].legend()\n",
    "\n",
    "# 임계값별 Precision-Recall 트레이드오프\n",
    "thrs = np.linspace(0.05, 0.95, 50)\n",
    "precs, recs, f1s = [], [], []\n",
    "for thr in thrs:\n",
    "    pred_thr = (probs_te >= thr).astype(int)\n",
    "    if pred_thr.sum() == 0:\n",
    "        precs.append(0.0)\n",
    "    else:\n",
    "        precs.append(precision_score(y_te, pred_thr, zero_division=0))\n",
    "    recs.append(recall_score(y_te, pred_thr, zero_division=0))\n",
    "    p, r = precs[-1], recs[-1]\n",
    "    f1s.append(2*p*r/(p+r) if (p+r) > 0 else 0)\n",
    "\n",
    "axes[1].plot(thrs, precs, color='steelblue', lw=2, label='Precision')\n",
    "axes[1].plot(thrs, recs,  color='tomato',    lw=2, label='Recall')\n",
    "axes[1].plot(thrs, f1s,   color='seagreen',  lw=2, label='F1')\n",
    "axes[1].axvline(0.5, color='gray', linestyle='--', lw=1.5, label='임계값=0.5')\n",
    "axes[1].set_title('임계값에 따른 Precision / Recall / F1')\n",
    "axes[1].set_xlabel('임계값 (Threshold)')\n",
    "axes[1].set_ylabel('점수')\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'AUC = {auc:.4f}')\n",
    "best_thr_idx = np.argmax(f1s)\n",
    "print(f'최적 임계값(F1 기준): {thrs[best_thr_idx]:.2f}  F1={f1s[best_thr_idx]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise — MNIST 소프트맥스 분류기\n",
    "\n",
    "> **MNIST**: 0~9 손글씨 숫자 이미지 (28×28 픽셀, 10 클래스)\n",
    "\n",
    "아래 셀들을 순서대로 완성하여 MNIST 분류기를 구축하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 로드 (처음 실행 시 다운로드, 약 10~30초)\n",
    "try:\n",
    "    from torchvision import datasets, transforms\n",
    "    transform = transforms.ToTensor()\n",
    "    mnist_tr = datasets.MNIST(root='/tmp/mnist', train=True,  download=True, transform=transform)\n",
    "    mnist_te = datasets.MNIST(root='/tmp/mnist', train=False, download=True, transform=transform)\n",
    "\n",
    "    # 실습을 위해 일부만 사용 (각 클래스 당 600/100 개)\n",
    "    # 전체를 사용하고 싶으면 아래 슬라이싱을 제거하세요\n",
    "    X_mn_tr = mnist_tr.data[:6000].float().view(-1, 784) / 255.0\n",
    "    y_mn_tr = mnist_tr.targets[:6000]\n",
    "    X_mn_te = mnist_te.data[:1000].float().view(-1, 784) / 255.0\n",
    "    y_mn_te = mnist_te.targets[:1000]\n",
    "    print(f'Train: {X_mn_tr.shape},  Test: {X_mn_te.shape}')\n",
    "    MNIST_LOADED = True\n",
    "except ImportError:\n",
    "    # torchvision 없을 경우 sklearn으로 fallback\n",
    "    print('torchvision 없음 → sklearn MNIST 사용')\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist_sk = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "    Xm = mnist_sk.data[:7000].astype(np.float32) / 255.0\n",
    "    ym = mnist_sk.target[:7000].astype(np.int64)\n",
    "    X_mn_tr = torch.tensor(Xm[:6000])\n",
    "    y_mn_tr = torch.tensor(ym[:6000])\n",
    "    X_mn_te = torch.tensor(Xm[6000:])\n",
    "    y_mn_te = torch.tensor(ym[6000:])\n",
    "    MNIST_LOADED = True\n",
    "\n",
    "# 샘플 이미지 시각화\n",
    "fig, axes = plt.subplots(2, 10, figsize=(14, 3))\n",
    "for digit in range(10):\n",
    "    idx = (y_mn_tr == digit).nonzero(as_tuple=True)[0][0]\n",
    "    img = X_mn_tr[idx].view(28, 28).numpy()\n",
    "    axes[0, digit].imshow(img, cmap='gray')\n",
    "    axes[0, digit].set_title(str(digit))\n",
    "    axes[0, digit].axis('off')\n",
    "    axes[1, digit].hist(img.ravel(), bins=20, color='steelblue')\n",
    "    axes[1, digit].axis('off')\n",
    "plt.suptitle('MNIST 각 클래스 샘플 이미지 (위: 이미지, 아래: 픽셀 분포)', y=1.04)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 1. MNIST 소프트맥스 분류기 완성\n",
    "\n",
    "아래 모델과 학습 루프를 완성하세요.\n",
    "\n",
    "**요구 사항:**\n",
    "- 모델 구조: `784 → 128 → ReLU → 64 → ReLU → 10`\n",
    "- 손실 함수: `CrossEntropyLoss`\n",
    "- 옵티마이저: `Adam`, `lr=0.001`\n",
    "- DataLoader 배치 크기: `256`\n",
    "- 학습: `20 에폭`\n",
    "- 매 5 에폭마다 Train Loss와 Test Accuracy 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: MNIST 분류기 완성\n",
    "# Your code here\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Your code here: 784 → 128 → ReLU → 64 → ReLU → 10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "# 데이터로더, 손실, 옵티마이저, 학습 루프\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 2. 혼동 행렬과 오분류 샘플 분석\n",
    "\n",
    "학습한 MNIST 모델로:\n",
    "1. Test 셋 혼동 행렬을 시각화하세요.\n",
    "2. **가장 많이 혼동된 두 클래스** (예: 4↔9, 3↔8)를 찾아 오분류된 이미지 5장을 보여주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: 혼동 행렬 + 오분류 샘플\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md4",
   "metadata": {},
   "source": [
    "### Exercise 3. (도전) 결정 경계 시각화\n",
    "\n",
    "Iris 데이터에서 **꽃잎 길이, 꽃잎 너비** 2개 특성만 사용해  \n",
    "3-클래스 소프트맥스 분류기를 학습하고 결정 경계를 시각화하세요.\n",
    "\n",
    "힌트: `plt.contourf`로 그리드 전체에 대한 예측 클래스를 색으로 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: 3-클래스 결정 경계 시각화\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| 개념 | 핵심 내용 |\n",
    "|---|---|\n",
    "| **시그모이드** | $\\sigma(z) = 1/(1+e^{-z})$, 이진 분류 확률 출력 |\n",
    "| **결정 경계** | $\\mathbf{w}^\\top\\mathbf{x} + b = 0$ — 클래스 구분선 |\n",
    "| **BCE Loss** | $-[y\\log\\hat{p} + (1-y)\\log(1-\\hat{p})]$ |\n",
    "| **소프트맥스** | $e^{z_k} / \\sum e^{z_j}$ — 다중 클래스 확률 |\n",
    "| **CrossEntropyLoss** | `nn.CrossEntropyLoss` = Softmax + NLLLoss (내부 포함) |\n",
    "| **Accuracy** | 전체 예측 중 올바른 비율 |\n",
    "| **Precision** | 양성 예측 중 진짜 양성 비율 (FP 최소화) |\n",
    "| **Recall** | 실제 양성 중 올바르게 찾은 비율 (FN 최소화) |\n",
    "| **F1** | Precision·Recall 조화 평균 |\n",
    "| **ROC-AUC** | 임계값 무관 종합 성능 지표 (1.0이 최고) |\n",
    "| **혼동 행렬** | 클래스별 예측 오류 분포 파악 |\n",
    "\n",
    "---\n",
    "\n",
    "**다음 강의 (Week 7):** 신경망 — 다층 퍼셉트론, 활성화 함수, 역전파"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
