{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-title",
   "metadata": {},
   "source": [
    "# Lab 14 â€” Attention & Transformer\n",
    "\n",
    "> **ê°•ì˜ ì‹œê°„:** ì•½ 2ì‹œê°„  \n",
    "> **ì£¼ì œ:** Attention ë©”ì»¤ë‹ˆì¦˜, Positional Encoding, Transformer Encoder, ê°ì„± ë¶„ë¥˜\n",
    "\n",
    "---\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "| # | ëª©í‘œ | ì˜ˆìƒ ì‹œê°„ |\n",
    "|---|---|---|\n",
    "| 1 | Scaled Dot-Product Attention ìˆ˜ë™ êµ¬í˜„ ë° íˆíŠ¸ë§µ ì‹œê°í™” | 30ë¶„ |\n",
    "| 2 | Positional Encoding êµ¬í˜„ + Multi-Head Attention íŒ¨í„´ ë¹„êµ | 30ë¶„ |\n",
    "| 3 | Transformer Encoder ì¡°ë¦½ + í•©ì„± ê°ì„± ë°ì´í„° ë¶„ë¥˜ í•™ìŠµ | 30ë¶„ |\n",
    "| 4 | Exercise | 30ë¶„ |\n",
    "\n",
    "---\n",
    "\n",
    "**ë°ì´í„°ì…‹:** numpy í•©ì„± ê°ì„± ë°ì´í„° (ì¸í„°ë„· ë¶ˆí•„ìš”)  \n",
    "- `vocab_size=100` (0=PAD, 1=CLS, 2\\~49=ê¸ì • ë‹¨ì–´, 50\\~99=ë¶€ì • ë‹¨ì–´)  \n",
    "- `seq_len=20` (CLS í† í° + ë‚´ìš© í† í° 19ê°œ), `n_samples=2000` (train 1600 / test 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "_fp = '/System/Library/Fonts/AppleGothic.ttf'\n",
    "fm.fontManager.addfont(_fp)\n",
    "plt.rcParams['font.family'] = fm.FontProperties(fname=_fp).get_name()\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Using device   : {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ í•©ì„± ê°ì„± ë°ì´í„° ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# vocab: 0=PAD, 1=CLS, 2~49=ê¸ì • ë‹¨ì–´(48ê°œ), 50~99=ë¶€ì • ë‹¨ì–´(50ê°œ)\n",
    "\n",
    "VOCAB_SIZE = 100\n",
    "SEQ_LEN    = 20    # CLS(1) + ë‚´ìš© í† í° 19ê°œ\n",
    "N_SAMPLES  = 2000  # train 1600 / test 400\n",
    "POS_RANGE  = (2,  50)   # ê¸ì • í† í° ë²”ìœ„ [2, 49]\n",
    "NEG_RANGE  = (50, 100)  # ë¶€ì • í† í° ë²”ìœ„ [50, 99]\n",
    "\n",
    "\n",
    "def generate_sentiment_data(n_samples=N_SAMPLES, seq_len=SEQ_LEN, seed=42):\n",
    "    \"\"\"\n",
    "    ê¸ì •(label=1) / ë¶€ì •(label=0) í•©ì„± ê°ì„± ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    - ê° ì‹œí€€ìŠ¤ëŠ” CLS(=1) í† í°ìœ¼ë¡œ ì‹œì‘\n",
    "    - ê¸ì •: ë‚´ìš© í† í°ì˜ ì•½ 60%ê°€ ê¸ì • ì˜ì—­(2~49)\n",
    "    - ë¶€ì •: ë‚´ìš© í† í°ì˜ ì•½ 60%ê°€ ë¶€ì • ì˜ì—­(50~99)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    tokens_list, labels = [], []\n",
    "    content_len = seq_len - 1            # CLS ì œì™¸ ë‚´ìš© ê¸¸ì´\n",
    "    n_signal    = int(content_len * 0.6) # 60% ì‹ í˜¸ í† í°\n",
    "    n_noise     = content_len - n_signal # 40% ë…¸ì´ì¦ˆ í† í°\n",
    "    for _ in range(n_samples):\n",
    "        label = rng.randint(0, 2)\n",
    "        if label == 1:   # ê¸ì •\n",
    "            signal = rng.randint(*POS_RANGE, size=n_signal)\n",
    "            noise  = rng.randint(*NEG_RANGE, size=n_noise)\n",
    "        else:            # ë¶€ì •\n",
    "            signal = rng.randint(*NEG_RANGE, size=n_signal)\n",
    "            noise  = rng.randint(*POS_RANGE, size=n_noise)\n",
    "        content = np.concatenate([signal, noise])\n",
    "        rng.shuffle(content)\n",
    "        seq = np.concatenate([[1], content])  # CLS í† í° ë§¨ ì•\n",
    "        tokens_list.append(seq)\n",
    "        labels.append(label)\n",
    "    X = torch.LongTensor(np.array(tokens_list))  # (N, seq_len)\n",
    "    y = torch.LongTensor(np.array(labels))        # (N,)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, train_ratio=0.8):\n",
    "    n = int(len(X) * train_ratio)\n",
    "    return X[:n], y[:n], X[n:], y[n:]\n",
    "\n",
    "\n",
    "X_all, y_all   = generate_sentiment_data()\n",
    "X_tr, y_tr, X_te, y_te = split_data(X_all, y_all)\n",
    "train_ds  = TensorDataset(X_tr, y_tr)\n",
    "train_ldr = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "print('=== í•©ì„± ê°ì„± ë°ì´í„°ì…‹ ===')\n",
    "print(f'  vocab_size : {VOCAB_SIZE}  (0=PAD, 1=CLS, 2~49=ê¸ì •, 50~99=ë¶€ì •)')\n",
    "print(f'  seq_len    : {SEQ_LEN}   (CLS + ë‚´ìš© {SEQ_LEN-1}í† í°)')\n",
    "print(f'  ì´ ìƒ˜í”Œ    : {len(X_all)}  (Train {len(X_tr)} / Test {len(X_te)})')\n",
    "print(f'  ê¸ì • ë¹„ìœ¨  : {y_all.float().mean():.2f}')\n",
    "print('\\nìƒ˜í”Œ ì‹œí€€ìŠ¤ (ì²˜ìŒ 3ê°œ):')\n",
    "for i in range(3):\n",
    "    lbl = 'ê¸ì •' if y_all[i].item() == 1 else 'ë¶€ì •'\n",
    "    print(f'  [{lbl}] {X_all[i].tolist()}')\n",
    "\n",
    "# â”€â”€ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "for ax_idx, label_val in enumerate([1, 0]):\n",
    "    idxs = (y_all == label_val).nonzero(as_tuple=True)[0][:5]\n",
    "    lbl_str = 'ê¸ì •(label=1)' if label_val == 1 else 'ë¶€ì •(label=0)'\n",
    "    pos_c, neg_c = [], []\n",
    "    for idx in idxs:\n",
    "        seq = X_all[idx, 1:].numpy()  # CLS ì œì™¸\n",
    "        pos_c.append(int((seq < 50).sum()))\n",
    "        neg_c.append(int((seq >= 50).sum()))\n",
    "    x = np.arange(5)\n",
    "    axes[ax_idx].bar(x - 0.2, pos_c, width=0.4, label='ê¸ì • í† í°(2~49)', color='seagreen', alpha=0.8)\n",
    "    axes[ax_idx].bar(x + 0.2, neg_c, width=0.4, label='ë¶€ì • í† í°(50~99)', color='tomato',  alpha=0.8)\n",
    "    axes[ax_idx].set_title(f'{lbl_str} ìƒ˜í”Œ 5ê°œì˜ í† í° ë¶„í¬')\n",
    "    axes[ax_idx].set_xlabel('ìƒ˜í”Œ ì¸ë±ìŠ¤'); axes[ax_idx].set_ylabel('í† í° ìˆ˜')\n",
    "    axes[ax_idx].set_xticks(x); axes[ax_idx].legend(fontsize=9)\n",
    "plt.suptitle('í•©ì„± ê°ì„± ë°ì´í„° â€” í† í° ë¶„í¬', fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. Scaled Dot-Product Attention\n",
    "\n",
    "### 1-1. í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\!\\\\left(\\\\frac{QK^\\\\top}{\\\\sqrt{d_k}}\\\\right)V$$\n",
    "\n",
    "| ê¸°í˜¸ | í¬ê¸° | ì„¤ëª… |\n",
    "|---|---|---|\n",
    "| $Q$ (Query) | $(\\\\text{seq\\\\_len},\\\\, d_k)$ | \"ë¬´ì—‡ì„ ì°¾ëŠ”ê°€\" |\n",
    "| $K$ (Key)   | $(\\\\text{seq\\\\_len},\\\\, d_k)$ | \"ì–´ë””ì— ì •ë³´ê°€ ìˆëŠ”ê°€\" |\n",
    "| $V$ (Value) | $(\\\\text{seq\\\\_len},\\\\, d_v)$ | \"ì‹¤ì œ ì •ë³´\" |\n",
    "| $\\\\sqrt{d_k}$ ë‚˜ëˆ—ì…ˆ | ìŠ¤ì¹¼ë¼ | ë‚´ì ê°’ì´ ì»¤ì§€ëŠ” ë¬¸ì œ ë°©ì§€ (softmax í¬í™” ë°©ì§€) |\n",
    "\n",
    "### 1-2. ì™œ âˆšd_kë¡œ ë‚˜ëˆ„ëŠ”ê°€?\n",
    "- Q, Kê°€ í‰ê·  0, ë¶„ì‚° 1ì´ë©´ $QK^\\\\top$ì˜ ë¶„ì‚° = $d_k$\n",
    "- âˆšd_kë¡œ ë‚˜ëˆ„ë©´ ë¶„ì‚° = 1 â†’ softmax ì…ë ¥ì´ ë„ˆë¬´ í¬ë©´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì‚¬ë¼ì§\n",
    "\n",
    "### 1-3. Self-Attention\n",
    "- Q = K = V = **ê°™ì€ ì‹œí€€ìŠ¤** â†’ ìê¸° ìì‹  ë‚´ë¶€ì˜ ëª¨ë“  ìœ„ì¹˜ ê´€ê³„ë¥¼ ë™ì‹œì— ê³„ì‚°\n",
    "- RNNê³¼ ë‹¬ë¦¬ ëª¨ë“  ìœ„ì¹˜ê°€ **ì§ì ‘** ì—°ê²°ë¨ â†’ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Scaled Dot-Product Attention ìˆ˜ë™ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def sdpa_numpy(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention (numpy)\n",
    "    Q, K: (seq_len, d_k)   V: (seq_len, d_v)\n",
    "    mask: (seq_len, seq_len) bool â€” Trueì¸ ìœ„ì¹˜ë¥¼ -infë¡œ ë§ˆìŠ¤í‚¹\n",
    "    \"\"\"\n",
    "    d_k    = Q.shape[-1]\n",
    "    scores = Q @ K.T / np.sqrt(d_k)              # (seq_len, seq_len)\n",
    "    if mask is not None:\n",
    "        scores[mask] = -np.inf\n",
    "    # ìˆ˜ì¹˜ ì•ˆì •í™” softmax\n",
    "    scores  = scores - scores.max(axis=-1, keepdims=True)\n",
    "    exp_s   = np.exp(scores)\n",
    "    weights = exp_s / exp_s.sum(axis=-1, keepdims=True)  # (seq_len, seq_len)\n",
    "    output  = weights @ V                                  # (seq_len, d_v)\n",
    "    return output, weights\n",
    "\n",
    "\n",
    "def sdpa_torch(Q, K, V, mask=None):\n",
    "    \"\"\"Scaled Dot-Product Attention (PyTorch ê²€ì¦ìš©)\"\"\"\n",
    "    d_k    = Q.shape[-1]\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, V), weights\n",
    "\n",
    "\n",
    "# â”€â”€ ë°ëª¨: ëœë¤ Q, K, V â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(0)\n",
    "seq_demo, d_k_demo, d_v_demo = 6, 8, 8\n",
    "\n",
    "Q_np = np.random.randn(seq_demo, d_k_demo).astype(np.float32)\n",
    "K_np = np.random.randn(seq_demo, d_k_demo).astype(np.float32)\n",
    "V_np = np.random.randn(seq_demo, d_v_demo).astype(np.float32)\n",
    "\n",
    "out_np, w_np = sdpa_numpy(Q_np, K_np, V_np)\n",
    "\n",
    "Q_pt = torch.FloatTensor(Q_np)\n",
    "K_pt = torch.FloatTensor(K_np)\n",
    "V_pt = torch.FloatTensor(V_np)\n",
    "out_pt, w_pt = sdpa_torch(Q_pt, K_pt, V_pt)\n",
    "\n",
    "max_out_diff = np.max(np.abs(out_np - out_pt.numpy()))\n",
    "max_w_diff   = np.max(np.abs(w_np  - w_pt.numpy()))\n",
    "print(f'numpy vs PyTorch ìµœëŒ€ ì˜¤ì°¨ â€” output: {max_out_diff:.2e},  weights: {max_w_diff:.2e}')\n",
    "print('(â‰ˆ0 ì´ë©´ ë‘ êµ¬í˜„ì´ ë™ì¼)\\n')\n",
    "\n",
    "print(f'ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í–‰ë³„ í•© (softmax â†’ í•­ìƒ 1.0):')\n",
    "print(f'  numpy  : {w_np.sum(axis=-1).round(4)}')\n",
    "print(f'  PyTorch: {w_pt.sum(dim=-1).numpy().round(4)}\\n')\n",
    "\n",
    "print(f'í…ì„œ í¬ê¸° ìš”ì•½:')\n",
    "print(f'  Q, K : {Q_np.shape}  â†’  scores : ({seq_demo}, {seq_demo})')\n",
    "print(f'  weights (softmax) : ({seq_demo}, {seq_demo})')\n",
    "print(f'  V : {V_np.shape}  â†’  output : ({seq_demo}, {d_v_demo})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p1-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Self-Attention íˆíŠ¸ë§µ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "np.random.seed(7)\n",
    "seq_vis = SEQ_LEN\n",
    "d_k_vis = 16\n",
    "\n",
    "Q_vis = np.random.randn(seq_vis, d_k_vis).astype(np.float32)\n",
    "K_vis = np.random.randn(seq_vis, d_k_vis).astype(np.float32)\n",
    "V_vis = np.random.randn(seq_vis, d_k_vis).astype(np.float32)\n",
    "\n",
    "# ë§ˆìŠ¤í‚¹ ì—†ìŒ\n",
    "_, w_no_mask = sdpa_numpy(Q_vis, K_vis, V_vis)\n",
    "\n",
    "# ë§ˆì§€ë§‰ 5í† í°ì„ íŒ¨ë”©ìœ¼ë¡œ ê°€ì • â†’ Key ì—´ ë§ˆìŠ¤í‚¹\n",
    "mask_vis = np.zeros((seq_vis, seq_vis), dtype=bool)\n",
    "mask_vis[:, -5:] = True\n",
    "_, w_masked = sdpa_numpy(Q_vis, K_vis, V_vis, mask=mask_vis)\n",
    "\n",
    "tick_lbl = ['CLS'] + [f't{i}' for i in range(1, seq_vis)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im0 = axes[0].imshow(w_no_mask, cmap='Blues', vmin=0, vmax=w_no_mask.max())\n",
    "axes[0].set_title('Self-Attention ê°€ì¤‘ì¹˜ (ë§ˆìŠ¤í‚¹ ì—†ìŒ)')\n",
    "axes[0].set_xlabel('Key ìœ„ì¹˜ (ì–´ë””ì— ì§‘ì¤‘?)')\n",
    "axes[0].set_ylabel('Query ìœ„ì¹˜ (ì–´ëŠ í† í°ì´?)')\n",
    "axes[0].set_xticks(range(seq_vis)); axes[0].set_xticklabels(tick_lbl, fontsize=7, rotation=45)\n",
    "axes[0].set_yticks(range(seq_vis)); axes[0].set_yticklabels(tick_lbl, fontsize=7)\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im1 = axes[1].imshow(w_masked, cmap='Blues', vmin=0, vmax=w_masked.max())\n",
    "axes[1].set_title('Self-Attention ê°€ì¤‘ì¹˜ (ë§ˆì§€ë§‰ 5í† í° ë§ˆìŠ¤í‚¹)')\n",
    "axes[1].set_xlabel('Key ìœ„ì¹˜'); axes[1].set_ylabel('Query ìœ„ì¹˜')\n",
    "axes[1].set_xticks(range(seq_vis)); axes[1].set_xticklabels(tick_lbl, fontsize=7, rotation=45)\n",
    "axes[1].set_yticks(range(seq_vis)); axes[1].set_yticklabels(tick_lbl, fontsize=7)\n",
    "# ë§ˆìŠ¤í‚¹ ì˜ì—­ í‘œì‹œ (ë¹¨ê°„ ì„¸ë¡œì„ )\n",
    "for col in range(seq_vis - 5, seq_vis):\n",
    "    axes[1].axvline(x=col - 0.5, color='red', lw=0.7, alpha=0.7)\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Scaled Dot-Product Self-Attention ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ\\n'\n",
    "             'í–‰ = Query ìœ„ì¹˜, ì—´ = Key ìœ„ì¹˜, ë°ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜', fontsize=11)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('ğŸ“Œ í•µì‹¬ ê´€ì°°:')\n",
    "print('  - ê° QueryëŠ” ëª¨ë“  Key ìœ„ì¹˜ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.')\n",
    "print('  - ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜(ë¹¨ê°„ ì„  ì˜¤ë¥¸ìª½)ëŠ” ì–´í…ì…˜ì—ì„œ ì™„ì „íˆ ì œì™¸ë©ë‹ˆë‹¤.')\n",
    "print(f'  - ê°€ì¤‘ì¹˜ í–‰ë³„ í•© = 1: {w_masked.sum(axis=-1).round(3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. Positional Encoding + Multi-Head Attention\n",
    "\n",
    "### 2-1. Positional Encoding\n",
    "\n",
    "TransformerëŠ” **ìˆœì„œ ì •ë³´ê°€ ì—†ë‹¤** â†’ sin/cos í•¨ìˆ˜ë¡œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”©ì— ì£¼ì…:\n",
    "\n",
    "$$PE_{(pos,\\, 2i)}   = \\\\sin\\\\!\\\\left(\\\\frac{pos}{10000^{2i/d_{\\\\text{model}}}}\\\\right)$$\n",
    "$$PE_{(pos,\\, 2i+1)} = \\\\cos\\\\!\\\\left(\\\\frac{pos}{10000^{2i/d_{\\\\text{model}}}}\\\\right)$$\n",
    "\n",
    "- **ë‚®ì€ ì°¨ì›** (i ì‘ìŒ): ë¹ ë¥´ê²Œ ë³€í•˜ëŠ” íŒ¨í„´ â†’ ê°€ê¹Œìš´ ìœ„ì¹˜ êµ¬ë¶„  \n",
    "- **ë†’ì€ ì°¨ì›** (i í¼): ëŠë¦¬ê²Œ ë³€í•˜ëŠ” íŒ¨í„´ â†’ ë¨¼ ìœ„ì¹˜ êµ¬ë¶„  \n",
    "- `register_buffer` â†’ í•™ìŠµ íŒŒë¼ë¯¸í„° ì—†ìŒ, ê³ ì •ê°’\n",
    "\n",
    "### 2-2. Multi-Head Attention\n",
    "\n",
    "ë‹¨ì¼ ì–´í…ì…˜ì€ 1ê°€ì§€ ê´€ê³„ë§Œ í¬ì°© â†’ **hê°œ í—¤ë“œë¥¼ ë³‘ë ¬ë¡œ** ì‹¤í–‰í•´ ë‹¤ì–‘í•œ ê´€ê³„ ë™ì‹œ í¬ì°©:\n",
    "\n",
    "$$\\\\text{MultiHead}(Q,K,V) = \\\\text{Concat}(\\\\text{head}_1,\\\\ldots,\\\\text{head}_h)W^O$$\n",
    "$$\\\\text{head}_i = \\\\text{Attention}(QW_i^Q,\\\\, KW_i^K,\\\\, VW_i^V)$$\n",
    "\n",
    "| ë¹„êµ | Single-Head | Multi-Head (h=4) |\n",
    "|---|---|---|\n",
    "| Q/K/V ì°¨ì› | $d_k = d_{\\\\text{model}}$ | $d_k = d_{\\\\text{model}} / h$ |\n",
    "| ì–´í…ì…˜ ê³„ì‚° | 1íšŒ | híšŒ ë³‘ë ¬ |\n",
    "| ì´ íŒŒë¼ë¯¸í„° | $4d^2$ | $4d^2$ (ë™ì¼!) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Positional Encoding êµ¬í˜„ ë° ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding (Vaswani et al. 2017)\n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe       = torch.zeros(max_len, d_model)                     # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()     # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )                                                             # (d_model/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                 # ì§ìˆ˜ ì°¨ì›\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                 # í™€ìˆ˜ ì°¨ì›\n",
    "        pe = pe.unsqueeze(0)                                         # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)                               # í•™ìŠµ íŒŒë¼ë¯¸í„° ì•„ë‹˜\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, seq_len, d_model)\"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# â”€â”€ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "d_model_vis = 64\n",
    "max_len_vis = 50\n",
    "pe_module   = PositionalEncoding(d_model_vis, max_len=max_len_vis)\n",
    "pe_matrix   = pe_module.pe[0].numpy()    # (max_len, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4.5))\n",
    "\n",
    "im = axes[0].imshow(pe_matrix, cmap='RdBu', vmin=-1, vmax=1, aspect='auto')\n",
    "axes[0].set_title(f'PE í–‰ë ¬ ({max_len_vis} ìœ„ì¹˜ Ã— {d_model_vis} ì°¨ì›)')\n",
    "axes[0].set_xlabel('ì°¨ì› (d_model)'); axes[0].set_ylabel('ìœ„ì¹˜ (pos)')\n",
    "plt.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "pos_range = np.arange(max_len_vis)\n",
    "for dim_idx, color in zip([0, 2, 4, 10, 20],\n",
    "                          ['steelblue','tomato','seagreen','purple','orange']):\n",
    "    axes[1].plot(pos_range, pe_matrix[:, dim_idx], color=color, lw=2, label=f'dim={dim_idx}')\n",
    "axes[1].set_title('ì°¨ì›ë³„ PE ê°’ (ìœ„ì¹˜ì— ë”°ë¥¸ sin/cos ì£¼ê¸°)')\n",
    "axes[1].set_xlabel('ìœ„ì¹˜ (pos)'); axes[1].set_ylabel('PE ê°’')\n",
    "axes[1].set_ylim(-1.1, 1.1); axes[1].axhline(y=0, color='gray', ls='--', lw=0.8)\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Sinusoidal Positional Encoding ì‹œê°í™”\\n'\n",
    "             'ì°¨ì› ë²ˆí˜¸ê°€ í´ìˆ˜ë¡ ë” ë‚®ì€ ì£¼íŒŒìˆ˜ (ì²œì²œíˆ ë³€í•˜ëŠ” íŒ¨í„´)', fontsize=11)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('ğŸ“Œ PositionalEncoding íŠ¹ì„±:')\n",
    "print('  - ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ d_modelì°¨ì› ë²¡í„° ë¶€ì—¬')\n",
    "print('  - ë‚®ì€ ì°¨ì›(0, 2): ë¹ ë¥´ê²Œ ë³€í•˜ëŠ” íŒ¨í„´ â†’ ê°€ê¹Œìš´ ìœ„ì¹˜ êµ¬ë¶„')\n",
    "print('  - ë†’ì€ ì°¨ì›(20):   ëŠë¦¬ê²Œ ë³€í•˜ëŠ” íŒ¨í„´ â†’ ë¨¼ ìœ„ì¹˜ êµ¬ë¶„')\n",
    "print('  - register_buffer: í•™ìŠµí•˜ì§€ ì•ŠëŠ” ê³ ì • íŒŒë¼ë¯¸í„°')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p2-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Multi-Head Attention í—¤ë“œë³„ ì–´í…ì…˜ íŒ¨í„´ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "nhead_demo  = 4\n",
    "d_model_mha = 64\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mha_layer = nn.MultiheadAttention(\n",
    "    embed_dim=d_model_mha, num_heads=nhead_demo, dropout=0.0, batch_first=True\n",
    ")\n",
    "emb_demo = nn.Embedding(VOCAB_SIZE, d_model_mha)\n",
    "\n",
    "# ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ 1ê°œ ì¶”ì¶œ â†’ ëœë¤ ì„ë² ë”©ìœ¼ë¡œ MHA ì‹œì—°\n",
    "sample_tok = X_tr[0:1]   # (1, seq_len)\n",
    "with torch.no_grad():\n",
    "    x_mha = emb_demo(sample_tok)    # (1, seq_len, d_model)\n",
    "    # average_attn_weights=False â†’ ê° í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ë°˜í™˜ (PyTorch >= 1.13)\n",
    "    _, attn_w = mha_layer(\n",
    "        x_mha, x_mha, x_mha,\n",
    "        need_weights=True, average_attn_weights=False\n",
    "    )    # attn_w: (1, nhead, seq_len, seq_len)\n",
    "\n",
    "attn_np = attn_w[0].numpy()    # (nhead, seq_len, seq_len)\n",
    "tick_lbl = ['CLS'] + [f't{i}' for i in range(1, SEQ_LEN)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for h in range(nhead_demo):\n",
    "    ax = axes[h // 2][h % 2]\n",
    "    im = ax.imshow(attn_np[h], cmap='Blues', vmin=0, vmax=attn_np[h].max())\n",
    "    ax.set_title(f'Head {h+1} ì–´í…ì…˜ ê°€ì¤‘ì¹˜', fontsize=10)\n",
    "    ax.set_xlabel('Key ìœ„ì¹˜'); ax.set_ylabel('Query ìœ„ì¹˜')\n",
    "    ax.set_xticks(range(SEQ_LEN)); ax.set_xticklabels(tick_lbl, fontsize=6, rotation=45)\n",
    "    ax.set_yticks(range(SEQ_LEN)); ax.set_yticklabels(tick_lbl, fontsize=6)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(f'Multi-Head Attention ({nhead_demo} heads) â€” í—¤ë“œë³„ ì–´í…ì…˜ íŒ¨í„´\\n'\n",
    "             'ê° í—¤ë“œëŠ” ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ ê´€ê³„ì— ì§‘ì¤‘í•©ë‹ˆë‹¤ (ì´ˆê¸°í™” ì§í›„, í•™ìŠµ ì „)', fontsize=11)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f'ğŸ“Œ Multi-Head Attention í•µì‹¬:')\n",
    "print(f'  embed_dim={d_model_mha}, num_heads={nhead_demo}')\n",
    "print(f'  ê° í—¤ë“œì˜ d_k = d_model / nhead = {d_model_mha // nhead_demo}')\n",
    "print(f'  ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in mha_layer.parameters()):,}')\n",
    "print(f'  â†’ í—¤ë“œ ìˆ˜ë¥¼ ëŠ˜ë ¤ë„ ì´ íŒŒë¼ë¯¸í„°ëŠ” Single-Headì™€ ë™ì¼!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. Transformer Encoder + ê°ì„± ë¶„ë¥˜\n",
    "\n",
    "### 3-1. Transformer Encoder Block êµ¬ì¡°\n",
    "\n",
    "```\n",
    "ì…ë ¥ x\n",
    "  â”‚\n",
    "  â”œâ”€â†’ [Multi-Head Self-Attention] â”€â†’ Dropout â”€â†’ + x â”€â†’ LayerNorm â”€â†’ z\n",
    "  â”‚                                                                    â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "  z\n",
    "  â”‚\n",
    "  â”œâ”€â†’ [FFN: Linearâ†’ReLUâ†’Dropoutâ†’Linear] â”€â†’ + z â”€â†’ LayerNorm â”€â†’ output\n",
    "  â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "```\n",
    "\n",
    "**FFN:** $\\\\text{FFN}(z) = \\\\text{ReLU}(zW_1 + b_1)W_2 + b_2$  \n",
    "ë‚´ë¶€ ì°¨ì› $d_{ff} = 4 \\\\times d_{\\\\text{model}}$ (ì›ë…¼ë¬¸ ê´€ë¡€)\n",
    "\n",
    "| êµ¬ì„±ìš”ì†Œ | ì—­í•  |\n",
    "|---|---|\n",
    "| **Residual ì—°ê²°** | ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë³´í˜¸ (ResNetê³¼ ë™ì¼ ì›ë¦¬) |\n",
    "| **LayerNorm** | ê° ë ˆì´ì–´ ì¶œë ¥ì„ ì •ê·œí™” â†’ í•™ìŠµ ì•ˆì •í™” |\n",
    "| **FFN** | ê° ìœ„ì¹˜ ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• ë³€í™˜ (ìœ„ì¹˜ ê°„ ì •ë³´ ê³µìœ  ì—†ìŒ) |\n",
    "\n",
    "### 3-2. CLS í† í° ë¶„ë¥˜ ì „ëµ\n",
    "- ì‹œí€€ìŠ¤ ë§¨ ì•ì— íŠ¹ìˆ˜ `CLS(=1)` í† í° ì¶”ê°€\n",
    "- Self-Attentionì„ í†µí•´ CLSê°€ ì „ì²´ ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ì§‘ì•½\n",
    "- ìµœì¢… ë ˆì´ì–´ì˜ CLS ìœ„ì¹˜ ì¶œë ¥ â†’ Linear â†’ í´ë˜ìŠ¤ ì˜ˆì¸¡\n",
    "- BERTì˜ ë¶„ë¥˜ ë°©ì‹ê³¼ ë™ì¼í•œ ì›ë¦¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ Transformer Encoder ë ˆì´ì–´\n",
    "    x â†’ MHA â†’ Add & Norm â†’ FFN â†’ Add & Norm â†’ output\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn  = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.ff    = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        self.last_attn_weights = None   # Exercise 2 ìš© ì €ì¥ ìŠ¬ë¡¯\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # 1) Multi-Head Self-Attention + Residual + LayerNorm\n",
    "        attn_out, attn_w = self.attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False   # (B, nhead, seq, seq)\n",
    "        )\n",
    "        self.last_attn_weights = attn_w.detach().cpu()\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        # 2) FFN + Residual + LayerNorm\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder ê¸°ë°˜ ê°ì„± ë¶„ë¥˜ê¸°\n",
    "    Embedding â†’ PositionalEncoding â†’ N Ã— EncoderBlock â†’ CLS í’€ë§ â†’ Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=4, d_ff=256,\n",
    "                 n_layers=2, n_classes=2, dropout=0.1, use_pe=True):\n",
    "        super().__init__()\n",
    "        self.d_model   = d_model\n",
    "        self.use_pe    = use_pe\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe        = PositionalEncoding(d_model, max_len=512, dropout=dropout)\n",
    "        self.blocks    = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, nhead, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # 1) í† í° ì„ë² ë”© (âˆšd_model ìŠ¤ì¼€ì¼ë§ â€” ì›ë…¼ë¬¸ ê´€ë¡€)\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)  # (B, seq_len, d_model)\n",
    "        # 2) Positional Encoding\n",
    "        if self.use_pe:\n",
    "            x = self.pe(x)\n",
    "        # 3) Nê°œ EncoderBlock í†µê³¼\n",
    "        for block in self.blocks:\n",
    "            x = block(x, key_padding_mask=key_padding_mask)\n",
    "        # 4) CLS í† í° (ìœ„ì¹˜ 0) ì¶œë ¥ìœ¼ë¡œ ë¶„ë¥˜\n",
    "        cls_out = x[:, 0, :]               # (B, d_model)\n",
    "        return self.classifier(cls_out)    # (B, n_classes)\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def train_transformer(model, train_loader, X_val, y_val,\n",
    "                      n_epochs=30, lr=3e-4, verbose=True):\n",
    "    \"\"\"Transformer ê°ì„± ë¶„ë¥˜ í•™ìŠµ (CrossEntropyLoss + Adam)\"\"\"\n",
    "    model     = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    X_v = X_val.to(device)\n",
    "    y_v = y_val.to(device)\n",
    "    train_losses, val_accs = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        ep_loss = 0.\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "        train_losses.append(ep_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_v)\n",
    "            val_acc    = (val_logits.argmax(1) == y_v).float().mean().item()\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        if verbose and (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} | '\n",
    "                  f'Train Loss: {train_losses[-1]:.4f} | Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    model.cpu()\n",
    "    return train_losses, val_accs\n",
    "\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
    "print(f'{\"n_layers\":<10} {\"íŒŒë¼ë¯¸í„° ìˆ˜\":>14}')\n",
    "print('-' * 26)\n",
    "for nl in [1, 2, 4]:\n",
    "    m = TransformerSentimentClassifier(VOCAB_SIZE, d_model=64, nhead=4, d_ff=256, n_layers=nl)\n",
    "    print(f'{nl:<10} {count_params(m):>14,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-p3-code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('=== Transformer ê°ì„± ë¶„ë¥˜ê¸° í•™ìŠµ (30 ì—í¬í¬) ===')\n",
    "torch.manual_seed(42)\n",
    "model_base = TransformerSentimentClassifier(\n",
    "    vocab_size=VOCAB_SIZE, d_model=64, nhead=4, d_ff=256,\n",
    "    n_layers=2, n_classes=2, dropout=0.1\n",
    ")\n",
    "print(f'íŒŒë¼ë¯¸í„° ìˆ˜: {count_params(model_base):,}\\n')\n",
    "\n",
    "base_losses, base_accs = train_transformer(\n",
    "    model_base, train_ldr, X_te, y_te, n_epochs=30\n",
    ")\n",
    "\n",
    "model_base.eval()\n",
    "with torch.no_grad():\n",
    "    te_acc = (model_base(X_te).argmax(1) == y_te).float().mean().item()\n",
    "print(f'\\nìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {te_acc:.4f}')\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "ep = range(1, 31)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(ep, base_losses, 'steelblue', lw=2)\n",
    "axes[0].set_title('Train Loss (ì—í¬í¬ë³„)')\n",
    "axes[0].set_xlabel('ì—í¬í¬'); axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "\n",
    "axes[1].plot(ep, base_accs, 'seagreen', lw=2)\n",
    "axes[1].axhline(y=0.5, color='gray', ls='--', lw=1, label='ë¬´ì‘ìœ„ ê¸°ì¤€ì„  (0.5)')\n",
    "axes[1].set_title('Val Accuracy (ì—í¬í¬ë³„)')\n",
    "axes[1].set_xlabel('ì—í¬í¬'); axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_ylim(0, 1.05); axes[1].legend()\n",
    "\n",
    "plt.suptitle(f'Transformer ê°ì„± ë¶„ë¥˜ê¸° í•™ìŠµ ê²°ê³¼\\n'\n",
    "             f'(d_model=64, nhead=4, n_layers=2, ìµœì¢… Val Acc: {base_accs[-1]:.4f})', fontsize=11)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('\\nğŸ“Œ ì„¤ëª…:')\n",
    "print('  - CLS í† í°ì´ Self-Attentionì„ í†µí•´ ì „ì²´ ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ì§‘ì•½í•©ë‹ˆë‹¤.')\n",
    "print('  - í•©ì„± ë°ì´í„°ì—ì„œ ê¸ì •/ë¶€ì • í† í° ë¹„ìœ¨ íŒ¨í„´ì„ Transformerê°€ í•™ìŠµí•©ë‹ˆë‹¤.')\n",
    "print('  - ë¬´ì‘ìœ„ ê¸°ì¤€ì„ (0.5) ëŒ€ë¹„ ì„±ëŠ¥ì´ ì˜¬ë¼ê°ˆìˆ˜ë¡ í•™ìŠµì´ ì˜ ëœ ê²ƒì…ë‹ˆë‹¤.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise\n",
    "\n",
    "### Exercise 1. Encoder ë ˆì´ì–´ ìˆ˜ íƒìƒ‰\n",
    "\n",
    "`n_layers âˆˆ {1, 2, 4}`ë¡œ `TransformerSentimentClassifier`ë¥¼ í•™ìŠµí•˜ì„¸ìš”.\n",
    "\n",
    "| n_layers | ë¹„ê³  |\n",
    "|---|---|\n",
    "| 1 | ë‹¨ì¼ Encoder ë ˆì´ì–´ |\n",
    "| 2 | Part 3ì™€ ë™ì¼ (ê¸°ì¤€ì„ ) |\n",
    "| 4 | ê¹Šì€ Encoder |\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "- ë™ì¼ ì¡°ê±´: `d_model=64, nhead=4, d_ff=256, dropout=0.1, n_epochs=30`\n",
    "- ìµœì¢… Val Accuracy + íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í‘œë¡œ ì¶œë ¥\n",
    "- ë§‰ëŒ€ê·¸ë˜í”„ ì‹œê°í™” (x=n_layers, y=Val Acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Encoder ë ˆì´ì–´ ìˆ˜ íƒìƒ‰\n",
    "\n",
    "layer_configs = [1, 2, 4]\n",
    "ex1_results   = {}   # {n_layers: (val_acc, n_params)}\n",
    "\n",
    "for n_layers in layer_configs:\n",
    "    print(f'--- n_layers={n_layers} í•™ìŠµ ì¤‘ ---')\n",
    "    torch.manual_seed(42)\n",
    "    # Your code here: TransformerSentimentClassifier ìƒì„± ë° train_transformer í˜¸ì¶œ\n",
    "    # m = TransformerSentimentClassifier(...)\n",
    "    # losses, accs = train_transformer(m, train_ldr, X_te, y_te, n_epochs=30, verbose=False)\n",
    "    # ex1_results[n_layers] = (accs[-1], count_params(m))\n",
    "    pass\n",
    "\n",
    "# Your code here: ê²°ê³¼ í‘œ ì¶œë ¥\n",
    "# print(f'{\"n_layers\":>10} {\"Val Acc\":>10} {\"íŒŒë¼ë¯¸í„°\":>12}')\n",
    "# ...\n",
    "\n",
    "# Your code here: ë§‰ëŒ€ê·¸ë˜í”„ ì‹œê°í™” (x=n_layers, y=Val Acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md2",
   "metadata": {},
   "source": [
    "### Exercise 2. í•™ìŠµëœ ì–´í…ì…˜ íˆíŠ¸ë§µ ì‹œê°í™”\n",
    "\n",
    "í•™ìŠµëœ `model_base`ì—ì„œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ 1ê°œë¥¼ ì„ íƒí•˜ì—¬ **ê° ë ˆì´ì–´, ê° í—¤ë“œ**ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ì„¸ìš”.\n",
    "\n",
    "**íŒíŠ¸:**  \n",
    "- `model_base.blocks[i].last_attn_weights`ëŠ” forward í˜¸ì¶œ í›„ ìë™ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.  \n",
    "- shape: `(1, nhead, seq_len, seq_len)`\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "1. `model_base.eval()` + `torch.no_grad()`ë¡œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ forward\n",
    "2. `n_layers Ã— nhead` í¬ê¸°ì˜ subplot gridë¡œ íˆíŠ¸ë§µ ì‹œê°í™”\n",
    "3. ê´€ì°°: ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ CLS í† í°(í–‰/ì—´ 0ë²ˆ)ì˜ ì–´í…ì…˜ íŒ¨í„´ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: í•™ìŠµëœ ì–´í…ì…˜ íˆíŠ¸ë§µ ì‹œê°í™”\n",
    "\n",
    "sample_idx = 0\n",
    "sample_tok = X_te[sample_idx:sample_idx+1]   # (1, seq_len)\n",
    "label_str  = 'ê¸ì •' if y_te[sample_idx].item() == 1 else 'ë¶€ì •'\n",
    "print(f'ì„ íƒëœ ìƒ˜í”Œ: {label_str} (token: {sample_tok[0].tolist()})')\n",
    "\n",
    "# Your code here:\n",
    "# 1) model_base.eval() + torch.no_grad()ë¡œ forward í˜¸ì¶œ\n",
    "#    â†’ ê° blockì˜ last_attn_weightsê°€ ìë™ ì €ì¥ë¨\n",
    "\n",
    "# 2) n_layers Ã— nhead íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°\n",
    "#    íŒíŠ¸: model_base.blocks[i].last_attn_weights[0].numpy()  â†’ (nhead, seq_len, seq_len)\n",
    "\n",
    "# 3) ê´€ì°° ê²°ê³¼ë¥¼ ì£¼ì„ìœ¼ë¡œ ì‘ì„±:\n",
    "# ì˜ˆ) ë ˆì´ì–´ 1ì˜ CLSëŠ” ... ì— ì§‘ì¤‘, ë ˆì´ì–´ 2ì˜ CLSëŠ” ... ì— ì§‘ì¤‘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-ex-md3",
   "metadata": {},
   "source": [
    "### Exercise 3. (ë„ì „) nhead íƒìƒ‰ + Positional Encoding ì œê±° ì‹¤í—˜\n",
    "\n",
    "ë‘ ê°€ì§€ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "**ì‹¤í—˜ A â€” nhead íƒìƒ‰:** `nhead âˆˆ {1, 2, 4, 8}` (`d_model=64`ì´ë¯€ë¡œ ëª¨ë‘ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§)  \n",
    "- ê°ê° í•™ìŠµ í›„ ìµœì¢… Val Accuracy ë¹„êµ  \n",
    "- ë§‰ëŒ€ê·¸ë˜í”„ ì‹œê°í™”\n",
    "\n",
    "**ì‹¤í—˜ B â€” PE ì œê±° ì‹¤í—˜:** `TransformerSentimentClassifier(use_pe=False)`ë¡œ í•™ìŠµ  \n",
    "- PE ìˆìŒ(ê¸°ì¡´ `base_accs`) vs PE ì—†ìŒì˜ Val Accuracy ê³¡ì„  ë¹„êµ  \n",
    "- ì§ˆë¬¸: í•©ì„± ê°ì„± ë°ì´í„°ì—ì„œ ìˆœì„œ ì •ë³´(PE)ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œê°€? ì´ìœ ëŠ”?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 (ë„ì „): nhead íƒìƒ‰ + PE ì œê±° ì‹¤í—˜\n",
    "\n",
    "# â”€â”€ ì‹¤í—˜ A: nhead íƒìƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "nhead_configs = [1, 2, 4, 8]\n",
    "ex3a_results  = {}   # {nhead: val_acc}\n",
    "\n",
    "for nhead in nhead_configs:\n",
    "    torch.manual_seed(42)\n",
    "    # Your code here: TransformerSentimentClassifier(nhead=nhead, ...) í•™ìŠµ\n",
    "    # m = TransformerSentimentClassifier(...)\n",
    "    # _, accs = train_transformer(m, train_ldr, X_te, y_te, n_epochs=30, verbose=False)\n",
    "    # ex3a_results[nhead] = accs[-1]\n",
    "    pass\n",
    "\n",
    "# Your code here: nheadë³„ Val Acc ë§‰ëŒ€ê·¸ë˜í”„\n",
    "\n",
    "\n",
    "# â”€â”€ ì‹¤í—˜ B: PE ì œê±° ì‹¤í—˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "torch.manual_seed(42)\n",
    "# Your code here: TransformerSentimentClassifier(use_pe=False) í•™ìŠµ\n",
    "# model_no_pe = TransformerSentimentClassifier(..., use_pe=False)\n",
    "# _, accs_nope = train_transformer(...)\n",
    "\n",
    "# Your code here: PE ìˆìŒ vs ì—†ìŒ ë¹„êµ ê·¸ë˜í”„ (base_accs vs accs_nope)\n",
    "\n",
    "# ê´€ì°° ë° ì´ìœ  ì‘ì„± (ì£¼ì„):\n",
    "# í•©ì„± ë°ì´í„°ì—ì„œ PEì˜ ì¤‘ìš”ë„:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| ê°œë… | í•µì‹¬ ë‚´ìš© |\n",
    "|---|---|\n",
    "| **Query / Key / Value** | Q=\"ì°¾ê³  ì‹¶ì€ ê²ƒ\", K=\"ì–´ë”” ìˆë‚˜\", V=\"ì‹¤ì œ ì •ë³´\"; Attention = softmax(QKáµ€/âˆšd_k)V |\n",
    "| **âˆšd_k ìŠ¤ì¼€ì¼ë§** | ë‚´ì  ë¶„ì‚° = d_k â†’ ë‚˜ëˆ„ë©´ ë¶„ì‚° = 1 â†’ softmax í¬í™” ë°©ì§€ |\n",
    "| **Self-Attention** | Q=K=V=ê°™ì€ ì‹œí€€ìŠ¤; ëª¨ë“  ìœ„ì¹˜ ìŒì„ ë™ì‹œì— ê³„ì‚° â†’ RNNì˜ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° |\n",
    "| **Multi-Head Attention** | í—¤ë“œ hê°œê°€ d_model/h ì°¨ì›ì—ì„œ ë³‘ë ¬ ì–´í…ì…˜ â†’ ë‹¤ì–‘í•œ ê´€ê³„ ë™ì‹œ í¬ì°©, íŒŒë¼ë¯¸í„°ëŠ” ë™ì¼ |\n",
    "| **Positional Encoding** | sin/cosìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ ì£¼ì…; TransformerëŠ” ìì²´ì ìœ¼ë¡œ ìˆœì„œ ëª¨ë¦„ |\n",
    "| **Encoder Block** | MHA â†’ Add&Norm â†’ FFN â†’ Add&Norm; Residual + LayerNormìœ¼ë¡œ í•™ìŠµ ì•ˆì •í™” |\n",
    "| **FFN** | Linear(d_modelâ†’d_ff) â†’ ReLU â†’ Linear(d_ffâ†’d_model); d_ff = 4Ã—d_model ê´€ë¡€ |\n",
    "| **CLS í† í°** | ì²« ìœ„ì¹˜ì— ê³ ì • í† í° ì¶”ê°€ â†’ Self-Attentionìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ í‘œí˜„ ì§‘ì•½ â†’ ë¶„ë¥˜ |\n",
    "| **O(nÂ²) ë³µì¡ë„** | ì–´í…ì…˜ì€ ëª¨ë“  ìœ„ì¹˜ ìŒ ê³„ì‚° â†’ seq_lenâ†‘ ì‹œ ë©”ëª¨ë¦¬/ì‹œê°„ ê¸‰ì¦; Flash Attention ë“±ìœ¼ë¡œ ê°œì„  |\n",
    "| **RNN vs Transformer** | RNN: ìˆœì°¨ ì²˜ë¦¬, ì¥ê¸° ì˜ì¡´ì„± ì·¨ì•½ / Transformer: ë³‘ë ¬ ì²˜ë¦¬, ëª¨ë“  ìœ„ì¹˜ ì§ì ‘ ì—°ê²° |\n",
    "\n",
    "---\n",
    "\n",
    "**ë‹¤ìŒ ê°•ì˜:** BERT, GPT â€” ì‚¬ì „í•™ìŠµ(Pre-training) + íŒŒì¸íŠœë‹(Fine-tuning) íŒ¨ëŸ¬ë‹¤ì„\n"
   ]
  }
 ]
}